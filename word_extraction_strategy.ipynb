{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ada48a",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This notebook outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847371cf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and other necessary libraries for the word extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f498d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866e8e6",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file for word extraction processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa9299",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0d50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ spacy installed successfully\n",
      "✓ spacy installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ Dutch language model downloaded successfully\n",
      "✓ Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"✓ Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b84080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"✓ Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"✗ Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8888f",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc25ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82f065",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943fb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c7e61",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b90d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: output/test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"output/test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b917",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69511f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"output/words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b28d1",
   "metadata": {},
   "source": [
    "## Step 5.5: Multi-threaded Processing Pipeline (Performance Optimized)\n",
    "\n",
    "This improved version uses multi-threading to process articles in parallel, significantly reducing processing time. Each thread processes a batch of articles independently, and results are safely written to the database using locks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22994673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-threaded processing functions defined!\n",
      "Ready for high-performance article processing.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import concurrent.futures\n",
    "from queue import Queue\n",
    "import time\n",
    "import logging\n",
    "import signal\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging for multi-threaded processing (avoid duplicate handlers)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Only configure logging if it hasn't been configured yet\n",
    "if not logger.handlers:\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - [Thread-%(thread)d] - %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Prevent propagation to root logger to avoid duplicates\n",
    "    logger.propagate = False\n",
    "\n",
    "# Global flag for interrupt handling\n",
    "interrupt_flag = threading.Event()\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle keyboard interrupt (Ctrl+C) gracefully.\"\"\"\n",
    "    logger.warning(\"🛑 Interrupt received! Stopping processing gracefully...\")\n",
    "    interrupt_flag.set()\n",
    "\n",
    "# Set up signal handler for interrupt\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "class ThreadSafeDatabase:\n",
    "    \"\"\"Thread-safe database operations for concurrent processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.lock = threading.Lock()\n",
    "        self.local = threading.local()\n",
    "        logger.info(f\"📁 Initialized thread-safe database: {db_path}\")\n",
    "    \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a thread-local database connection.\"\"\"\n",
    "        if not hasattr(self.local, 'conn'):\n",
    "            self.local.conn = sqlite3.connect(self.db_path)\n",
    "            logger.debug(f\"🔗 Created database connection for thread {threading.current_thread().ident}\")\n",
    "        return self.local.conn\n",
    "    \n",
    "    def insert_word_data_threadsafe(self, word_data, year):\n",
    "        \"\"\"Thread-safe version of insert_word_data.\"\"\"\n",
    "        with self.lock:\n",
    "            conn = self.get_connection()\n",
    "            insert_word_data(conn, word_data, year)\n",
    "            logger.debug(f\"💾 Inserted {len(word_data)} words for year {year}\")\n",
    "    \n",
    "    def close_all_connections(self):\n",
    "        \"\"\"Close all thread-local connections.\"\"\"\n",
    "        if hasattr(self.local, 'conn'):\n",
    "            self.local.conn.close()\n",
    "            logger.debug(f\"🔒 Closed database connection for thread {threading.current_thread().ident}\")\n",
    "\n",
    "def process_article_batch_threaded(batch_data):\n",
    "    \"\"\"\n",
    "    Process a batch of articles in a single thread with interrupt handling.\n",
    "    \n",
    "    Args:\n",
    "        batch_data (tuple): (batch_df, nlp_model, thread_safe_db, thread_id, batch_num)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing statistics for this batch\n",
    "    \"\"\"\n",
    "    batch_df, nlp_model, thread_safe_db, thread_id, batch_num = batch_data\n",
    "    \n",
    "    articles_processed = 0\n",
    "    words_extracted = 0\n",
    "    errors_count = 0\n",
    "    \n",
    "    logger.info(f\"🚀 Thread {thread_id} starting batch {batch_num} with {len(batch_df)} articles\")\n",
    "    \n",
    "    for idx, row in batch_df.iterrows():\n",
    "        # Check for interrupt signal\n",
    "        if interrupt_flag.is_set():\n",
    "            logger.warning(f\"⏹️ Thread {thread_id} stopping due to interrupt signal\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Extract year from published_time\n",
    "            if pd.notna(row['published_time']):\n",
    "                if isinstance(row['published_time'], str):\n",
    "                    year = pd.to_datetime(row['published_time']).year\n",
    "                else:\n",
    "                    year = row['published_time'].year\n",
    "            else:\n",
    "                year = 2020  # Default year if missing\n",
    "            \n",
    "            # Process different text fields\n",
    "            text_fields = ['title', 'description', 'content']\n",
    "            all_text = []\n",
    "            \n",
    "            for field in text_fields:\n",
    "                if field in row and pd.notna(row[field]):\n",
    "                    if field == 'content':\n",
    "                        # Clean HTML from content\n",
    "                        clean_text = clean_html_content(row[field])\n",
    "                    else:\n",
    "                        clean_text = str(row[field])\n",
    "                    \n",
    "                    preprocessed = preprocess_text(clean_text)\n",
    "                    if preprocessed:\n",
    "                        all_text.append(preprocessed)\n",
    "            \n",
    "            # Combine all text\n",
    "            combined_text = ' '.join(all_text)\n",
    "            \n",
    "            if combined_text:\n",
    "                # Extract words\n",
    "                words = extract_words_from_text(combined_text, nlp_model)\n",
    "                \n",
    "                if words:\n",
    "                    # Insert into database (thread-safe)\n",
    "                    thread_safe_db.insert_word_data_threadsafe(words, year)\n",
    "                    words_extracted += len(words)\n",
    "            \n",
    "            articles_processed += 1\n",
    "            \n",
    "            # Log progress every 50 articles within the batch\n",
    "            if articles_processed % 50 == 0:\n",
    "                logger.debug(f\"📊 Thread {thread_id} batch {batch_num}: {articles_processed}/{len(batch_df)} articles processed\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(f\"⚠️ Thread {thread_id} received keyboard interrupt\")\n",
    "            interrupt_flag.set()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            errors_count += 1\n",
    "            logger.error(f\"❌ Thread {thread_id}: Error processing article {idx}: {e}\")\n",
    "            if errors_count > 10:  # Stop if too many errors\n",
    "                logger.error(f\"🚨 Thread {thread_id}: Too many errors, stopping batch\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"✅ Thread {thread_id} completed batch {batch_num}: {articles_processed} articles, {words_extracted} words, {errors_count} errors\")\n",
    "    \n",
    "    return {\n",
    "        'thread_id': thread_id,\n",
    "        'batch_num': batch_num,\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': words_extracted,\n",
    "        'errors_count': errors_count,\n",
    "        'interrupted': interrupt_flag.is_set()\n",
    "    }\n",
    "\n",
    "def process_articles_multithreaded(df, nlp_models, db_path=\"output/words_database.sqlite\", \n",
    "                                 batch_size=500, num_threads=None, log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Multi-threaded pipeline to process articles with improved performance, logging, and interrupt handling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_models: List of spaCy models (one per thread) or single model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles per batch\n",
    "        num_threads (int): Number of threads to use (defaults to CPU count)\n",
    "        log_level: Logging level for detailed output\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing statistics\n",
    "    \"\"\"\n",
    "    # Reset interrupt flag\n",
    "    interrupt_flag.clear()\n",
    "    \n",
    "    # Set logging level\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    if num_threads is None:\n",
    "        num_threads = min(8, os.cpu_count())  # Limit to 8 threads max\n",
    "    \n",
    "    logger.info(\"🚀 Starting multi-threaded processing...\")\n",
    "    logger.info(f\"📊 Configuration:\")\n",
    "    logger.info(f\"   Articles to process: {len(df):,}\")\n",
    "    logger.info(f\"   Batch size: {batch_size}\")\n",
    "    logger.info(f\"   Number of threads: {num_threads}\")\n",
    "    logger.info(f\"   Database path: {db_path}\")\n",
    "    logger.info(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Setup database\n",
    "    logger.info(\"🗄️ Setting up database...\")\n",
    "    setup_database(db_path)\n",
    "    thread_safe_db = ThreadSafeDatabase(db_path)\n",
    "    \n",
    "    # Prepare spaCy models for each thread\n",
    "    logger.info(\"🧠 Loading spaCy models for threads...\")\n",
    "    if isinstance(nlp_models, list):\n",
    "        if len(nlp_models) != num_threads:\n",
    "            logger.warning(f\"⚠️ {len(nlp_models)} models provided for {num_threads} threads\")\n",
    "            # Use first model for all threads if not enough models\n",
    "            nlp_models = [nlp_models[0]] * num_threads\n",
    "    else:\n",
    "        # Single model - each thread will need its own copy\n",
    "        nlp_models = []\n",
    "        for i in range(num_threads):\n",
    "            try:\n",
    "                nlp_model = spacy.load(\"nl_core_news_sm\")\n",
    "                nlp_models.append(nlp_model)\n",
    "                logger.info(f\"   ✅ Thread {i+1} model loaded\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"   ❌ Error loading model for thread {i+1}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    # Split dataframe into batches\n",
    "    logger.info(\"📦 Creating batches...\")\n",
    "    batches = []\n",
    "    total_articles = len(df)\n",
    "    \n",
    "    for start_idx in range(0, total_articles, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        batches.append(batch_df)\n",
    "    \n",
    "    logger.info(f\"   Created {len(batches)} batches for processing\")\n",
    "    \n",
    "    # Prepare batch data for threads\n",
    "    batch_data_list = []\n",
    "    for i, batch_df in enumerate(batches):\n",
    "        thread_id = i % num_threads\n",
    "        nlp_model = nlp_models[thread_id]\n",
    "        batch_data_list.append((batch_df, nlp_model, thread_safe_db, thread_id, i))\n",
    "    \n",
    "    # Process batches using ThreadPoolExecutor\n",
    "    logger.info(\"⚡ Starting thread pool execution...\")\n",
    "    start_time = time.time()\n",
    "    total_articles_processed = 0\n",
    "    total_words_extracted = 0\n",
    "    total_errors = 0\n",
    "    completed_batches = 0\n",
    "    interrupted = False\n",
    "    \n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            logger.info(f\"🎯 Submitted {len(batch_data_list)} batches to {num_threads} threads\")\n",
    "            \n",
    "            # Submit all batches\n",
    "            future_to_batch = {\n",
    "                executor.submit(process_article_batch_threaded, batch_data): i \n",
    "                for i, batch_data in enumerate(batch_data_list)\n",
    "            }\n",
    "            \n",
    "            # Process completed batches\n",
    "            for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                batch_num = future_to_batch[future]\n",
    "                \n",
    "                try:\n",
    "                    result = future.result(timeout=30)  # 30 second timeout per batch\n",
    "                    total_articles_processed += result['articles_processed']\n",
    "                    total_words_extracted += result['words_extracted']\n",
    "                    total_errors += result.get('errors_count', 0)\n",
    "                    completed_batches += 1\n",
    "                    \n",
    "                    if result.get('interrupted', False):\n",
    "                        interrupted = True\n",
    "                        logger.warning(f\"🛑 Batch {batch_num} was interrupted\")\n",
    "                    \n",
    "                    # Progress update\n",
    "                    progress = (completed_batches / len(batches)) * 100\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = total_articles_processed / elapsed if elapsed > 0 else 0\n",
    "                    \n",
    "                    if completed_batches % 5 == 0 or completed_batches == len(batches) or interrupted:\n",
    "                        logger.info(f\"📈 Progress: {completed_batches}/{len(batches)} batches ({progress:.1f}%)\")\n",
    "                        logger.info(f\"   📊 Articles: {total_articles_processed:,}/{total_articles:,}\")\n",
    "                        logger.info(f\"   💬 Words extracted: {total_words_extracted:,}\")\n",
    "                        logger.info(f\"   ⏱️ Time: {elapsed:.1f}s ({rate:.1f} articles/sec)\")\n",
    "                        logger.info(f\"   ❌ Errors: {total_errors}\")\n",
    "                        \n",
    "                        if interrupted:\n",
    "                            logger.warning(\"🚨 Interrupt detected, stopping remaining batches...\")\n",
    "                            break\n",
    "                            \n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    logger.error(f\"⏰ Batch {batch_num} timed out\")\n",
    "                    total_errors += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"💥 Batch {batch_num} generated an exception: {e}\")\n",
    "                    total_errors += 1\n",
    "            \n",
    "            if interrupted:\n",
    "                # Cancel remaining futures\n",
    "                for future in future_to_batch:\n",
    "                    future.cancel()\n",
    "                logger.warning(\"🛑 Cancelled remaining batch processing due to interrupt\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"⚠️ Keyboard interrupt received in main thread\")\n",
    "        interrupt_flag.set()\n",
    "        interrupted = True\n",
    "    \n",
    "    # Close all database connections\n",
    "    logger.info(\"🔒 Closing database connections...\")\n",
    "    thread_safe_db.close_all_connections()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Log final results to database\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        status = \"INTERRUPTED\" if interrupted else \"COMPLETED\"\n",
    "        cursor.execute('''\n",
    "            INSERT INTO processing_log (articles_processed, words_extracted, notes) \n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (\n",
    "            total_articles_processed, \n",
    "            total_words_extracted, \n",
    "            f\"Multi-threaded processing {status} - {num_threads} threads, {batch_size} batch size, {processing_time:.1f}s, {total_errors} errors\"\n",
    "        ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        logger.info(\"📝 Logged processing results to database\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log to database: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    status_emoji = \"⚠️\" if interrupted else \"✅\"\n",
    "    logger.info(f\"\\n{status_emoji} === MULTI-THREADED PROCESSING {'INTERRUPTED' if interrupted else 'COMPLETE'} ===\")\n",
    "    logger.info(f\"📊 Total articles processed: {total_articles_processed:,}/{total_articles:,}\")\n",
    "    logger.info(f\"💬 Total words extracted: {total_words_extracted:,}\")\n",
    "    logger.info(f\"⏱️ Processing time: {processing_time:.1f} seconds\")\n",
    "    logger.info(f\"🚀 Articles per second: {total_articles_processed/processing_time:.1f}\")\n",
    "    logger.info(f\"❌ Total errors: {total_errors}\")\n",
    "    logger.info(f\"📁 Database saved to: {db_path}\")\n",
    "    logger.info(f\"🏁 End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': total_articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'processing_time': processing_time,\n",
    "        'articles_per_second': total_articles_processed/processing_time if processing_time > 0 else 0,\n",
    "        'database_path': db_path,\n",
    "        'num_threads': num_threads,\n",
    "        'total_errors': total_errors,\n",
    "        'interrupted': interrupted,\n",
    "        'batches_completed': completed_batches,\n",
    "        'batches_total': len(batches)\n",
    "    }\n",
    "\n",
    "print(\"✅ Multi-threaded processing functions defined!\")\n",
    "print(\"Ready for high-performance article processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Interrupt handling and monitoring utilities defined!\n",
      "Functions available:\n",
      "  - reset_interrupt(): Reset interrupt flag\n",
      "  - force_interrupt(): Manually trigger interrupt\n",
      "  - check_interrupt_status(): Check current status\n",
      "  - monitor_processing_progress(): Check database progress\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for interrupt handling and monitoring\n",
    "def reset_logging():\n",
    "    \"\"\"Reset logging configuration to prevent duplicates.\"\"\"\n",
    "    global logger\n",
    "    # Clear all handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Recreate formatter and handler\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - [Thread-%(thread)d] - %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    \n",
    "    logger.info(\"🔄 Logging configuration reset\")\n",
    "\n",
    "def reset_interrupt():\n",
    "    \"\"\"Reset the interrupt flag to allow new processing.\"\"\"\n",
    "    interrupt_flag.clear()\n",
    "    logger.info(\"🔄 Interrupt flag reset - ready for new processing\")\n",
    "\n",
    "def force_interrupt():\n",
    "    \"\"\"Manually trigger an interrupt (useful for testing).\"\"\"\n",
    "    interrupt_flag.set()\n",
    "    logger.warning(\"🛑 Manual interrupt triggered\")\n",
    "\n",
    "def check_interrupt_status():\n",
    "    \"\"\"Check if processing is currently interrupted.\"\"\"\n",
    "    status = \"INTERRUPTED\" if interrupt_flag.is_set() else \"NORMAL\"\n",
    "    logger.info(f\"📊 Current interrupt status: {status}\")\n",
    "    return interrupt_flag.is_set()\n",
    "\n",
    "def monitor_processing_progress(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"Monitor the progress of ongoing processing by checking the database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get latest processing log entry\n",
    "        cursor.execute('''\n",
    "            SELECT articles_processed, words_extracted, processing_date, notes \n",
    "            FROM processing_log \n",
    "            ORDER BY processing_date DESC \n",
    "            LIMIT 1\n",
    "        ''')\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            articles, words, date, notes = result\n",
    "            logger.info(f\"📈 Latest processing status:\")\n",
    "            logger.info(f\"   📊 Articles processed: {articles:,}\")\n",
    "            logger.info(f\"   💬 Words extracted: {words:,}\")\n",
    "            logger.info(f\"   ⏰ Last update: {date}\")\n",
    "            logger.info(f\"   📝 Notes: {notes}\")\n",
    "        else:\n",
    "            logger.info(\"📭 No processing log entries found\")\n",
    "            \n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error monitoring progress: {e}\")\n",
    "\n",
    "print(\"🛠️ Interrupt handling and monitoring utilities defined!\")\n",
    "print(\"Functions available:\")\n",
    "print(\"  - reset_logging(): Reset logging configuration\")\n",
    "print(\"  - reset_interrupt(): Reset interrupt flag\")\n",
    "print(\"  - force_interrupt(): Manually trigger interrupt\")\n",
    "print(\"  - check_interrupt_status(): Check current status\")\n",
    "print(\"  - monitor_processing_progress(): Check database progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f49f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:21 - INFO - [Thread-25780] - 🔄 Interrupt flag reset - ready for new processing\n",
      "22:33:21 - INFO - [Thread-25780] - 🚀 Starting multi-threaded processing...\n",
      "22:33:21 - INFO - [Thread-25780] - 📊 Configuration:\n",
      "22:33:21 - INFO - [Thread-25780] -    Articles to process: 100\n",
      "22:33:21 - INFO - [Thread-25780] -    Batch size: 20\n",
      "22:33:21 - INFO - [Thread-25780] -    Number of threads: 3\n",
      "22:33:21 - INFO - [Thread-25780] -    Database path: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:21\n",
      "22:33:21 - INFO - [Thread-25780] - 🗄️ Setting up database...\n",
      "22:33:21 - INFO - [Thread-25780] - 🚀 Starting multi-threaded processing...\n",
      "22:33:21 - INFO - [Thread-25780] - 📊 Configuration:\n",
      "22:33:21 - INFO - [Thread-25780] -    Articles to process: 100\n",
      "22:33:21 - INFO - [Thread-25780] -    Batch size: 20\n",
      "22:33:21 - INFO - [Thread-25780] -    Number of threads: 3\n",
      "22:33:21 - INFO - [Thread-25780] -    Database path: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:21\n",
      "22:33:21 - INFO - [Thread-25780] - 🗄️ Setting up database...\n",
      "22:33:21 - INFO - [Thread-25780] - 📁 Initialized thread-safe database: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] - 🧠 Loading spaCy models for threads...\n",
      "22:33:21 - INFO - [Thread-25780] - 📁 Initialized thread-safe database: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] - 🧠 Loading spaCy models for threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing enhanced multi-threaded pipeline...\n",
      "Created sample with 100 articles for testing\n",
      "\n",
      "📊 Enhanced Features Demo:\n",
      "1. Detailed logging with timestamps and thread IDs\n",
      "2. Interrupt handling (Ctrl+C or notebook interrupt)\n",
      "3. Progress monitoring and error tracking\n",
      "4. Graceful shutdown and cleanup\n",
      "\n",
      "🚀 Starting enhanced multi-threaded processing...\n",
      "Database setup complete: output/enhanced_test.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:22 - INFO - [Thread-25780] -    ✅ Thread 1 model loaded\n",
      "22:33:22 - INFO - [Thread-25780] -    ✅ Thread 2 model loaded\n",
      "22:33:22 - INFO - [Thread-25780] -    ✅ Thread 2 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] -    ✅ Thread 3 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] - 📦 Creating batches...\n",
      "22:33:23 - INFO - [Thread-25780] -    Created 5 batches for processing\n",
      "22:33:23 - INFO - [Thread-25780] - ⚡ Starting thread pool execution...\n",
      "22:33:23 - INFO - [Thread-25780] - 🎯 Submitted 5 batches to 3 threads\n",
      "22:33:23 - INFO - [Thread-24600] - 🚀 Thread 0 starting batch 0 with 20 articles\n",
      "22:33:23 - INFO - [Thread-30948] - 🚀 Thread 1 starting batch 1 with 20 articles\n",
      "22:33:23 - INFO - [Thread-25780] -    ✅ Thread 3 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] - 📦 Creating batches...\n",
      "22:33:23 - INFO - [Thread-25780] -    Created 5 batches for processing\n",
      "22:33:23 - INFO - [Thread-25780] - ⚡ Starting thread pool execution...\n",
      "22:33:23 - INFO - [Thread-25780] - 🎯 Submitted 5 batches to 3 threads\n",
      "22:33:23 - INFO - [Thread-24600] - 🚀 Thread 0 starting batch 0 with 20 articles\n",
      "22:33:23 - INFO - [Thread-30948] - 🚀 Thread 1 starting batch 1 with 20 articles\n",
      "22:33:23 - INFO - [Thread-31384] - 🚀 Thread 2 starting batch 2 with 20 articles\n",
      "22:33:23 - INFO - [Thread-31384] - 🚀 Thread 2 starting batch 2 with 20 articles\n",
      "22:33:25 - INFO - [Thread-30948] - ✅ Thread 1 completed batch 1: 20 articles, 4356 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-30948] - 🚀 Thread 0 starting batch 3 with 20 articles\n",
      "22:33:25 - INFO - [Thread-30948] - ✅ Thread 1 completed batch 1: 20 articles, 4356 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-30948] - 🚀 Thread 0 starting batch 3 with 20 articles\n",
      "22:33:25 - INFO - [Thread-24600] - ✅ Thread 0 completed batch 0: 20 articles, 5655 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-24600] - 🚀 Thread 1 starting batch 4 with 20 articles\n",
      "22:33:25 - INFO - [Thread-24600] - ✅ Thread 0 completed batch 0: 20 articles, 5655 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-24600] - 🚀 Thread 1 starting batch 4 with 20 articles\n",
      "22:33:25 - INFO - [Thread-31384] - ✅ Thread 2 completed batch 2: 20 articles, 5743 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-31384] - ✅ Thread 2 completed batch 2: 20 articles, 5743 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-24600] - ✅ Thread 1 completed batch 4: 20 articles, 6193 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-24600] - ✅ Thread 1 completed batch 4: 20 articles, 6193 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-30948] - ✅ Thread 0 completed batch 3: 20 articles, 7749 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - 📈 Progress: 5/5 batches (100.0%)\n",
      "22:33:27 - INFO - [Thread-25780] -    📊 Articles: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] -    💬 Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ⏱️ Time: 4.3s (23.5 articles/sec)\n",
      "22:33:27 - INFO - [Thread-25780] -    ❌ Errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - 🔒 Closing database connections...\n",
      "22:33:27 - INFO - [Thread-25780] - 📝 Logged processing results to database\n",
      "22:33:27 - INFO - [Thread-30948] - ✅ Thread 0 completed batch 3: 20 articles, 7749 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - 📈 Progress: 5/5 batches (100.0%)\n",
      "22:33:27 - INFO - [Thread-25780] -    📊 Articles: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] -    💬 Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ⏱️ Time: 4.3s (23.5 articles/sec)\n",
      "22:33:27 - INFO - [Thread-25780] -    ❌ Errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - 🔒 Closing database connections...\n",
      "22:33:27 - INFO - [Thread-25780] - 📝 Logged processing results to database\n",
      "22:33:27 - INFO - [Thread-25780] - \n",
      "✅ === MULTI-THREADED PROCESSING COMPLETE ===\n",
      "22:33:27 - INFO - [Thread-25780] - 📊 Total articles processed: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] - 💬 Total words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] - ⏱️ Processing time: 4.3 seconds\n",
      "22:33:27 - INFO - [Thread-25780] - 🚀 Articles per second: 23.5\n",
      "22:33:27 - INFO - [Thread-25780] - ❌ Total errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - 📁 Database saved to: output/enhanced_test.sqlite\n",
      "22:33:27 - INFO - [Thread-25780] - 🏁 End time: 2025-08-01 22:33:27\n",
      "22:33:27 - INFO - [Thread-25780] - 📊 Current interrupt status: NORMAL\n",
      "22:33:27 - INFO - [Thread-25780] - 📈 Latest processing status:\n",
      "22:33:27 - INFO - [Thread-25780] -    📊 Articles processed: 100\n",
      "22:33:27 - INFO - [Thread-25780] -    💬 Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ⏰ Last update: 2025-08-01 20:33:27\n",
      "22:33:27 - INFO - [Thread-25780] -    📝 Notes: Multi-threaded processing COMPLETED - 3 threads, 20 batch size, 4.3s, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - \n",
      "✅ === MULTI-THREADED PROCESSING COMPLETE ===\n",
      "22:33:27 - INFO - [Thread-25780] - 📊 Total articles processed: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] - 💬 Total words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] - ⏱️ Processing time: 4.3 seconds\n",
      "22:33:27 - INFO - [Thread-25780] - 🚀 Articles per second: 23.5\n",
      "22:33:27 - INFO - [Thread-25780] - ❌ Total errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - 📁 Database saved to: output/enhanced_test.sqlite\n",
      "22:33:27 - INFO - [Thread-25780] - 🏁 End time: 2025-08-01 22:33:27\n",
      "22:33:27 - INFO - [Thread-25780] - 📊 Current interrupt status: NORMAL\n",
      "22:33:27 - INFO - [Thread-25780] - 📈 Latest processing status:\n",
      "22:33:27 - INFO - [Thread-25780] -    📊 Articles processed: 100\n",
      "22:33:27 - INFO - [Thread-25780] -    💬 Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ⏰ Last update: 2025-08-01 20:33:27\n",
      "22:33:27 - INFO - [Thread-25780] -    📝 Notes: Multi-threaded processing COMPLETED - 3 threads, 20 batch size, 4.3s, 0 errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Enhanced processing complete!\n",
      "📊 Results summary:\n",
      "   articles_processed: 100\n",
      "   words_extracted: 29696\n",
      "   processing_time: 4.25283670425415\n",
      "   articles_per_second: 23.513717303081286\n",
      "   database_path: output/enhanced_test.sqlite\n",
      "   num_threads: 3\n",
      "   total_errors: 0\n",
      "   interrupted: False\n",
      "   batches_completed: 5\n",
      "   batches_total: 5\n",
      "\n",
      "🔍 Testing monitoring functions:\n",
      "\n",
      "💡 To interrupt processing:\n",
      "   - Use Jupyter notebook interrupt button\n",
      "   - Press Ctrl+C in terminal\n",
      "   - Call force_interrupt() function\n",
      "   - Processing will stop gracefully and save progress\n"
     ]
    }
   ],
   "source": [
    "# Test multi-threaded processing with enhanced logging and interrupt handling\n",
    "print(\"🧪 Testing enhanced multi-threaded pipeline...\")\n",
    "\n",
    "# Reset any previous interrupt state and logging\n",
    "reset_interrupt()\n",
    "reset_logging()\n",
    "\n",
    "# Use smaller sample for testing multi-threading\n",
    "SAMPLE_SIZE_MT = 100  # Smaller for testing interrupt functionality\n",
    "\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample for multi-threaded testing\n",
    "sample_df_mt = df.head(SAMPLE_SIZE_MT)\n",
    "print(f\"Created sample with {len(sample_df_mt)} articles for testing\")\n",
    "\n",
    "print(\"\\n📊 Enhanced Features Demo:\")\n",
    "print(\"1. Detailed logging with timestamps and thread IDs\")\n",
    "print(\"2. Interrupt handling (Ctrl+C or notebook interrupt)\")\n",
    "print(\"3. Progress monitoring and error tracking\")\n",
    "print(\"4. Graceful shutdown and cleanup\")\n",
    "\n",
    "print(\"\\n🚀 Starting enhanced multi-threaded processing...\")\n",
    "\n",
    "# Run with detailed logging\n",
    "results = process_articles_multithreaded(\n",
    "    df=sample_df_mt,\n",
    "    nlp_models=nlp,\n",
    "    db_path=\"output/enhanced_test.sqlite\",\n",
    "    batch_size=20,  # Small batches for more frequent updates\n",
    "    num_threads=3,  # Fewer threads for easier monitoring\n",
    "    log_level=logging.INFO  # Detailed logging\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Enhanced processing complete!\")\n",
    "print(f\"📊 Results summary:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Demonstrate monitoring functions\n",
    "print(f\"\\n🔍 Testing monitoring functions:\")\n",
    "check_interrupt_status()\n",
    "monitor_processing_progress(\"output/enhanced_test.sqlite\")\n",
    "\n",
    "print(f\"\\n💡 To interrupt processing:\")\n",
    "print(\"   - Use Jupyter notebook interrupt button\")\n",
    "print(\"   - Press Ctrl+C in terminal\")\n",
    "print(\"   - Call force_interrupt() function\")\n",
    "print(\"   - Processing will stop gracefully and save progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca8c6",
   "metadata": {},
   "source": [
    "## Performance Optimization Guide\n",
    "\n",
    "### Multi-threading Configuration\n",
    "\n",
    "**Optimal Thread Count:**\n",
    "- **CPU-bound tasks**: Use `CPU cores - 2` threads (leaves cores for OS)\n",
    "- **Memory-bound tasks**: Use `CPU cores` or slightly more\n",
    "- **For this workload**: Start with 4-6 threads, adjust based on performance\n",
    "\n",
    "**Batch Size Recommendations:**\n",
    "- **Small batches (100-200)**: Better memory usage, more frequent database commits\n",
    "- **Large batches (500-1000)**: Less database overhead, higher memory usage\n",
    "- **Recommended**: 500 articles per batch for balanced performance\n",
    "\n",
    "**Performance Tips:**\n",
    "1. **Monitor system resources** during processing\n",
    "2. **Reduce threads** if CPU usage is maxed out\n",
    "3. **Increase batch size** if I/O is the bottleneck\n",
    "4. **Use SSD storage** for database for faster writes\n",
    "5. **Close other applications** to free up memory\n",
    "\n",
    "**Expected Performance:**\n",
    "- Single-threaded: ~50-100 articles/second\n",
    "- Multi-threaded (4 cores): ~200-400 articles/second\n",
    "- Multi-threaded (8 cores): ~400-600 articles/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69bf05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️  SYSTEM ANALYSIS FOR MULTI-THREADING\n",
      "==================================================\n",
      "CPU Cores: 24\n",
      "CPU Frequency: 3701 MHz (max: 3701 MHz)\n",
      "Total RAM: 31.9 GB\n",
      "Available RAM: 13.1 GB\n",
      "Memory Usage: 59.1%\n",
      "Output Drive Free Space: 288.3 GB\n",
      "\n",
      "🎯 RECOMMENDED CONFIGURATION:\n",
      "========================================\n",
      "Recommended Threads: 8\n",
      "Recommended Batch Size: 500\n",
      "Estimated Memory Usage: 4.0 GB\n",
      "\n",
      "💡 Usage Example:\n",
      "```python\n",
      "results = process_articles_multithreaded(\n",
      "    df=df,\n",
      "    nlp_models=nlp,\n",
      "    db_path='output/dutch_words_full.sqlite',\n",
      "    batch_size=500,\n",
      "    num_threads=8\n",
      ")\n",
      "```\n",
      "\n",
      "⏱️  Estimated Processing Time:\n",
      "~12 minutes for 295,000 articles\n",
      "(400 articles/second estimated)\n"
     ]
    }
   ],
   "source": [
    "# System Analysis for Optimal Thread Configuration\n",
    "import psutil\n",
    "import multiprocessing\n",
    "\n",
    "print(\"🖥️  SYSTEM ANALYSIS FOR MULTI-THREADING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CPU Information\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    print(f\"CPU Frequency: {cpu_freq.current:.0f} MHz (max: {cpu_freq.max:.0f} MHz)\")\n",
    "\n",
    "# Memory Information\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"Memory Usage: {memory.percent}%\")\n",
    "\n",
    "# Disk Information (for database storage)\n",
    "try:\n",
    "    disk = psutil.disk_usage('output')\n",
    "    print(f\"Output Drive Free Space: {disk.free / (1024**3):.1f} GB\")\n",
    "except:\n",
    "    print(\"Could not check output drive space\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED CONFIGURATION:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate recommendations\n",
    "recommended_threads = max(2, min(8, cpu_count - 1))\n",
    "recommended_batch_size = 500\n",
    "\n",
    "if memory.available / (1024**3) < 4:  # Less than 4GB available\n",
    "    recommended_batch_size = 250\n",
    "    recommended_threads = max(2, recommended_threads - 1)\n",
    "    print(\"⚠️  Limited memory detected - reducing batch size and threads\")\n",
    "\n",
    "print(f\"Recommended Threads: {recommended_threads}\")\n",
    "print(f\"Recommended Batch Size: {recommended_batch_size}\")\n",
    "\n",
    "# Memory estimate\n",
    "estimated_memory_per_thread = 0.5  # GB per thread (rough estimate)\n",
    "total_estimated_memory = recommended_threads * estimated_memory_per_thread\n",
    "\n",
    "print(f\"Estimated Memory Usage: {total_estimated_memory:.1f} GB\")\n",
    "\n",
    "if total_estimated_memory > memory.available / (1024**3) * 0.8:\n",
    "    print(\"⚠️  WARNING: Estimated memory usage is high. Consider reducing threads.\")\n",
    "    recommended_threads = max(2, int(memory.available / (1024**3) * 0.8 / estimated_memory_per_thread))\n",
    "    print(f\"Adjusted recommendation: {recommended_threads} threads\")\n",
    "\n",
    "print(f\"\\n💡 Usage Example:\")\n",
    "print(\"```python\")\n",
    "print(\"results = process_articles_multithreaded(\")\n",
    "print(\"    df=df,\")\n",
    "print(\"    nlp_models=nlp,\")\n",
    "print(\"    db_path='output/dutch_words_full.sqlite',\")\n",
    "print(f\"    batch_size={recommended_batch_size},\")\n",
    "print(f\"    num_threads={recommended_threads}\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "\n",
    "print(f\"\\n⏱️  Estimated Processing Time:\")\n",
    "articles_per_second = recommended_threads * 50  # Rough estimate\n",
    "total_articles = 295000  # Approximate article count\n",
    "estimated_time = total_articles / articles_per_second\n",
    "print(f\"~{estimated_time/60:.0f} minutes for {total_articles:,} articles\")\n",
    "print(f\"({articles_per_second} articles/second estimated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f04c2",
   "metadata": {},
   "source": [
    "## 🛑 Interrupt Handling Guide\n",
    "\n",
    "### How to Safely Stop Processing\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "1. Click the **\"Interrupt\"** button in the toolbar\n",
    "2. Processing will stop gracefully after current batches complete\n",
    "3. Partial results are automatically saved to the database\n",
    "\n",
    "**Terminal/Command Line:**\n",
    "1. Press **Ctrl+C** (Windows/Linux) or **Cmd+C** (Mac)\n",
    "2. Processing will stop gracefully\n",
    "3. All completed work is preserved\n",
    "\n",
    "**Programmatically:**\n",
    "```python\n",
    "# Force interrupt from code\n",
    "force_interrupt()\n",
    "\n",
    "# Check if processing was interrupted\n",
    "if check_interrupt_status():\n",
    "    print(\"Processing is currently interrupted\")\n",
    "\n",
    "# Reset to allow new processing\n",
    "reset_interrupt()\n",
    "```\n",
    "\n",
    "### 📊 Monitoring Progress\n",
    "\n",
    "**Real-time Monitoring:**\n",
    "- Detailed logs show progress every 5 completed batches\n",
    "- Thread-level logging shows individual thread progress\n",
    "- Error tracking and timeout handling\n",
    "\n",
    "**Database Monitoring:**\n",
    "```python\n",
    "# Check latest progress\n",
    "monitor_processing_progress(\"output/dutch_words_full.sqlite\")\n",
    "\n",
    "# Analyze partial results\n",
    "analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "```\n",
    "\n",
    "### 🔄 Resuming Interrupted Processing\n",
    "\n",
    "**Safe Resume Process:**\n",
    "1. Check interrupt status: `check_interrupt_status()`\n",
    "2. Reset interrupt flag: `reset_interrupt()`\n",
    "3. Re-run the processing cell\n",
    "4. Processing will continue where it left off (database handles duplicates)\n",
    "\n",
    "### ⚡ Performance vs Safety Balance\n",
    "\n",
    "**Trade-offs:**\n",
    "- **More frequent saves**: Slower processing, safer against data loss\n",
    "- **Larger batches**: Faster processing, more data loss risk if interrupted\n",
    "- **More threads**: Higher resource usage, harder to interrupt cleanly\n",
    "\n",
    "**Recommendations:**\n",
    "- Use **500 article batches** for optimal balance\n",
    "- Set **num_threads = CPU_cores - 2** to leave resources for interrupts\n",
    "- Monitor system resources during processing\n",
    "- Always test interrupt functionality with small samples first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87367c7b",
   "metadata": {},
   "source": [
    "## Step 6: Analysis and Export Functions\n",
    "\n",
    "Define the analysis and export functions that will be used to analyze the word database and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb82429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis and export functions defined and ready for use.\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"output/words_database.sqlite\", output_dir=\"output/exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\t{freq}\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\t{freq}\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\n✅ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "print(\"Analysis and export functions defined and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db965c",
   "metadata": {},
   "source": [
    "## Step 7: Test Processing with Small Sample\n",
    "\n",
    "Test the processing pipeline with a small sample of articles to verify everything works correctly before running on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc128f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing word extraction pipeline with small sample...\n",
      "Sample size: 100 articles\n",
      "Created sample with 100 articles\n",
      "\n",
      "Starting sample processing...\n",
      "Database setup complete: output/test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 1/4 [00:01<00:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25/100 articles, extracted 6,787 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: output/test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "✅ Sample processing complete!\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'output/test_dutch_words.sqlite'}\n",
      "\n",
      "📊 Quick analysis of sample results:\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 59,392\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 3,724 times\n",
      "   2. in (in) [preposition] - 1,764 times\n",
      "   3. van (van) [preposition] - 1,704 times\n",
      "   4. een (een) [determiner] - 1,560 times\n",
      "   5. het (het) [determiner] - 1,384 times\n",
      "   6. en (en) [conjunction] - 1,032 times\n",
      "   7. is (zijn) [auxiliary] - 860 times\n",
      "   8. op (op) [preposition] - 784 times\n",
      "   9. met (met) [preposition] - 556 times\n",
      "  10. voor (voor) [preposition] - 496 times\n",
      "  11. er (er) [adverb] - 462 times\n",
      "  12. het (het) [pronoun] - 396 times\n",
      "  13. te (te) [preposition] - 386 times\n",
      "  14. dat (dat) [conjunction] - 386 times\n",
      "  15. niet (niet) [adverb] - 382 times\n",
      "  16. zijn (zijn) [auxiliary] - 372 times\n",
      "  17. hij (hij) [pronoun] - 368 times\n",
      "  18. bij (bij) [preposition] - 360 times\n",
      "  19. jaar (jaar) [noun] - 358 times\n",
      "  20. die (die) [pronoun] - 326 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 5.2)\n",
      "  verb: 1,349 words (avg freq: 5.1)\n",
      "  proper_noun: 1,225 words (avg freq: 4.6)\n",
      "  adjective: 752 words (avg freq: 6.0)\n",
      "  adverb: 181 words (avg freq: 22.3)\n",
      "  other: 83 words (avg freq: 3.1)\n",
      "  preposition: 66 words (avg freq: 136.1)\n",
      "  pronoun: 65 words (avg freq: 49.3)\n",
      "  determiner: 51 words (avg freq: 147.8)\n",
      "  number: 45 words (avg freq: 15.8)\n",
      "  auxiliary: 37 words (avg freq: 84.7)\n",
      "  conjunction: 33 words (avg freq: 71.6)\n",
      "  interjection: 3 words (avg freq: 2.7)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 59,392 total instances\n",
      "\n",
      "📁 Creating sample exports...\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 874 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 723 words to game_words.txt\n",
      "\n",
      "✅ All exports completed in: output/test_exports\n",
      "\n",
      "✅ Sample testing completed successfully!\n",
      "Ready to process full dataset in the next step.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the processing pipeline with a small sample first\n",
    "SAMPLE_SIZE = 100  # Number of articles to test with\n",
    "\n",
    "print(\"🧪 Testing word extraction pipeline with small sample...\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} articles\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample dataset\n",
    "sample_df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Created sample with {len(sample_df)} articles\")\n",
    "\n",
    "# Run the processing pipeline on sample\n",
    "print(\"\\nStarting sample processing...\")\n",
    "test_results = process_articles_pipeline(\n",
    "    df=sample_df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/test_dutch_words.sqlite\",\n",
    "    batch_size=25  # Small batches for testing\n",
    ")\n",
    "\n",
    "print(\"✅ Sample processing complete!\")\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Quick analysis of test results\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    print(\"\\n📊 Quick analysis of sample results:\")\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    \n",
    "    # Export sample results\n",
    "    print(\"\\n📁 Creating sample exports...\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "    \n",
    "    print(f\"\\n✅ Sample testing completed successfully!\")\n",
    "    print(f\"Ready to process full dataset in the next step.\")\n",
    "else:\n",
    "    print(\"❌ Test database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867572",
   "metadata": {},
   "source": [
    "## Step 8: Full Dataset Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!\n",
    "\n",
    "Run this step only after successfully testing with the sample in Step 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:34 - INFO - [Thread-25780] - 🔄 Interrupt flag reset - ready for new processing\n",
      "22:33:34 - INFO - [Thread-25780] - 🚀 Starting multi-threaded processing...\n",
      "22:33:34 - INFO - [Thread-25780] - 📊 Configuration:\n",
      "22:33:34 - INFO - [Thread-25780] -    Articles to process: 295,259\n",
      "22:33:34 - INFO - [Thread-25780] -    Batch size: 500\n",
      "22:33:34 - INFO - [Thread-25780] - 🚀 Starting multi-threaded processing...\n",
      "22:33:34 - INFO - [Thread-25780] - 📊 Configuration:\n",
      "22:33:34 - INFO - [Thread-25780] -    Articles to process: 295,259\n",
      "22:33:34 - INFO - [Thread-25780] -    Batch size: 500\n",
      "22:33:34 - INFO - [Thread-25780] -    Number of threads: 8\n",
      "22:33:34 - INFO - [Thread-25780] -    Database path: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:34\n",
      "22:33:34 - INFO - [Thread-25780] - 🗄️ Setting up database...\n",
      "22:33:34 - INFO - [Thread-25780] -    Number of threads: 8\n",
      "22:33:34 - INFO - [Thread-25780] -    Database path: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:34\n",
      "22:33:34 - INFO - [Thread-25780] - 🗄️ Setting up database...\n",
      "22:33:34 - INFO - [Thread-25780] - 📁 Initialized thread-safe database: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] - 🧠 Loading spaCy models for threads...\n",
      "22:33:34 - INFO - [Thread-25780] - 📁 Initialized thread-safe database: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] - 🧠 Loading spaCy models for threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting ENHANCED MULTI-THREADED processing of ALL articles...\n",
      "⚡ Features: Detailed logging, interrupt handling, progress monitoring\n",
      "🛑 You can interrupt processing safely with Ctrl+C or notebook interrupt\n",
      "Processing 295,259 articles with enhanced multi-threading...\n",
      "💡 System recommendations:\n",
      "   CPU cores: 24\n",
      "   Available memory: 13.1 GB\n",
      "   Recommended threads: 8\n",
      "Database setup complete: output/dutch_words_full.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:35 - INFO - [Thread-25780] -    ✅ Thread 1 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ✅ Thread 2 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ✅ Thread 2 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ✅ Thread 3 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ✅ Thread 3 model loaded\n",
      "22:33:37 - INFO - [Thread-25780] -    ✅ Thread 4 model loaded\n",
      "22:33:37 - INFO - [Thread-25780] -    ✅ Thread 4 model loaded\n",
      "22:33:38 - INFO - [Thread-25780] -    ✅ Thread 5 model loaded\n",
      "22:33:38 - INFO - [Thread-25780] -    ✅ Thread 5 model loaded\n",
      "22:33:39 - INFO - [Thread-25780] -    ✅ Thread 6 model loaded\n",
      "22:33:39 - INFO - [Thread-25780] -    ✅ Thread 6 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ✅ Thread 7 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ✅ Thread 7 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ✅ Thread 8 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] - 📦 Creating batches...\n",
      "22:33:40 - INFO - [Thread-25780] -    ✅ Thread 8 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] - 📦 Creating batches...\n",
      "22:33:41 - INFO - [Thread-25780] -    Created 591 batches for processing\n",
      "22:33:41 - INFO - [Thread-25780] - ⚡ Starting thread pool execution...\n",
      "22:33:41 - INFO - [Thread-25780] - 🎯 Submitted 591 batches to 8 threads\n",
      "22:33:41 - INFO - [Thread-30364] - 🚀 Thread 0 starting batch 0 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28564] - 🚀 Thread 1 starting batch 1 with 500 articles\n",
      "22:33:41 - INFO - [Thread-25780] -    Created 591 batches for processing\n",
      "22:33:41 - INFO - [Thread-25780] - ⚡ Starting thread pool execution...\n",
      "22:33:41 - INFO - [Thread-25780] - 🎯 Submitted 591 batches to 8 threads\n",
      "22:33:41 - INFO - [Thread-30364] - 🚀 Thread 0 starting batch 0 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28564] - 🚀 Thread 1 starting batch 1 with 500 articles\n",
      "22:33:41 - INFO - [Thread-33516] - 🚀 Thread 2 starting batch 2 with 500 articles\n",
      "22:33:41 - INFO - [Thread-30668] - 🚀 Thread 3 starting batch 3 with 500 articles\n",
      "22:33:41 - INFO - [Thread-17896] - 🚀 Thread 4 starting batch 4 with 500 articles\n",
      "22:33:41 - INFO - [Thread-33516] - 🚀 Thread 2 starting batch 2 with 500 articles\n",
      "22:33:41 - INFO - [Thread-30668] - 🚀 Thread 3 starting batch 3 with 500 articles\n",
      "22:33:41 - INFO - [Thread-17896] - 🚀 Thread 4 starting batch 4 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28624] - 🚀 Thread 5 starting batch 5 with 500 articles\n",
      "22:33:41 - INFO - [Thread-23248] - 🚀 Thread 6 starting batch 6 with 500 articles\n",
      "22:33:41 - INFO - [Thread-31704] - 🚀 Thread 7 starting batch 7 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28624] - 🚀 Thread 5 starting batch 5 with 500 articles\n",
      "22:33:41 - INFO - [Thread-23248] - 🚀 Thread 6 starting batch 6 with 500 articles\n",
      "22:33:41 - INFO - [Thread-31704] - 🚀 Thread 7 starting batch 7 with 500 articles\n",
      "22:36:52 - INFO - [Thread-31704] - ✅ Thread 7 completed batch 7: 500 articles, 137955 words, 0 errors\n",
      "22:36:52 - INFO - [Thread-31704] - 🚀 Thread 0 starting batch 8 with 500 articles\n",
      "22:36:52 - INFO - [Thread-31704] - ✅ Thread 7 completed batch 7: 500 articles, 137955 words, 0 errors\n",
      "22:36:52 - INFO - [Thread-31704] - 🚀 Thread 0 starting batch 8 with 500 articles\n",
      "22:36:53 - INFO - [Thread-23248] - ✅ Thread 6 completed batch 6: 500 articles, 130145 words, 0 errors\n",
      "22:36:53 - INFO - [Thread-23248] - 🚀 Thread 1 starting batch 9 with 500 articles\n",
      "22:36:53 - INFO - [Thread-23248] - ✅ Thread 6 completed batch 6: 500 articles, 130145 words, 0 errors\n",
      "22:36:53 - INFO - [Thread-23248] - 🚀 Thread 1 starting batch 9 with 500 articles\n",
      "22:36:54 - INFO - [Thread-17896] - ✅ Thread 4 completed batch 4: 500 articles, 140461 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-17896] - 🚀 Thread 2 starting batch 10 with 500 articles\n",
      "22:36:54 - INFO - [Thread-17896] - ✅ Thread 4 completed batch 4: 500 articles, 140461 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-17896] - 🚀 Thread 2 starting batch 10 with 500 articles\n",
      "22:36:54 - INFO - [Thread-30668] - ✅ Thread 3 completed batch 3: 500 articles, 137438 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-30668] - 🚀 Thread 3 starting batch 11 with 500 articles\n",
      "22:36:54 - INFO - [Thread-30668] - ✅ Thread 3 completed batch 3: 500 articles, 137438 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-30668] - 🚀 Thread 3 starting batch 11 with 500 articles\n",
      "22:36:55 - INFO - [Thread-33516] - ✅ Thread 2 completed batch 2: 500 articles, 140353 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-33516] - 🚀 Thread 4 starting batch 12 with 500 articles\n",
      "22:36:55 - INFO - [Thread-25780] - 📈 Progress: 5/591 batches (0.8%)\n",
      "22:36:55 - INFO - [Thread-25780] -    📊 Articles: 2,500/295,259\n",
      "22:36:55 - INFO - [Thread-25780] -    💬 Words extracted: 686,352\n",
      "22:36:55 - INFO - [Thread-25780] -    ⏱️ Time: 194.4s (12.9 articles/sec)\n",
      "22:36:55 - INFO - [Thread-25780] -    ❌ Errors: 0\n",
      "22:36:55 - INFO - [Thread-33516] - ✅ Thread 2 completed batch 2: 500 articles, 140353 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-33516] - 🚀 Thread 4 starting batch 12 with 500 articles\n",
      "22:36:55 - INFO - [Thread-25780] - 📈 Progress: 5/591 batches (0.8%)\n",
      "22:36:55 - INFO - [Thread-25780] -    📊 Articles: 2,500/295,259\n",
      "22:36:55 - INFO - [Thread-25780] -    💬 Words extracted: 686,352\n",
      "22:36:55 - INFO - [Thread-25780] -    ⏱️ Time: 194.4s (12.9 articles/sec)\n",
      "22:36:55 - INFO - [Thread-25780] -    ❌ Errors: 0\n",
      "22:36:55 - INFO - [Thread-30364] - ✅ Thread 0 completed batch 0: 500 articles, 139591 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-30364] - 🚀 Thread 5 starting batch 13 with 500 articles\n",
      "22:36:55 - INFO - [Thread-30364] - ✅ Thread 0 completed batch 0: 500 articles, 139591 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-30364] - 🚀 Thread 5 starting batch 13 with 500 articles\n",
      "22:36:57 - INFO - [Thread-28624] - ✅ Thread 5 completed batch 5: 500 articles, 140752 words, 0 errors\n",
      "22:36:57 - INFO - [Thread-28624] - 🚀 Thread 6 starting batch 14 with 500 articles\n",
      "22:36:57 - INFO - [Thread-28624] - ✅ Thread 5 completed batch 5: 500 articles, 140752 words, 0 errors\n",
      "22:36:57 - INFO - [Thread-28624] - 🚀 Thread 6 starting batch 14 with 500 articles\n",
      "22:36:59 - INFO - [Thread-28564] - ✅ Thread 1 completed batch 1: 500 articles, 149269 words, 0 errors\n",
      "22:36:59 - INFO - [Thread-28564] - 🚀 Thread 7 starting batch 15 with 500 articles\n",
      "22:36:59 - INFO - [Thread-28564] - ✅ Thread 1 completed batch 1: 500 articles, 149269 words, 0 errors\n",
      "22:36:59 - INFO - [Thread-28564] - 🚀 Thread 7 starting batch 15 with 500 articles\n"
     ]
    }
   ],
   "source": [
    "# Process the full dataset with enhanced multi-threading (logging + interrupt handling)\n",
    "print(\"🚀 Starting ENHANCED MULTI-THREADED processing of ALL articles...\")\n",
    "print(\"⚡ Features: Detailed logging, interrupt handling, progress monitoring\")\n",
    "print(\"🛑 You can interrupt processing safely with Ctrl+C or notebook interrupt\")\n",
    "\n",
    "# Reset interrupt flag\n",
    "reset_interrupt()\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "print(f\"Processing {len(df):,} articles with enhanced multi-threading...\")\n",
    "\n",
    "# Get system recommendations\n",
    "import psutil\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "memory = psutil.virtual_memory()\n",
    "recommended_threads = max(2, min(8, cpu_count - 1))\n",
    "\n",
    "if memory.available / (1024**3) < 4:  # Less than 4GB available\n",
    "    recommended_threads = max(2, recommended_threads - 1)\n",
    "\n",
    "print(f\"💡 System recommendations:\")\n",
    "print(f\"   CPU cores: {cpu_count}\")\n",
    "print(f\"   Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"   Recommended threads: {recommended_threads}\")\n",
    "\n",
    "# Run the ENHANCED MULTI-THREADED processing pipeline\n",
    "try:\n",
    "    results = process_articles_multithreaded(\n",
    "        df=df,\n",
    "        nlp_models=nlp,  # Will create copies for each thread\n",
    "        db_path=\"output/dutch_words_full.sqlite\",\n",
    "        batch_size=500,  # Optimized batch size\n",
    "        num_threads=recommended_threads,  # Use system recommendations\n",
    "        log_level=logging.INFO  # Detailed logging\n",
    "    )\n",
    "    \n",
    "    status = \"INTERRUPTED\" if results.get('interrupted', False) else \"COMPLETED\"\n",
    "    print(f\"\\n{'⚠️' if results.get('interrupted') else '✅'} FULL PROCESSING {status}!\")\n",
    "    \n",
    "    print(f\"\\n📊 Final Results:\")\n",
    "    print(f\"   Articles processed: {results['articles_processed']:,}\")\n",
    "    print(f\"   Words extracted: {results['words_extracted']:,}\")\n",
    "    print(f\"   Processing time: {results['processing_time']:.1f} seconds\")\n",
    "    print(f\"   Articles per second: {results['articles_per_second']:.1f}\")\n",
    "    print(f\"   Errors encountered: {results['total_errors']}\")\n",
    "    print(f\"   Batches completed: {results['batches_completed']}/{results['batches_total']}\")\n",
    "    \n",
    "    if results.get('interrupted'):\n",
    "        print(f\"\\n⚠️ Processing was interrupted but data has been saved.\")\n",
    "        print(f\"📄 You can resume processing by running this cell again.\")\n",
    "        print(f\"📊 Progress monitoring available with: monitor_processing_progress()\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n🛑 Processing interrupted by user\")\n",
    "    print(f\"📁 Partial results saved to database\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during processing: {e}\")\n",
    "    logger.error(f\"Processing failed: {e}\")\n",
    "\n",
    "# Always run analysis if database exists\n",
    "if os.path.exists(\"output/dutch_words_full.sqlite\"):\n",
    "    print(f\"\\n📊 Database analysis:\")\n",
    "    try:\n",
    "        analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "        \n",
    "        # Export results if processing completed successfully\n",
    "        if 'results' in locals() and not results.get('interrupted', False):\n",
    "            print(f\"\\n📁 Creating production exports...\")\n",
    "            export_word_lists(\"output/dutch_words_full.sqlite\", \"output/exports\")\n",
    "            print(f\"\\n🎉 COMPLETE! Full Dutch word database created successfully!\")\n",
    "        else:\n",
    "            print(f\"\\n💾 Partial results available in database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis: {e}\")\n",
    "else:\n",
    "    print(f\"❌ Database file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852156c4",
   "metadata": {},
   "source": [
    "## Step 8: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
