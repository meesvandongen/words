{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ada48a",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This notebook outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847371cf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and other necessary libraries for the word extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f498d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866e8e6",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file for word extraction processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa9299",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0d50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ spacy installed successfully\n",
      "✓ spacy installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ Dutch language model downloaded successfully\n",
      "✓ Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"✓ Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b84080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"✓ Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"✗ Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8888f",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc25ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82f065",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943fb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c7e61",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b90d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: output/test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"output/test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b917",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69511f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"output/words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db965c",
   "metadata": {},
   "source": [
    "## Step 6: Test Processing with Small Sample\n",
    "\n",
    "Test the processing pipeline with a small sample of articles to verify everything works correctly before running on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc128f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing word extraction pipeline with small sample...\n",
      "Sample size: 100 articles\n",
      "Created sample with 100 articles\n",
      "\n",
      "Starting sample processing...\n",
      "Database setup complete: output/test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25/100 articles, extracted 6,787 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: output/test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "✅ Sample processing complete!\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'output/test_dutch_words.sqlite'}\n",
      "\n",
      "📊 Quick analysis of sample results:\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 59,392\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 3,724 times\n",
      "   2. in (in) [preposition] - 1,764 times\n",
      "   3. van (van) [preposition] - 1,704 times\n",
      "   4. een (een) [determiner] - 1,560 times\n",
      "   5. het (het) [determiner] - 1,384 times\n",
      "   6. en (en) [conjunction] - 1,032 times\n",
      "   7. is (zijn) [auxiliary] - 860 times\n",
      "   8. op (op) [preposition] - 784 times\n",
      "   9. met (met) [preposition] - 556 times\n",
      "  10. voor (voor) [preposition] - 496 times\n",
      "  11. er (er) [adverb] - 462 times\n",
      "  12. het (het) [pronoun] - 396 times\n",
      "  13. dat (dat) [conjunction] - 386 times\n",
      "  14. te (te) [preposition] - 386 times\n",
      "  15. niet (niet) [adverb] - 382 times\n",
      "  16. zijn (zijn) [auxiliary] - 372 times\n",
      "  17. hij (hij) [pronoun] - 368 times\n",
      "  18. bij (bij) [preposition] - 360 times\n",
      "  19. jaar (jaar) [noun] - 358 times\n",
      "  20. die (die) [pronoun] - 326 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 5.2)\n",
      "  verb: 1,349 words (avg freq: 5.1)\n",
      "  proper_noun: 1,225 words (avg freq: 4.6)\n",
      "  adjective: 752 words (avg freq: 6.0)\n",
      "  adverb: 181 words (avg freq: 22.3)\n",
      "  other: 83 words (avg freq: 3.1)\n",
      "  preposition: 66 words (avg freq: 136.1)\n",
      "  pronoun: 65 words (avg freq: 49.3)\n",
      "  determiner: 51 words (avg freq: 147.8)\n",
      "  number: 45 words (avg freq: 15.8)\n",
      "  auxiliary: 37 words (avg freq: 84.7)\n",
      "  conjunction: 33 words (avg freq: 71.6)\n",
      "  interjection: 3 words (avg freq: 2.7)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 59,392 total instances\n",
      "\n",
      "📁 Creating sample exports...\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 874 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 723 words to game_words.txt\n",
      "\n",
      "✅ All exports completed in: output/test_exports\n",
      "\n",
      "✅ Sample testing completed successfully!\n",
      "Ready to process full dataset in the next step.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the processing pipeline with a small sample first\n",
    "SAMPLE_SIZE = 100  # Number of articles to test with\n",
    "\n",
    "print(\"🧪 Testing word extraction pipeline with small sample...\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} articles\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample dataset\n",
    "sample_df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Created sample with {len(sample_df)} articles\")\n",
    "\n",
    "# Run the processing pipeline on sample\n",
    "print(\"\\nStarting sample processing...\")\n",
    "test_results = process_articles_pipeline(\n",
    "    df=sample_df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/test_dutch_words.sqlite\",\n",
    "    batch_size=25  # Small batches for testing\n",
    ")\n",
    "\n",
    "print(\"✅ Sample processing complete!\")\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Quick analysis of test results\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    print(\"\\n📊 Quick analysis of sample results:\")\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    \n",
    "    # Export sample results\n",
    "    print(\"\\n📁 Creating sample exports...\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "    \n",
    "    print(f\"\\n✅ Sample testing completed successfully!\")\n",
    "    print(f\"Ready to process full dataset in the next step.\")\n",
    "else:\n",
    "    print(\"❌ Test database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867572",
   "metadata": {},
   "source": [
    "## Step 7: Full Dataset Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!\n",
    "\n",
    "Run this step only after successfully testing with the sample in Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4af9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting full processing of ALL articles...\n",
      "⚠️  This may take several hours depending on your system.\n",
      "You can monitor progress and stop if needed.\n",
      "Processing 295,259 articles...\n",
      "Database setup complete: output/dutch_words_full.sqlite\n",
      "Starting processing of 295,259 articles...\n",
      "Batch size: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/591 [00:36<6:02:38, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/295,259 articles, extracted 139,591 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▏         | 8/591 [05:14<6:22:09, 39.33s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m articles...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the processing pipeline on full dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m results = \u001b[43mprocess_articles_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnlp_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput/dutch_words_full.sqlite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optimized batch size for full processing\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ FULL PROCESSING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mprocess_articles_pipeline\u001b[39m\u001b[34m(df, nlp_model, db_path, batch_size)\u001b[39m\n\u001b[32m     61\u001b[39m combined_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(all_text)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combined_text:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Extract words\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     words = \u001b[43mextract_words_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m words:\n\u001b[32m     68\u001b[39m         \u001b[38;5;66;03m# Insert into database\u001b[39;00m\n\u001b[32m     69\u001b[39m         insert_word_data(conn, words, year)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mextract_words_from_text\u001b[39m\u001b[34m(text, nlp_model)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Process text with spaCy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     doc = \u001b[43mnlp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     words = []\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# Filter out unwanted tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process the full dataset\n",
    "print(\"🚀 Starting full processing of ALL articles...\")\n",
    "print(\"⚠️  This may take several hours depending on your system.\")\n",
    "print(\"You can monitor progress and stop if needed.\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "print(f\"Processing {len(df):,} articles...\")\n",
    "\n",
    "# Run the processing pipeline on full dataset\n",
    "results = process_articles_pipeline(\n",
    "    df=df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/dutch_words_full.sqlite\",\n",
    "    batch_size=500  # Optimized batch size for full processing\n",
    ")\n",
    "\n",
    "print(\"✅ FULL PROCESSING COMPLETE!\")\n",
    "print(f\"Results: {results}\")\n",
    "\n",
    "# Comprehensive analysis of full results\n",
    "if os.path.exists(\"output/dutch_words_full.sqlite\"):\n",
    "    print(\"\\n📊 Full database analysis:\")\n",
    "    analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "    \n",
    "    # Export full results\n",
    "    print(\"\\n📁 Creating production exports...\")\n",
    "    export_word_lists(\"output/dutch_words_full.sqlite\", \"output/exports\")\n",
    "    \n",
    "    print(f\"\\n🎉 COMPLETE! Full Dutch word database created successfully!\")\n",
    "else:\n",
    "    print(\"❌ Full database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852156c4",
   "metadata": {},
   "source": [
    "## Step 8: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8578258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing analysis functions...\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 29,696\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 1,862 times\n",
      "   2. in (in) [preposition] - 882 times\n",
      "   3. van (van) [preposition] - 852 times\n",
      "   4. een (een) [determiner] - 780 times\n",
      "   5. het (het) [determiner] - 692 times\n",
      "   6. en (en) [conjunction] - 516 times\n",
      "   7. is (zijn) [auxiliary] - 430 times\n",
      "   8. op (op) [preposition] - 392 times\n",
      "   9. met (met) [preposition] - 278 times\n",
      "  10. voor (voor) [preposition] - 248 times\n",
      "  11. er (er) [adverb] - 231 times\n",
      "  12. het (het) [pronoun] - 198 times\n",
      "  13. dat (dat) [conjunction] - 193 times\n",
      "  14. te (te) [preposition] - 193 times\n",
      "  15. niet (niet) [adverb] - 191 times\n",
      "  16. zijn (zijn) [auxiliary] - 186 times\n",
      "  17. hij (hij) [pronoun] - 184 times\n",
      "  18. bij (bij) [preposition] - 180 times\n",
      "  19. jaar (jaar) [noun] - 179 times\n",
      "  20. die (die) [pronoun] - 163 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 2.6)\n",
      "  verb: 1,349 words (avg freq: 2.6)\n",
      "  proper_noun: 1,225 words (avg freq: 2.3)\n",
      "  adjective: 752 words (avg freq: 3.0)\n",
      "  adverb: 181 words (avg freq: 11.2)\n",
      "  other: 83 words (avg freq: 1.6)\n",
      "  preposition: 66 words (avg freq: 68.0)\n",
      "  pronoun: 65 words (avg freq: 24.7)\n",
      "  determiner: 51 words (avg freq: 73.9)\n",
      "  number: 45 words (avg freq: 7.9)\n",
      "  auxiliary: 37 words (avg freq: 42.4)\n",
      "  conjunction: 33 words (avg freq: 35.8)\n",
      "  interjection: 3 words (avg freq: 1.3)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 29,696 total instances\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 381 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 403 words to game_words.txt\n",
      "\n",
      "✅ All exports completed in: output/test_exports\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"output/words_database.sqlite\", output_dir=\"output/exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\t{freq}\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\t{freq}\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\n✅ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "# Test analysis (will work with existing test database)\n",
    "print(\"Testing analysis functions...\")\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "else:\n",
    "    print(\"No test database found. Run the processing steps first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
