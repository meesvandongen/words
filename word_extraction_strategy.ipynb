{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ada48a",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This notebook outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847371cf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and other necessary libraries for the word extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f498d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866e8e6",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file for word extraction processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa9299",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0d50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ spacy installed successfully\n",
      "✓ spacy installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ Dutch language model downloaded successfully\n",
      "✓ Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"✓ Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b84080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"✓ Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"✗ Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8888f",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc25ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82f065",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943fb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c7e61",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b90d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:18:59')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:18:59')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:18:59')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b917",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69511f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db965c",
   "metadata": {},
   "source": [
    "## Step 6: Run the Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc128f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Full processing is disabled for safety.\n",
      "Set RUN_FULL_PROCESSING = True to enable full processing.\n",
      "\n",
      "📊 Alternative: Run a test with a small sample:\n",
      "\n",
      "🧪 Running test with 100 articles...\n",
      "Database setup complete: test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|█████     | 1/2 [00:03<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 articles, extracted 13,013 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'test_dutch_words.sqlite'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAFETY CHECK: Only run if you're ready to process all articles\n",
    "RUN_FULL_PROCESSING = False  # Set to True when ready to run\n",
    "\n",
    "if RUN_FULL_PROCESSING:\n",
    "    print(\"🚀 Starting full processing of all articles...\")\n",
    "    print(\"This may take several hours depending on your system.\")\n",
    "    print(\"You can monitor progress and stop if needed.\")\n",
    "    \n",
    "    # Load the full dataset if not already loaded\n",
    "    if 'df' not in locals() or df is None:\n",
    "        print(\"Loading dataset...\")\n",
    "        df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "        print(f\"Dataset loaded: {df.shape}\")\n",
    "    \n",
    "    # Check if spaCy model is loaded\n",
    "    if nlp is None:\n",
    "        print(\"Error: Dutch spaCy model not loaded. Please run the installation steps first.\")\n",
    "    else:\n",
    "        # Run the processing pipeline\n",
    "        results = process_articles_pipeline(\n",
    "            df=df,\n",
    "            nlp_model=nlp,\n",
    "            db_path=\"dutch_words_database.sqlite\",\n",
    "            batch_size=500  # Smaller batches for better memory management\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Full processing complete!\")\n",
    "        print(f\"Results: {results}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  Full processing is disabled for safety.\")\n",
    "    print(\"Set RUN_FULL_PROCESSING = True to enable full processing.\")\n",
    "    print(\"\\n📊 Alternative: Run a test with a small sample:\")\n",
    "    \n",
    "    # Test with a small sample instead\n",
    "    if 'df' in locals() and df is not None:\n",
    "        sample_size = 100\n",
    "        sample_df = df.head(sample_size)\n",
    "        \n",
    "        print(f\"\\n🧪 Running test with {sample_size} articles...\")\n",
    "        \n",
    "        if nlp is not None:\n",
    "            test_results = process_articles_pipeline(\n",
    "                df=sample_df,\n",
    "                nlp_model=nlp,\n",
    "                db_path=\"test_dutch_words.sqlite\",\n",
    "                batch_size=50\n",
    "            )\n",
    "            print(f\"Test results: {test_results}\")\n",
    "        else:\n",
    "            print(\"Cannot run test - spaCy model not loaded\")\n",
    "    else:\n",
    "        print(\"Dataset not loaded. Please run the data loading cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852156c4",
   "metadata": {},
   "source": [
    "## Step 7: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8578258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing analysis functions...\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 29,696\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 1,862 times\n",
      "   2. in (in) [preposition] - 882 times\n",
      "   3. van (van) [preposition] - 852 times\n",
      "   4. een (een) [determiner] - 780 times\n",
      "   5. het (het) [determiner] - 692 times\n",
      "   6. en (en) [conjunction] - 516 times\n",
      "   7. is (zijn) [auxiliary] - 430 times\n",
      "   8. op (op) [preposition] - 392 times\n",
      "   9. met (met) [preposition] - 278 times\n",
      "  10. voor (voor) [preposition] - 248 times\n",
      "  11. er (er) [adverb] - 231 times\n",
      "  12. het (het) [pronoun] - 198 times\n",
      "  13. dat (dat) [conjunction] - 193 times\n",
      "  14. te (te) [preposition] - 193 times\n",
      "  15. niet (niet) [adverb] - 191 times\n",
      "  16. zijn (zijn) [auxiliary] - 186 times\n",
      "  17. hij (hij) [pronoun] - 184 times\n",
      "  18. bij (bij) [preposition] - 180 times\n",
      "  19. jaar (jaar) [noun] - 179 times\n",
      "  20. die (die) [pronoun] - 163 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 2.6)\n",
      "  verb: 1,349 words (avg freq: 2.6)\n",
      "  proper_noun: 1,225 words (avg freq: 2.3)\n",
      "  adjective: 752 words (avg freq: 3.0)\n",
      "  adverb: 181 words (avg freq: 11.2)\n",
      "  other: 83 words (avg freq: 1.6)\n",
      "  preposition: 66 words (avg freq: 68.0)\n",
      "  pronoun: 65 words (avg freq: 24.7)\n",
      "  determiner: 51 words (avg freq: 73.9)\n",
      "  number: 45 words (avg freq: 7.9)\n",
      "  auxiliary: 37 words (avg freq: 42.4)\n",
      "  conjunction: 33 words (avg freq: 35.8)\n",
      "  interjection: 3 words (avg freq: 1.3)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 29,696 total instances\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 381 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 403 words to game_words.txt\n",
      "\n",
      "✅ All exports completed in: test_exports\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"dutch_words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"dutch_words_database.sqlite\", output_dir=\"exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\t{freq}\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\t{freq}\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\n✅ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "# Test analysis (will work with existing test database)\n",
    "print(\"Testing analysis functions...\")\n",
    "if os.path.exists(\"test_dutch_words.sqlite\"):\n",
    "    analyze_word_database(\"test_dutch_words.sqlite\")\n",
    "    export_word_lists(\"test_dutch_words.sqlite\", \"test_exports\")\n",
    "else:\n",
    "    print(\"No test database found. Run the processing steps first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
