{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ada48a",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This notebook outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847371cf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and other necessary libraries for the word extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f498d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866e8e6",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file for word extraction processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa9299",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0d50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "‚úì spacy installed successfully\n",
      "‚úì spacy installed successfully\n",
      "‚úì beautifulsoup4 installed successfully\n",
      "‚úì beautifulsoup4 installed successfully\n",
      "‚úì lxml installed successfully\n",
      "‚úì lxml installed successfully\n",
      "‚úì html5lib installed successfully\n",
      "‚úì html5lib installed successfully\n",
      "‚úì tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "‚úì tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "‚úì Dutch language model downloaded successfully\n",
      "‚úì Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚úó Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"‚úì Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚úó Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b84080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "‚úì Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n",
      "‚úì Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"‚úì Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"‚úó Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8888f",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc25ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82f065",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943fb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c7e61",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b90d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: output/test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"output/test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b917",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69511f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"output/words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db965c",
   "metadata": {},
   "source": [
    "## Step 6: Test Processing with Small Sample\n",
    "\n",
    "Test the processing pipeline with a small sample of articles to verify everything works correctly before running on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc128f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing word extraction pipeline with small sample...\n",
      "Sample size: 100 articles\n",
      "Created sample with 100 articles\n",
      "\n",
      "Starting sample processing...\n",
      "Database setup complete: output/test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25/100 articles, extracted 6,787 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: output/test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "‚úÖ Sample processing complete!\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'output/test_dutch_words.sqlite'}\n",
      "\n",
      "üìä Quick analysis of sample results:\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 59,392\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 3,724 times\n",
      "   2. in (in) [preposition] - 1,764 times\n",
      "   3. van (van) [preposition] - 1,704 times\n",
      "   4. een (een) [determiner] - 1,560 times\n",
      "   5. het (het) [determiner] - 1,384 times\n",
      "   6. en (en) [conjunction] - 1,032 times\n",
      "   7. is (zijn) [auxiliary] - 860 times\n",
      "   8. op (op) [preposition] - 784 times\n",
      "   9. met (met) [preposition] - 556 times\n",
      "  10. voor (voor) [preposition] - 496 times\n",
      "  11. er (er) [adverb] - 462 times\n",
      "  12. het (het) [pronoun] - 396 times\n",
      "  13. dat (dat) [conjunction] - 386 times\n",
      "  14. te (te) [preposition] - 386 times\n",
      "  15. niet (niet) [adverb] - 382 times\n",
      "  16. zijn (zijn) [auxiliary] - 372 times\n",
      "  17. hij (hij) [pronoun] - 368 times\n",
      "  18. bij (bij) [preposition] - 360 times\n",
      "  19. jaar (jaar) [noun] - 358 times\n",
      "  20. die (die) [pronoun] - 326 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 5.2)\n",
      "  verb: 1,349 words (avg freq: 5.1)\n",
      "  proper_noun: 1,225 words (avg freq: 4.6)\n",
      "  adjective: 752 words (avg freq: 6.0)\n",
      "  adverb: 181 words (avg freq: 22.3)\n",
      "  other: 83 words (avg freq: 3.1)\n",
      "  preposition: 66 words (avg freq: 136.1)\n",
      "  pronoun: 65 words (avg freq: 49.3)\n",
      "  determiner: 51 words (avg freq: 147.8)\n",
      "  number: 45 words (avg freq: 15.8)\n",
      "  auxiliary: 37 words (avg freq: 84.7)\n",
      "  conjunction: 33 words (avg freq: 71.6)\n",
      "  interjection: 3 words (avg freq: 2.7)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 59,392 total instances\n",
      "\n",
      "üìÅ Creating sample exports...\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 874 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 723 words to game_words.txt\n",
      "\n",
      "‚úÖ All exports completed in: output/test_exports\n",
      "\n",
      "‚úÖ Sample testing completed successfully!\n",
      "Ready to process full dataset in the next step.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the processing pipeline with a small sample first\n",
    "SAMPLE_SIZE = 100  # Number of articles to test with\n",
    "\n",
    "print(\"üß™ Testing word extraction pipeline with small sample...\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} articles\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample dataset\n",
    "sample_df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Created sample with {len(sample_df)} articles\")\n",
    "\n",
    "# Run the processing pipeline on sample\n",
    "print(\"\\nStarting sample processing...\")\n",
    "test_results = process_articles_pipeline(\n",
    "    df=sample_df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/test_dutch_words.sqlite\",\n",
    "    batch_size=25  # Small batches for testing\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample processing complete!\")\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Quick analysis of test results\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    print(\"\\nüìä Quick analysis of sample results:\")\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    \n",
    "    # Export sample results\n",
    "    print(\"\\nüìÅ Creating sample exports...\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sample testing completed successfully!\")\n",
    "    print(f\"Ready to process full dataset in the next step.\")\n",
    "else:\n",
    "    print(\"‚ùå Test database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867572",
   "metadata": {},
   "source": [
    "## Step 7: Full Dataset Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!\n",
    "\n",
    "Run this step only after successfully testing with the sample in Step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4af9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full processing of ALL articles...\n",
      "‚ö†Ô∏è  This may take several hours depending on your system.\n",
      "You can monitor progress and stop if needed.\n",
      "Processing 295,259 articles...\n",
      "Database setup complete: output/dutch_words_full.sqlite\n",
      "Starting processing of 295,259 articles...\n",
      "Batch size: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/591 [00:36<6:02:38, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/295,259 articles, extracted 139,591 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|‚ñè         | 8/591 [05:14<6:22:09, 39.33s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m articles...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the processing pipeline on full dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m results = \u001b[43mprocess_articles_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnlp_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput/dutch_words_full.sqlite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optimized batch size for full processing\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ FULL PROCESSING COMPLETE!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mprocess_articles_pipeline\u001b[39m\u001b[34m(df, nlp_model, db_path, batch_size)\u001b[39m\n\u001b[32m     61\u001b[39m combined_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(all_text)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combined_text:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Extract words\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     words = \u001b[43mextract_words_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m words:\n\u001b[32m     68\u001b[39m         \u001b[38;5;66;03m# Insert into database\u001b[39;00m\n\u001b[32m     69\u001b[39m         insert_word_data(conn, words, year)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mextract_words_from_text\u001b[39m\u001b[34m(text, nlp_model)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Process text with spaCy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     doc = \u001b[43mnlp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     words = []\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# Filter out unwanted tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\projects\\words\\.venv\\Lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process the full dataset\n",
    "print(\"üöÄ Starting full processing of ALL articles...\")\n",
    "print(\"‚ö†Ô∏è  This may take several hours depending on your system.\")\n",
    "print(\"You can monitor progress and stop if needed.\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "print(f\"Processing {len(df):,} articles...\")\n",
    "\n",
    "# Run the processing pipeline on full dataset\n",
    "results = process_articles_pipeline(\n",
    "    df=df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/dutch_words_full.sqlite\",\n",
    "    batch_size=500  # Optimized batch size for full processing\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FULL PROCESSING COMPLETE!\")\n",
    "print(f\"Results: {results}\")\n",
    "\n",
    "# Comprehensive analysis of full results\n",
    "if os.path.exists(\"output/dutch_words_full.sqlite\"):\n",
    "    print(\"\\nüìä Full database analysis:\")\n",
    "    analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "    \n",
    "    # Export full results\n",
    "    print(\"\\nüìÅ Creating production exports...\")\n",
    "    export_word_lists(\"output/dutch_words_full.sqlite\", \"output/exports\")\n",
    "    \n",
    "    print(f\"\\nüéâ COMPLETE! Full Dutch word database created successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Full database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852156c4",
   "metadata": {},
   "source": [
    "## Step 8: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8578258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing analysis functions...\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 29,696\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 1,862 times\n",
      "   2. in (in) [preposition] - 882 times\n",
      "   3. van (van) [preposition] - 852 times\n",
      "   4. een (een) [determiner] - 780 times\n",
      "   5. het (het) [determiner] - 692 times\n",
      "   6. en (en) [conjunction] - 516 times\n",
      "   7. is (zijn) [auxiliary] - 430 times\n",
      "   8. op (op) [preposition] - 392 times\n",
      "   9. met (met) [preposition] - 278 times\n",
      "  10. voor (voor) [preposition] - 248 times\n",
      "  11. er (er) [adverb] - 231 times\n",
      "  12. het (het) [pronoun] - 198 times\n",
      "  13. dat (dat) [conjunction] - 193 times\n",
      "  14. te (te) [preposition] - 193 times\n",
      "  15. niet (niet) [adverb] - 191 times\n",
      "  16. zijn (zijn) [auxiliary] - 186 times\n",
      "  17. hij (hij) [pronoun] - 184 times\n",
      "  18. bij (bij) [preposition] - 180 times\n",
      "  19. jaar (jaar) [noun] - 179 times\n",
      "  20. die (die) [pronoun] - 163 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 2.6)\n",
      "  verb: 1,349 words (avg freq: 2.6)\n",
      "  proper_noun: 1,225 words (avg freq: 2.3)\n",
      "  adjective: 752 words (avg freq: 3.0)\n",
      "  adverb: 181 words (avg freq: 11.2)\n",
      "  other: 83 words (avg freq: 1.6)\n",
      "  preposition: 66 words (avg freq: 68.0)\n",
      "  pronoun: 65 words (avg freq: 24.7)\n",
      "  determiner: 51 words (avg freq: 73.9)\n",
      "  number: 45 words (avg freq: 7.9)\n",
      "  auxiliary: 37 words (avg freq: 42.4)\n",
      "  conjunction: 33 words (avg freq: 35.8)\n",
      "  interjection: 3 words (avg freq: 1.3)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 29,696 total instances\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 381 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 403 words to game_words.txt\n",
      "\n",
      "‚úÖ All exports completed in: output/test_exports\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"output/words_database.sqlite\", output_dir=\"output/exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\t{freq}\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\t{freq}\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\n‚úÖ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "# Test analysis (will work with existing test database)\n",
    "print(\"Testing analysis functions...\")\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "else:\n",
    "    print(\"No test database found. Run the processing steps first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
