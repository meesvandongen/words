{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ada48a",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This notebook outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847371cf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation and other necessary libraries for the word extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f498d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866e8e6",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file for word extraction processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa9299",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0d50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "‚úì spacy installed successfully\n",
      "‚úì spacy installed successfully\n",
      "‚úì beautifulsoup4 installed successfully\n",
      "‚úì beautifulsoup4 installed successfully\n",
      "‚úì lxml installed successfully\n",
      "‚úì lxml installed successfully\n",
      "‚úì html5lib installed successfully\n",
      "‚úì html5lib installed successfully\n",
      "‚úì tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "‚úì tqdm installed successfully\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "‚úì Dutch language model downloaded successfully\n",
      "‚úì Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚úó Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"‚úì Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚úó Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b84080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "‚úì Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n",
      "‚úì Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"‚úì Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"‚úó Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8888f",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc25ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82f065",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943fb4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c7e61",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b90d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: output/test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 8, '2023-01-01', '2023-12-31', '2025-07-31 19:28:56')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"output/test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162b917",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69511f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"output/words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b28d1",
   "metadata": {},
   "source": [
    "## Step 5.5: Multi-threaded Processing Pipeline (Performance Optimized)\n",
    "\n",
    "This improved version uses multi-threading to process articles in parallel, significantly reducing processing time. Each thread processes a batch of articles independently, and results are safely written to the database using locks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22994673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Multi-threaded processing functions defined!\n",
      "Ready for high-performance article processing.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import concurrent.futures\n",
    "from queue import Queue\n",
    "import time\n",
    "import logging\n",
    "import signal\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging for multi-threaded processing (avoid duplicate handlers)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Only configure logging if it hasn't been configured yet\n",
    "if not logger.handlers:\n",
    "    # Clear any existing handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - [Thread-%(thread)d] - %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # Add handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Prevent propagation to root logger to avoid duplicates\n",
    "    logger.propagate = False\n",
    "\n",
    "# Global flag for interrupt handling\n",
    "interrupt_flag = threading.Event()\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    \"\"\"Handle keyboard interrupt (Ctrl+C) gracefully.\"\"\"\n",
    "    logger.warning(\"üõë Interrupt received! Stopping processing gracefully...\")\n",
    "    interrupt_flag.set()\n",
    "\n",
    "# Set up signal handler for interrupt\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "class ThreadSafeDatabase:\n",
    "    \"\"\"Thread-safe database operations for concurrent processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        self.lock = threading.Lock()\n",
    "        self.local = threading.local()\n",
    "        logger.info(f\"üìÅ Initialized thread-safe database: {db_path}\")\n",
    "    \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a thread-local database connection.\"\"\"\n",
    "        if not hasattr(self.local, 'conn'):\n",
    "            self.local.conn = sqlite3.connect(self.db_path)\n",
    "            logger.debug(f\"üîó Created database connection for thread {threading.current_thread().ident}\")\n",
    "        return self.local.conn\n",
    "    \n",
    "    def insert_word_data_threadsafe(self, word_data, year):\n",
    "        \"\"\"Thread-safe version of insert_word_data.\"\"\"\n",
    "        with self.lock:\n",
    "            conn = self.get_connection()\n",
    "            insert_word_data(conn, word_data, year)\n",
    "            logger.debug(f\"üíæ Inserted {len(word_data)} words for year {year}\")\n",
    "    \n",
    "    def close_all_connections(self):\n",
    "        \"\"\"Close all thread-local connections.\"\"\"\n",
    "        if hasattr(self.local, 'conn'):\n",
    "            self.local.conn.close()\n",
    "            logger.debug(f\"üîí Closed database connection for thread {threading.current_thread().ident}\")\n",
    "\n",
    "def process_article_batch_threaded(batch_data):\n",
    "    \"\"\"\n",
    "    Process a batch of articles in a single thread with interrupt handling.\n",
    "    \n",
    "    Args:\n",
    "        batch_data (tuple): (batch_df, nlp_model, thread_safe_db, thread_id, batch_num)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing statistics for this batch\n",
    "    \"\"\"\n",
    "    batch_df, nlp_model, thread_safe_db, thread_id, batch_num = batch_data\n",
    "    \n",
    "    articles_processed = 0\n",
    "    words_extracted = 0\n",
    "    errors_count = 0\n",
    "    \n",
    "    logger.info(f\"üöÄ Thread {thread_id} starting batch {batch_num} with {len(batch_df)} articles\")\n",
    "    \n",
    "    for idx, row in batch_df.iterrows():\n",
    "        # Check for interrupt signal\n",
    "        if interrupt_flag.is_set():\n",
    "            logger.warning(f\"‚èπÔ∏è Thread {thread_id} stopping due to interrupt signal\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Extract year from published_time\n",
    "            if pd.notna(row['published_time']):\n",
    "                if isinstance(row['published_time'], str):\n",
    "                    year = pd.to_datetime(row['published_time']).year\n",
    "                else:\n",
    "                    year = row['published_time'].year\n",
    "            else:\n",
    "                year = 2020  # Default year if missing\n",
    "            \n",
    "            # Process different text fields\n",
    "            text_fields = ['title', 'description', 'content']\n",
    "            all_text = []\n",
    "            \n",
    "            for field in text_fields:\n",
    "                if field in row and pd.notna(row[field]):\n",
    "                    if field == 'content':\n",
    "                        # Clean HTML from content\n",
    "                        clean_text = clean_html_content(row[field])\n",
    "                    else:\n",
    "                        clean_text = str(row[field])\n",
    "                    \n",
    "                    preprocessed = preprocess_text(clean_text)\n",
    "                    if preprocessed:\n",
    "                        all_text.append(preprocessed)\n",
    "            \n",
    "            # Combine all text\n",
    "            combined_text = ' '.join(all_text)\n",
    "            \n",
    "            if combined_text:\n",
    "                # Extract words\n",
    "                words = extract_words_from_text(combined_text, nlp_model)\n",
    "                \n",
    "                if words:\n",
    "                    # Insert into database (thread-safe)\n",
    "                    thread_safe_db.insert_word_data_threadsafe(words, year)\n",
    "                    words_extracted += len(words)\n",
    "            \n",
    "            articles_processed += 1\n",
    "            \n",
    "            # Log progress every 50 articles within the batch\n",
    "            if articles_processed % 50 == 0:\n",
    "                logger.debug(f\"üìä Thread {thread_id} batch {batch_num}: {articles_processed}/{len(batch_df)} articles processed\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(f\"‚ö†Ô∏è Thread {thread_id} received keyboard interrupt\")\n",
    "            interrupt_flag.set()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            errors_count += 1\n",
    "            logger.error(f\"‚ùå Thread {thread_id}: Error processing article {idx}: {e}\")\n",
    "            if errors_count > 10:  # Stop if too many errors\n",
    "                logger.error(f\"üö® Thread {thread_id}: Too many errors, stopping batch\")\n",
    "                break\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"‚úÖ Thread {thread_id} completed batch {batch_num}: {articles_processed} articles, {words_extracted} words, {errors_count} errors\")\n",
    "    \n",
    "    return {\n",
    "        'thread_id': thread_id,\n",
    "        'batch_num': batch_num,\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': words_extracted,\n",
    "        'errors_count': errors_count,\n",
    "        'interrupted': interrupt_flag.is_set()\n",
    "    }\n",
    "\n",
    "def process_articles_multithreaded(df, nlp_models, db_path=\"output/words_database.sqlite\", \n",
    "                                 batch_size=500, num_threads=None, log_level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Multi-threaded pipeline to process articles with improved performance, logging, and interrupt handling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_models: List of spaCy models (one per thread) or single model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles per batch\n",
    "        num_threads (int): Number of threads to use (defaults to CPU count)\n",
    "        log_level: Logging level for detailed output\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing statistics\n",
    "    \"\"\"\n",
    "    # Reset interrupt flag\n",
    "    interrupt_flag.clear()\n",
    "    \n",
    "    # Set logging level\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    if num_threads is None:\n",
    "        num_threads = min(8, os.cpu_count())  # Limit to 8 threads max\n",
    "    \n",
    "    logger.info(\"üöÄ Starting multi-threaded processing...\")\n",
    "    logger.info(f\"üìä Configuration:\")\n",
    "    logger.info(f\"   Articles to process: {len(df):,}\")\n",
    "    logger.info(f\"   Batch size: {batch_size}\")\n",
    "    logger.info(f\"   Number of threads: {num_threads}\")\n",
    "    logger.info(f\"   Database path: {db_path}\")\n",
    "    logger.info(f\"   Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Setup database\n",
    "    logger.info(\"üóÑÔ∏è Setting up database...\")\n",
    "    setup_database(db_path)\n",
    "    thread_safe_db = ThreadSafeDatabase(db_path)\n",
    "    \n",
    "    # Prepare spaCy models for each thread\n",
    "    logger.info(\"üß† Loading spaCy models for threads...\")\n",
    "    if isinstance(nlp_models, list):\n",
    "        if len(nlp_models) != num_threads:\n",
    "            logger.warning(f\"‚ö†Ô∏è {len(nlp_models)} models provided for {num_threads} threads\")\n",
    "            # Use first model for all threads if not enough models\n",
    "            nlp_models = [nlp_models[0]] * num_threads\n",
    "    else:\n",
    "        # Single model - each thread will need its own copy\n",
    "        nlp_models = []\n",
    "        for i in range(num_threads):\n",
    "            try:\n",
    "                nlp_model = spacy.load(\"nl_core_news_sm\")\n",
    "                nlp_models.append(nlp_model)\n",
    "                logger.info(f\"   ‚úÖ Thread {i+1} model loaded\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"   ‚ùå Error loading model for thread {i+1}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    # Split dataframe into batches\n",
    "    logger.info(\"üì¶ Creating batches...\")\n",
    "    batches = []\n",
    "    total_articles = len(df)\n",
    "    \n",
    "    for start_idx in range(0, total_articles, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        batches.append(batch_df)\n",
    "    \n",
    "    logger.info(f\"   Created {len(batches)} batches for processing\")\n",
    "    \n",
    "    # Prepare batch data for threads\n",
    "    batch_data_list = []\n",
    "    for i, batch_df in enumerate(batches):\n",
    "        thread_id = i % num_threads\n",
    "        nlp_model = nlp_models[thread_id]\n",
    "        batch_data_list.append((batch_df, nlp_model, thread_safe_db, thread_id, i))\n",
    "    \n",
    "    # Process batches using ThreadPoolExecutor\n",
    "    logger.info(\"‚ö° Starting thread pool execution...\")\n",
    "    start_time = time.time()\n",
    "    total_articles_processed = 0\n",
    "    total_words_extracted = 0\n",
    "    total_errors = 0\n",
    "    completed_batches = 0\n",
    "    interrupted = False\n",
    "    \n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            logger.info(f\"üéØ Submitted {len(batch_data_list)} batches to {num_threads} threads\")\n",
    "            \n",
    "            # Submit all batches\n",
    "            future_to_batch = {\n",
    "                executor.submit(process_article_batch_threaded, batch_data): i \n",
    "                for i, batch_data in enumerate(batch_data_list)\n",
    "            }\n",
    "            \n",
    "            # Process completed batches\n",
    "            for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                batch_num = future_to_batch[future]\n",
    "                \n",
    "                try:\n",
    "                    result = future.result(timeout=30)  # 30 second timeout per batch\n",
    "                    total_articles_processed += result['articles_processed']\n",
    "                    total_words_extracted += result['words_extracted']\n",
    "                    total_errors += result.get('errors_count', 0)\n",
    "                    completed_batches += 1\n",
    "                    \n",
    "                    if result.get('interrupted', False):\n",
    "                        interrupted = True\n",
    "                        logger.warning(f\"üõë Batch {batch_num} was interrupted\")\n",
    "                    \n",
    "                    # Progress update\n",
    "                    progress = (completed_batches / len(batches)) * 100\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = total_articles_processed / elapsed if elapsed > 0 else 0\n",
    "                    \n",
    "                    if completed_batches % 5 == 0 or completed_batches == len(batches) or interrupted:\n",
    "                        logger.info(f\"üìà Progress: {completed_batches}/{len(batches)} batches ({progress:.1f}%)\")\n",
    "                        logger.info(f\"   üìä Articles: {total_articles_processed:,}/{total_articles:,}\")\n",
    "                        logger.info(f\"   üí¨ Words extracted: {total_words_extracted:,}\")\n",
    "                        logger.info(f\"   ‚è±Ô∏è Time: {elapsed:.1f}s ({rate:.1f} articles/sec)\")\n",
    "                        logger.info(f\"   ‚ùå Errors: {total_errors}\")\n",
    "                        \n",
    "                        if interrupted:\n",
    "                            logger.warning(\"üö® Interrupt detected, stopping remaining batches...\")\n",
    "                            break\n",
    "                            \n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    logger.error(f\"‚è∞ Batch {batch_num} timed out\")\n",
    "                    total_errors += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"üí• Batch {batch_num} generated an exception: {e}\")\n",
    "                    total_errors += 1\n",
    "            \n",
    "            if interrupted:\n",
    "                # Cancel remaining futures\n",
    "                for future in future_to_batch:\n",
    "                    future.cancel()\n",
    "                logger.warning(\"üõë Cancelled remaining batch processing due to interrupt\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"‚ö†Ô∏è Keyboard interrupt received in main thread\")\n",
    "        interrupt_flag.set()\n",
    "        interrupted = True\n",
    "    \n",
    "    # Close all database connections\n",
    "    logger.info(\"üîí Closing database connections...\")\n",
    "    thread_safe_db.close_all_connections()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    # Log final results to database\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        status = \"INTERRUPTED\" if interrupted else \"COMPLETED\"\n",
    "        cursor.execute('''\n",
    "            INSERT INTO processing_log (articles_processed, words_extracted, notes) \n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (\n",
    "            total_articles_processed, \n",
    "            total_words_extracted, \n",
    "            f\"Multi-threaded processing {status} - {num_threads} threads, {batch_size} batch size, {processing_time:.1f}s, {total_errors} errors\"\n",
    "        ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        logger.info(\"üìù Logged processing results to database\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to log to database: {e}\")\n",
    "    \n",
    "    # Final summary\n",
    "    status_emoji = \"‚ö†Ô∏è\" if interrupted else \"‚úÖ\"\n",
    "    logger.info(f\"\\n{status_emoji} === MULTI-THREADED PROCESSING {'INTERRUPTED' if interrupted else 'COMPLETE'} ===\")\n",
    "    logger.info(f\"üìä Total articles processed: {total_articles_processed:,}/{total_articles:,}\")\n",
    "    logger.info(f\"üí¨ Total words extracted: {total_words_extracted:,}\")\n",
    "    logger.info(f\"‚è±Ô∏è Processing time: {processing_time:.1f} seconds\")\n",
    "    logger.info(f\"üöÄ Articles per second: {total_articles_processed/processing_time:.1f}\")\n",
    "    logger.info(f\"‚ùå Total errors: {total_errors}\")\n",
    "    logger.info(f\"üìÅ Database saved to: {db_path}\")\n",
    "    logger.info(f\"üèÅ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': total_articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'processing_time': processing_time,\n",
    "        'articles_per_second': total_articles_processed/processing_time if processing_time > 0 else 0,\n",
    "        'database_path': db_path,\n",
    "        'num_threads': num_threads,\n",
    "        'total_errors': total_errors,\n",
    "        'interrupted': interrupted,\n",
    "        'batches_completed': completed_batches,\n",
    "        'batches_total': len(batches)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Multi-threaded processing functions defined!\")\n",
    "print(\"Ready for high-performance article processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Interrupt handling and monitoring utilities defined!\n",
      "Functions available:\n",
      "  - reset_interrupt(): Reset interrupt flag\n",
      "  - force_interrupt(): Manually trigger interrupt\n",
      "  - check_interrupt_status(): Check current status\n",
      "  - monitor_processing_progress(): Check database progress\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for interrupt handling and monitoring\n",
    "def reset_logging():\n",
    "    \"\"\"Reset logging configuration to prevent duplicates.\"\"\"\n",
    "    global logger\n",
    "    # Clear all handlers\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Recreate formatter and handler\n",
    "    formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(levelname)s - [Thread-%(thread)d] - %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "    \n",
    "    logger.info(\"üîÑ Logging configuration reset\")\n",
    "\n",
    "def reset_interrupt():\n",
    "    \"\"\"Reset the interrupt flag to allow new processing.\"\"\"\n",
    "    interrupt_flag.clear()\n",
    "    logger.info(\"üîÑ Interrupt flag reset - ready for new processing\")\n",
    "\n",
    "def force_interrupt():\n",
    "    \"\"\"Manually trigger an interrupt (useful for testing).\"\"\"\n",
    "    interrupt_flag.set()\n",
    "    logger.warning(\"üõë Manual interrupt triggered\")\n",
    "\n",
    "def check_interrupt_status():\n",
    "    \"\"\"Check if processing is currently interrupted.\"\"\"\n",
    "    status = \"INTERRUPTED\" if interrupt_flag.is_set() else \"NORMAL\"\n",
    "    logger.info(f\"üìä Current interrupt status: {status}\")\n",
    "    return interrupt_flag.is_set()\n",
    "\n",
    "def monitor_processing_progress(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"Monitor the progress of ongoing processing by checking the database.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get latest processing log entry\n",
    "        cursor.execute('''\n",
    "            SELECT articles_processed, words_extracted, processing_date, notes \n",
    "            FROM processing_log \n",
    "            ORDER BY processing_date DESC \n",
    "            LIMIT 1\n",
    "        ''')\n",
    "        result = cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            articles, words, date, notes = result\n",
    "            logger.info(f\"üìà Latest processing status:\")\n",
    "            logger.info(f\"   üìä Articles processed: {articles:,}\")\n",
    "            logger.info(f\"   üí¨ Words extracted: {words:,}\")\n",
    "            logger.info(f\"   ‚è∞ Last update: {date}\")\n",
    "            logger.info(f\"   üìù Notes: {notes}\")\n",
    "        else:\n",
    "            logger.info(\"üì≠ No processing log entries found\")\n",
    "            \n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error monitoring progress: {e}\")\n",
    "\n",
    "print(\"üõ†Ô∏è Interrupt handling and monitoring utilities defined!\")\n",
    "print(\"Functions available:\")\n",
    "print(\"  - reset_logging(): Reset logging configuration\")\n",
    "print(\"  - reset_interrupt(): Reset interrupt flag\")\n",
    "print(\"  - force_interrupt(): Manually trigger interrupt\")\n",
    "print(\"  - check_interrupt_status(): Check current status\")\n",
    "print(\"  - monitor_processing_progress(): Check database progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f49f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:21 - INFO - [Thread-25780] - üîÑ Interrupt flag reset - ready for new processing\n",
      "22:33:21 - INFO - [Thread-25780] - üöÄ Starting multi-threaded processing...\n",
      "22:33:21 - INFO - [Thread-25780] - üìä Configuration:\n",
      "22:33:21 - INFO - [Thread-25780] -    Articles to process: 100\n",
      "22:33:21 - INFO - [Thread-25780] -    Batch size: 20\n",
      "22:33:21 - INFO - [Thread-25780] -    Number of threads: 3\n",
      "22:33:21 - INFO - [Thread-25780] -    Database path: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:21\n",
      "22:33:21 - INFO - [Thread-25780] - üóÑÔ∏è Setting up database...\n",
      "22:33:21 - INFO - [Thread-25780] - üöÄ Starting multi-threaded processing...\n",
      "22:33:21 - INFO - [Thread-25780] - üìä Configuration:\n",
      "22:33:21 - INFO - [Thread-25780] -    Articles to process: 100\n",
      "22:33:21 - INFO - [Thread-25780] -    Batch size: 20\n",
      "22:33:21 - INFO - [Thread-25780] -    Number of threads: 3\n",
      "22:33:21 - INFO - [Thread-25780] -    Database path: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:21\n",
      "22:33:21 - INFO - [Thread-25780] - üóÑÔ∏è Setting up database...\n",
      "22:33:21 - INFO - [Thread-25780] - üìÅ Initialized thread-safe database: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] - üß† Loading spaCy models for threads...\n",
      "22:33:21 - INFO - [Thread-25780] - üìÅ Initialized thread-safe database: output/enhanced_test.sqlite\n",
      "22:33:21 - INFO - [Thread-25780] - üß† Loading spaCy models for threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing enhanced multi-threaded pipeline...\n",
      "Created sample with 100 articles for testing\n",
      "\n",
      "üìä Enhanced Features Demo:\n",
      "1. Detailed logging with timestamps and thread IDs\n",
      "2. Interrupt handling (Ctrl+C or notebook interrupt)\n",
      "3. Progress monitoring and error tracking\n",
      "4. Graceful shutdown and cleanup\n",
      "\n",
      "üöÄ Starting enhanced multi-threaded processing...\n",
      "Database setup complete: output/enhanced_test.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:22 - INFO - [Thread-25780] -    ‚úÖ Thread 1 model loaded\n",
      "22:33:22 - INFO - [Thread-25780] -    ‚úÖ Thread 2 model loaded\n",
      "22:33:22 - INFO - [Thread-25780] -    ‚úÖ Thread 2 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] -    ‚úÖ Thread 3 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] - üì¶ Creating batches...\n",
      "22:33:23 - INFO - [Thread-25780] -    Created 5 batches for processing\n",
      "22:33:23 - INFO - [Thread-25780] - ‚ö° Starting thread pool execution...\n",
      "22:33:23 - INFO - [Thread-25780] - üéØ Submitted 5 batches to 3 threads\n",
      "22:33:23 - INFO - [Thread-24600] - üöÄ Thread 0 starting batch 0 with 20 articles\n",
      "22:33:23 - INFO - [Thread-30948] - üöÄ Thread 1 starting batch 1 with 20 articles\n",
      "22:33:23 - INFO - [Thread-25780] -    ‚úÖ Thread 3 model loaded\n",
      "22:33:23 - INFO - [Thread-25780] - üì¶ Creating batches...\n",
      "22:33:23 - INFO - [Thread-25780] -    Created 5 batches for processing\n",
      "22:33:23 - INFO - [Thread-25780] - ‚ö° Starting thread pool execution...\n",
      "22:33:23 - INFO - [Thread-25780] - üéØ Submitted 5 batches to 3 threads\n",
      "22:33:23 - INFO - [Thread-24600] - üöÄ Thread 0 starting batch 0 with 20 articles\n",
      "22:33:23 - INFO - [Thread-30948] - üöÄ Thread 1 starting batch 1 with 20 articles\n",
      "22:33:23 - INFO - [Thread-31384] - üöÄ Thread 2 starting batch 2 with 20 articles\n",
      "22:33:23 - INFO - [Thread-31384] - üöÄ Thread 2 starting batch 2 with 20 articles\n",
      "22:33:25 - INFO - [Thread-30948] - ‚úÖ Thread 1 completed batch 1: 20 articles, 4356 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-30948] - üöÄ Thread 0 starting batch 3 with 20 articles\n",
      "22:33:25 - INFO - [Thread-30948] - ‚úÖ Thread 1 completed batch 1: 20 articles, 4356 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-30948] - üöÄ Thread 0 starting batch 3 with 20 articles\n",
      "22:33:25 - INFO - [Thread-24600] - ‚úÖ Thread 0 completed batch 0: 20 articles, 5655 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-24600] - üöÄ Thread 1 starting batch 4 with 20 articles\n",
      "22:33:25 - INFO - [Thread-24600] - ‚úÖ Thread 0 completed batch 0: 20 articles, 5655 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-24600] - üöÄ Thread 1 starting batch 4 with 20 articles\n",
      "22:33:25 - INFO - [Thread-31384] - ‚úÖ Thread 2 completed batch 2: 20 articles, 5743 words, 0 errors\n",
      "22:33:25 - INFO - [Thread-31384] - ‚úÖ Thread 2 completed batch 2: 20 articles, 5743 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-24600] - ‚úÖ Thread 1 completed batch 4: 20 articles, 6193 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-24600] - ‚úÖ Thread 1 completed batch 4: 20 articles, 6193 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-30948] - ‚úÖ Thread 0 completed batch 3: 20 articles, 7749 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - üìà Progress: 5/5 batches (100.0%)\n",
      "22:33:27 - INFO - [Thread-25780] -    üìä Articles: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] -    üí¨ Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚è±Ô∏è Time: 4.3s (23.5 articles/sec)\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚ùå Errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - üîí Closing database connections...\n",
      "22:33:27 - INFO - [Thread-25780] - üìù Logged processing results to database\n",
      "22:33:27 - INFO - [Thread-30948] - ‚úÖ Thread 0 completed batch 3: 20 articles, 7749 words, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - üìà Progress: 5/5 batches (100.0%)\n",
      "22:33:27 - INFO - [Thread-25780] -    üìä Articles: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] -    üí¨ Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚è±Ô∏è Time: 4.3s (23.5 articles/sec)\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚ùå Errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - üîí Closing database connections...\n",
      "22:33:27 - INFO - [Thread-25780] - üìù Logged processing results to database\n",
      "22:33:27 - INFO - [Thread-25780] - \n",
      "‚úÖ === MULTI-THREADED PROCESSING COMPLETE ===\n",
      "22:33:27 - INFO - [Thread-25780] - üìä Total articles processed: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] - üí¨ Total words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] - ‚è±Ô∏è Processing time: 4.3 seconds\n",
      "22:33:27 - INFO - [Thread-25780] - üöÄ Articles per second: 23.5\n",
      "22:33:27 - INFO - [Thread-25780] - ‚ùå Total errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - üìÅ Database saved to: output/enhanced_test.sqlite\n",
      "22:33:27 - INFO - [Thread-25780] - üèÅ End time: 2025-08-01 22:33:27\n",
      "22:33:27 - INFO - [Thread-25780] - üìä Current interrupt status: NORMAL\n",
      "22:33:27 - INFO - [Thread-25780] - üìà Latest processing status:\n",
      "22:33:27 - INFO - [Thread-25780] -    üìä Articles processed: 100\n",
      "22:33:27 - INFO - [Thread-25780] -    üí¨ Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚è∞ Last update: 2025-08-01 20:33:27\n",
      "22:33:27 - INFO - [Thread-25780] -    üìù Notes: Multi-threaded processing COMPLETED - 3 threads, 20 batch size, 4.3s, 0 errors\n",
      "22:33:27 - INFO - [Thread-25780] - \n",
      "‚úÖ === MULTI-THREADED PROCESSING COMPLETE ===\n",
      "22:33:27 - INFO - [Thread-25780] - üìä Total articles processed: 100/100\n",
      "22:33:27 - INFO - [Thread-25780] - üí¨ Total words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] - ‚è±Ô∏è Processing time: 4.3 seconds\n",
      "22:33:27 - INFO - [Thread-25780] - üöÄ Articles per second: 23.5\n",
      "22:33:27 - INFO - [Thread-25780] - ‚ùå Total errors: 0\n",
      "22:33:27 - INFO - [Thread-25780] - üìÅ Database saved to: output/enhanced_test.sqlite\n",
      "22:33:27 - INFO - [Thread-25780] - üèÅ End time: 2025-08-01 22:33:27\n",
      "22:33:27 - INFO - [Thread-25780] - üìä Current interrupt status: NORMAL\n",
      "22:33:27 - INFO - [Thread-25780] - üìà Latest processing status:\n",
      "22:33:27 - INFO - [Thread-25780] -    üìä Articles processed: 100\n",
      "22:33:27 - INFO - [Thread-25780] -    üí¨ Words extracted: 29,696\n",
      "22:33:27 - INFO - [Thread-25780] -    ‚è∞ Last update: 2025-08-01 20:33:27\n",
      "22:33:27 - INFO - [Thread-25780] -    üìù Notes: Multi-threaded processing COMPLETED - 3 threads, 20 batch size, 4.3s, 0 errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Enhanced processing complete!\n",
      "üìä Results summary:\n",
      "   articles_processed: 100\n",
      "   words_extracted: 29696\n",
      "   processing_time: 4.25283670425415\n",
      "   articles_per_second: 23.513717303081286\n",
      "   database_path: output/enhanced_test.sqlite\n",
      "   num_threads: 3\n",
      "   total_errors: 0\n",
      "   interrupted: False\n",
      "   batches_completed: 5\n",
      "   batches_total: 5\n",
      "\n",
      "üîç Testing monitoring functions:\n",
      "\n",
      "üí° To interrupt processing:\n",
      "   - Use Jupyter notebook interrupt button\n",
      "   - Press Ctrl+C in terminal\n",
      "   - Call force_interrupt() function\n",
      "   - Processing will stop gracefully and save progress\n"
     ]
    }
   ],
   "source": [
    "# Test multi-threaded processing with enhanced logging and interrupt handling\n",
    "print(\"üß™ Testing enhanced multi-threaded pipeline...\")\n",
    "\n",
    "# Reset any previous interrupt state and logging\n",
    "reset_interrupt()\n",
    "reset_logging()\n",
    "\n",
    "# Use smaller sample for testing multi-threading\n",
    "SAMPLE_SIZE_MT = 100  # Smaller for testing interrupt functionality\n",
    "\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample for multi-threaded testing\n",
    "sample_df_mt = df.head(SAMPLE_SIZE_MT)\n",
    "print(f\"Created sample with {len(sample_df_mt)} articles for testing\")\n",
    "\n",
    "print(\"\\nüìä Enhanced Features Demo:\")\n",
    "print(\"1. Detailed logging with timestamps and thread IDs\")\n",
    "print(\"2. Interrupt handling (Ctrl+C or notebook interrupt)\")\n",
    "print(\"3. Progress monitoring and error tracking\")\n",
    "print(\"4. Graceful shutdown and cleanup\")\n",
    "\n",
    "print(\"\\nüöÄ Starting enhanced multi-threaded processing...\")\n",
    "\n",
    "# Run with detailed logging\n",
    "results = process_articles_multithreaded(\n",
    "    df=sample_df_mt,\n",
    "    nlp_models=nlp,\n",
    "    db_path=\"output/enhanced_test.sqlite\",\n",
    "    batch_size=20,  # Small batches for more frequent updates\n",
    "    num_threads=3,  # Fewer threads for easier monitoring\n",
    "    log_level=logging.INFO  # Detailed logging\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced processing complete!\")\n",
    "print(f\"üìä Results summary:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Demonstrate monitoring functions\n",
    "print(f\"\\nüîç Testing monitoring functions:\")\n",
    "check_interrupt_status()\n",
    "monitor_processing_progress(\"output/enhanced_test.sqlite\")\n",
    "\n",
    "print(f\"\\nüí° To interrupt processing:\")\n",
    "print(\"   - Use Jupyter notebook interrupt button\")\n",
    "print(\"   - Press Ctrl+C in terminal\")\n",
    "print(\"   - Call force_interrupt() function\")\n",
    "print(\"   - Processing will stop gracefully and save progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ca8c6",
   "metadata": {},
   "source": [
    "## Performance Optimization Guide\n",
    "\n",
    "### Multi-threading Configuration\n",
    "\n",
    "**Optimal Thread Count:**\n",
    "- **CPU-bound tasks**: Use `CPU cores - 2` threads (leaves cores for OS)\n",
    "- **Memory-bound tasks**: Use `CPU cores` or slightly more\n",
    "- **For this workload**: Start with 4-6 threads, adjust based on performance\n",
    "\n",
    "**Batch Size Recommendations:**\n",
    "- **Small batches (100-200)**: Better memory usage, more frequent database commits\n",
    "- **Large batches (500-1000)**: Less database overhead, higher memory usage\n",
    "- **Recommended**: 500 articles per batch for balanced performance\n",
    "\n",
    "**Performance Tips:**\n",
    "1. **Monitor system resources** during processing\n",
    "2. **Reduce threads** if CPU usage is maxed out\n",
    "3. **Increase batch size** if I/O is the bottleneck\n",
    "4. **Use SSD storage** for database for faster writes\n",
    "5. **Close other applications** to free up memory\n",
    "\n",
    "**Expected Performance:**\n",
    "- Single-threaded: ~50-100 articles/second\n",
    "- Multi-threaded (4 cores): ~200-400 articles/second\n",
    "- Multi-threaded (8 cores): ~400-600 articles/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69bf05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  SYSTEM ANALYSIS FOR MULTI-THREADING\n",
      "==================================================\n",
      "CPU Cores: 24\n",
      "CPU Frequency: 3701 MHz (max: 3701 MHz)\n",
      "Total RAM: 31.9 GB\n",
      "Available RAM: 13.1 GB\n",
      "Memory Usage: 59.1%\n",
      "Output Drive Free Space: 288.3 GB\n",
      "\n",
      "üéØ RECOMMENDED CONFIGURATION:\n",
      "========================================\n",
      "Recommended Threads: 8\n",
      "Recommended Batch Size: 500\n",
      "Estimated Memory Usage: 4.0 GB\n",
      "\n",
      "üí° Usage Example:\n",
      "```python\n",
      "results = process_articles_multithreaded(\n",
      "    df=df,\n",
      "    nlp_models=nlp,\n",
      "    db_path='output/dutch_words_full.sqlite',\n",
      "    batch_size=500,\n",
      "    num_threads=8\n",
      ")\n",
      "```\n",
      "\n",
      "‚è±Ô∏è  Estimated Processing Time:\n",
      "~12 minutes for 295,000 articles\n",
      "(400 articles/second estimated)\n"
     ]
    }
   ],
   "source": [
    "# System Analysis for Optimal Thread Configuration\n",
    "import psutil\n",
    "import multiprocessing\n",
    "\n",
    "print(\"üñ•Ô∏è  SYSTEM ANALYSIS FOR MULTI-THREADING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# CPU Information\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "if cpu_freq:\n",
    "    print(f\"CPU Frequency: {cpu_freq.current:.0f} MHz (max: {cpu_freq.max:.0f} MHz)\")\n",
    "\n",
    "# Memory Information\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"Memory Usage: {memory.percent}%\")\n",
    "\n",
    "# Disk Information (for database storage)\n",
    "try:\n",
    "    disk = psutil.disk_usage('output')\n",
    "    print(f\"Output Drive Free Space: {disk.free / (1024**3):.1f} GB\")\n",
    "except:\n",
    "    print(\"Could not check output drive space\")\n",
    "\n",
    "print(\"\\nüéØ RECOMMENDED CONFIGURATION:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate recommendations\n",
    "recommended_threads = max(2, min(8, cpu_count - 1))\n",
    "recommended_batch_size = 500\n",
    "\n",
    "if memory.available / (1024**3) < 4:  # Less than 4GB available\n",
    "    recommended_batch_size = 250\n",
    "    recommended_threads = max(2, recommended_threads - 1)\n",
    "    print(\"‚ö†Ô∏è  Limited memory detected - reducing batch size and threads\")\n",
    "\n",
    "print(f\"Recommended Threads: {recommended_threads}\")\n",
    "print(f\"Recommended Batch Size: {recommended_batch_size}\")\n",
    "\n",
    "# Memory estimate\n",
    "estimated_memory_per_thread = 0.5  # GB per thread (rough estimate)\n",
    "total_estimated_memory = recommended_threads * estimated_memory_per_thread\n",
    "\n",
    "print(f\"Estimated Memory Usage: {total_estimated_memory:.1f} GB\")\n",
    "\n",
    "if total_estimated_memory > memory.available / (1024**3) * 0.8:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Estimated memory usage is high. Consider reducing threads.\")\n",
    "    recommended_threads = max(2, int(memory.available / (1024**3) * 0.8 / estimated_memory_per_thread))\n",
    "    print(f\"Adjusted recommendation: {recommended_threads} threads\")\n",
    "\n",
    "print(f\"\\nüí° Usage Example:\")\n",
    "print(\"```python\")\n",
    "print(\"results = process_articles_multithreaded(\")\n",
    "print(\"    df=df,\")\n",
    "print(\"    nlp_models=nlp,\")\n",
    "print(\"    db_path='output/dutch_words_full.sqlite',\")\n",
    "print(f\"    batch_size={recommended_batch_size},\")\n",
    "print(f\"    num_threads={recommended_threads}\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Estimated Processing Time:\")\n",
    "articles_per_second = recommended_threads * 50  # Rough estimate\n",
    "total_articles = 295000  # Approximate article count\n",
    "estimated_time = total_articles / articles_per_second\n",
    "print(f\"~{estimated_time/60:.0f} minutes for {total_articles:,} articles\")\n",
    "print(f\"({articles_per_second} articles/second estimated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f04c2",
   "metadata": {},
   "source": [
    "## üõë Interrupt Handling Guide\n",
    "\n",
    "### How to Safely Stop Processing\n",
    "\n",
    "**Jupyter Notebook:**\n",
    "1. Click the **\"Interrupt\"** button in the toolbar\n",
    "2. Processing will stop gracefully after current batches complete\n",
    "3. Partial results are automatically saved to the database\n",
    "\n",
    "**Terminal/Command Line:**\n",
    "1. Press **Ctrl+C** (Windows/Linux) or **Cmd+C** (Mac)\n",
    "2. Processing will stop gracefully\n",
    "3. All completed work is preserved\n",
    "\n",
    "**Programmatically:**\n",
    "```python\n",
    "# Force interrupt from code\n",
    "force_interrupt()\n",
    "\n",
    "# Check if processing was interrupted\n",
    "if check_interrupt_status():\n",
    "    print(\"Processing is currently interrupted\")\n",
    "\n",
    "# Reset to allow new processing\n",
    "reset_interrupt()\n",
    "```\n",
    "\n",
    "### üìä Monitoring Progress\n",
    "\n",
    "**Real-time Monitoring:**\n",
    "- Detailed logs show progress every 5 completed batches\n",
    "- Thread-level logging shows individual thread progress\n",
    "- Error tracking and timeout handling\n",
    "\n",
    "**Database Monitoring:**\n",
    "```python\n",
    "# Check latest progress\n",
    "monitor_processing_progress(\"output/dutch_words_full.sqlite\")\n",
    "\n",
    "# Analyze partial results\n",
    "analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "```\n",
    "\n",
    "### üîÑ Resuming Interrupted Processing\n",
    "\n",
    "**Safe Resume Process:**\n",
    "1. Check interrupt status: `check_interrupt_status()`\n",
    "2. Reset interrupt flag: `reset_interrupt()`\n",
    "3. Re-run the processing cell\n",
    "4. Processing will continue where it left off (database handles duplicates)\n",
    "\n",
    "### ‚ö° Performance vs Safety Balance\n",
    "\n",
    "**Trade-offs:**\n",
    "- **More frequent saves**: Slower processing, safer against data loss\n",
    "- **Larger batches**: Faster processing, more data loss risk if interrupted\n",
    "- **More threads**: Higher resource usage, harder to interrupt cleanly\n",
    "\n",
    "**Recommendations:**\n",
    "- Use **500 article batches** for optimal balance\n",
    "- Set **num_threads = CPU_cores - 2** to leave resources for interrupts\n",
    "- Monitor system resources during processing\n",
    "- Always test interrupt functionality with small samples first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87367c7b",
   "metadata": {},
   "source": [
    "## Step 6: Analysis and Export Functions\n",
    "\n",
    "Define the analysis and export functions that will be used to analyze the word database and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb82429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis and export functions defined and ready for use.\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"output/words_database.sqlite\", output_dir=\"output/exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\t{freq}\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\t{freq}\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\n‚úÖ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "print(\"Analysis and export functions defined and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db965c",
   "metadata": {},
   "source": [
    "## Step 7: Test Processing with Small Sample\n",
    "\n",
    "Test the processing pipeline with a small sample of articles to verify everything works correctly before running on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc128f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing word extraction pipeline with small sample...\n",
      "Sample size: 100 articles\n",
      "Created sample with 100 articles\n",
      "\n",
      "Starting sample processing...\n",
      "Database setup complete: output/test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25/100 articles, extracted 6,787 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: output/test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "‚úÖ Sample processing complete!\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'output/test_dutch_words.sqlite'}\n",
      "\n",
      "üìä Quick analysis of sample results:\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: output/test_dutch_words.sqlite\n",
      "\n",
      "Basic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 59,392\n",
      "  POS categories: 13\n",
      "\n",
      "Top 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 3,724 times\n",
      "   2. in (in) [preposition] - 1,764 times\n",
      "   3. van (van) [preposition] - 1,704 times\n",
      "   4. een (een) [determiner] - 1,560 times\n",
      "   5. het (het) [determiner] - 1,384 times\n",
      "   6. en (en) [conjunction] - 1,032 times\n",
      "   7. is (zijn) [auxiliary] - 860 times\n",
      "   8. op (op) [preposition] - 784 times\n",
      "   9. met (met) [preposition] - 556 times\n",
      "  10. voor (voor) [preposition] - 496 times\n",
      "  11. er (er) [adverb] - 462 times\n",
      "  12. het (het) [pronoun] - 396 times\n",
      "  13. te (te) [preposition] - 386 times\n",
      "  14. dat (dat) [conjunction] - 386 times\n",
      "  15. niet (niet) [adverb] - 382 times\n",
      "  16. zijn (zijn) [auxiliary] - 372 times\n",
      "  17. hij (hij) [pronoun] - 368 times\n",
      "  18. bij (bij) [preposition] - 360 times\n",
      "  19. jaar (jaar) [noun] - 358 times\n",
      "  20. die (die) [pronoun] - 326 times\n",
      "\n",
      "Words by POS Category:\n",
      "  noun: 2,305 words (avg freq: 5.2)\n",
      "  verb: 1,349 words (avg freq: 5.1)\n",
      "  proper_noun: 1,225 words (avg freq: 4.6)\n",
      "  adjective: 752 words (avg freq: 6.0)\n",
      "  adverb: 181 words (avg freq: 22.3)\n",
      "  other: 83 words (avg freq: 3.1)\n",
      "  preposition: 66 words (avg freq: 136.1)\n",
      "  pronoun: 65 words (avg freq: 49.3)\n",
      "  determiner: 51 words (avg freq: 147.8)\n",
      "  number: 45 words (avg freq: 15.8)\n",
      "  auxiliary: 37 words (avg freq: 84.7)\n",
      "  conjunction: 33 words (avg freq: 71.6)\n",
      "  interjection: 3 words (avg freq: 2.7)\n",
      "\n",
      "Yearly Word Trends:\n",
      "  2015: 6,195 unique words, 59,392 total instances\n",
      "\n",
      "üìÅ Creating sample exports...\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: output/test_exports\n",
      "\n",
      "1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\n",
      "2. Exporting common words (frequency >= 10)...\n",
      "   Exported 874 words to common_words.txt\n",
      "\n",
      "3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\n",
      "4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\n",
      "5. Exporting game-friendly word list...\n",
      "   Exported 723 words to game_words.txt\n",
      "\n",
      "‚úÖ All exports completed in: output/test_exports\n",
      "\n",
      "‚úÖ Sample testing completed successfully!\n",
      "Ready to process full dataset in the next step.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the processing pipeline with a small sample first\n",
    "SAMPLE_SIZE = 100  # Number of articles to test with\n",
    "\n",
    "print(\"üß™ Testing word extraction pipeline with small sample...\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} articles\")\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "# Create sample dataset\n",
    "sample_df = df.head(SAMPLE_SIZE)\n",
    "print(f\"Created sample with {len(sample_df)} articles\")\n",
    "\n",
    "# Run the processing pipeline on sample\n",
    "print(\"\\nStarting sample processing...\")\n",
    "test_results = process_articles_pipeline(\n",
    "    df=sample_df,\n",
    "    nlp_model=nlp,\n",
    "    db_path=\"output/test_dutch_words.sqlite\",\n",
    "    batch_size=25  # Small batches for testing\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Sample processing complete!\")\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Quick analysis of test results\n",
    "if os.path.exists(\"output/test_dutch_words.sqlite\"):\n",
    "    print(\"\\nüìä Quick analysis of sample results:\")\n",
    "    analyze_word_database(\"output/test_dutch_words.sqlite\")\n",
    "    \n",
    "    # Export sample results\n",
    "    print(\"\\nüìÅ Creating sample exports...\")\n",
    "    export_word_lists(\"output/test_dutch_words.sqlite\", \"output/test_exports\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sample testing completed successfully!\")\n",
    "    print(f\"Ready to process full dataset in the next step.\")\n",
    "else:\n",
    "    print(\"‚ùå Test database was not created properly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35867572",
   "metadata": {},
   "source": [
    "## Step 8: Full Dataset Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!\n",
    "\n",
    "Run this step only after successfully testing with the sample in Step 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af9755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:34 - INFO - [Thread-25780] - üîÑ Interrupt flag reset - ready for new processing\n",
      "22:33:34 - INFO - [Thread-25780] - üöÄ Starting multi-threaded processing...\n",
      "22:33:34 - INFO - [Thread-25780] - üìä Configuration:\n",
      "22:33:34 - INFO - [Thread-25780] -    Articles to process: 295,259\n",
      "22:33:34 - INFO - [Thread-25780] -    Batch size: 500\n",
      "22:33:34 - INFO - [Thread-25780] - üöÄ Starting multi-threaded processing...\n",
      "22:33:34 - INFO - [Thread-25780] - üìä Configuration:\n",
      "22:33:34 - INFO - [Thread-25780] -    Articles to process: 295,259\n",
      "22:33:34 - INFO - [Thread-25780] -    Batch size: 500\n",
      "22:33:34 - INFO - [Thread-25780] -    Number of threads: 8\n",
      "22:33:34 - INFO - [Thread-25780] -    Database path: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:34\n",
      "22:33:34 - INFO - [Thread-25780] - üóÑÔ∏è Setting up database...\n",
      "22:33:34 - INFO - [Thread-25780] -    Number of threads: 8\n",
      "22:33:34 - INFO - [Thread-25780] -    Database path: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] -    Start time: 2025-08-01 22:33:34\n",
      "22:33:34 - INFO - [Thread-25780] - üóÑÔ∏è Setting up database...\n",
      "22:33:34 - INFO - [Thread-25780] - üìÅ Initialized thread-safe database: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] - üß† Loading spaCy models for threads...\n",
      "22:33:34 - INFO - [Thread-25780] - üìÅ Initialized thread-safe database: output/dutch_words_full.sqlite\n",
      "22:33:34 - INFO - [Thread-25780] - üß† Loading spaCy models for threads...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting ENHANCED MULTI-THREADED processing of ALL articles...\n",
      "‚ö° Features: Detailed logging, interrupt handling, progress monitoring\n",
      "üõë You can interrupt processing safely with Ctrl+C or notebook interrupt\n",
      "Processing 295,259 articles with enhanced multi-threading...\n",
      "üí° System recommendations:\n",
      "   CPU cores: 24\n",
      "   Available memory: 13.1 GB\n",
      "   Recommended threads: 8\n",
      "Database setup complete: output/dutch_words_full.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:33:35 - INFO - [Thread-25780] -    ‚úÖ Thread 1 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ‚úÖ Thread 2 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ‚úÖ Thread 2 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ‚úÖ Thread 3 model loaded\n",
      "22:33:36 - INFO - [Thread-25780] -    ‚úÖ Thread 3 model loaded\n",
      "22:33:37 - INFO - [Thread-25780] -    ‚úÖ Thread 4 model loaded\n",
      "22:33:37 - INFO - [Thread-25780] -    ‚úÖ Thread 4 model loaded\n",
      "22:33:38 - INFO - [Thread-25780] -    ‚úÖ Thread 5 model loaded\n",
      "22:33:38 - INFO - [Thread-25780] -    ‚úÖ Thread 5 model loaded\n",
      "22:33:39 - INFO - [Thread-25780] -    ‚úÖ Thread 6 model loaded\n",
      "22:33:39 - INFO - [Thread-25780] -    ‚úÖ Thread 6 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ‚úÖ Thread 7 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ‚úÖ Thread 7 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] -    ‚úÖ Thread 8 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] - üì¶ Creating batches...\n",
      "22:33:40 - INFO - [Thread-25780] -    ‚úÖ Thread 8 model loaded\n",
      "22:33:40 - INFO - [Thread-25780] - üì¶ Creating batches...\n",
      "22:33:41 - INFO - [Thread-25780] -    Created 591 batches for processing\n",
      "22:33:41 - INFO - [Thread-25780] - ‚ö° Starting thread pool execution...\n",
      "22:33:41 - INFO - [Thread-25780] - üéØ Submitted 591 batches to 8 threads\n",
      "22:33:41 - INFO - [Thread-30364] - üöÄ Thread 0 starting batch 0 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28564] - üöÄ Thread 1 starting batch 1 with 500 articles\n",
      "22:33:41 - INFO - [Thread-25780] -    Created 591 batches for processing\n",
      "22:33:41 - INFO - [Thread-25780] - ‚ö° Starting thread pool execution...\n",
      "22:33:41 - INFO - [Thread-25780] - üéØ Submitted 591 batches to 8 threads\n",
      "22:33:41 - INFO - [Thread-30364] - üöÄ Thread 0 starting batch 0 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28564] - üöÄ Thread 1 starting batch 1 with 500 articles\n",
      "22:33:41 - INFO - [Thread-33516] - üöÄ Thread 2 starting batch 2 with 500 articles\n",
      "22:33:41 - INFO - [Thread-30668] - üöÄ Thread 3 starting batch 3 with 500 articles\n",
      "22:33:41 - INFO - [Thread-17896] - üöÄ Thread 4 starting batch 4 with 500 articles\n",
      "22:33:41 - INFO - [Thread-33516] - üöÄ Thread 2 starting batch 2 with 500 articles\n",
      "22:33:41 - INFO - [Thread-30668] - üöÄ Thread 3 starting batch 3 with 500 articles\n",
      "22:33:41 - INFO - [Thread-17896] - üöÄ Thread 4 starting batch 4 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28624] - üöÄ Thread 5 starting batch 5 with 500 articles\n",
      "22:33:41 - INFO - [Thread-23248] - üöÄ Thread 6 starting batch 6 with 500 articles\n",
      "22:33:41 - INFO - [Thread-31704] - üöÄ Thread 7 starting batch 7 with 500 articles\n",
      "22:33:41 - INFO - [Thread-28624] - üöÄ Thread 5 starting batch 5 with 500 articles\n",
      "22:33:41 - INFO - [Thread-23248] - üöÄ Thread 6 starting batch 6 with 500 articles\n",
      "22:33:41 - INFO - [Thread-31704] - üöÄ Thread 7 starting batch 7 with 500 articles\n",
      "22:36:52 - INFO - [Thread-31704] - ‚úÖ Thread 7 completed batch 7: 500 articles, 137955 words, 0 errors\n",
      "22:36:52 - INFO - [Thread-31704] - üöÄ Thread 0 starting batch 8 with 500 articles\n",
      "22:36:52 - INFO - [Thread-31704] - ‚úÖ Thread 7 completed batch 7: 500 articles, 137955 words, 0 errors\n",
      "22:36:52 - INFO - [Thread-31704] - üöÄ Thread 0 starting batch 8 with 500 articles\n",
      "22:36:53 - INFO - [Thread-23248] - ‚úÖ Thread 6 completed batch 6: 500 articles, 130145 words, 0 errors\n",
      "22:36:53 - INFO - [Thread-23248] - üöÄ Thread 1 starting batch 9 with 500 articles\n",
      "22:36:53 - INFO - [Thread-23248] - ‚úÖ Thread 6 completed batch 6: 500 articles, 130145 words, 0 errors\n",
      "22:36:53 - INFO - [Thread-23248] - üöÄ Thread 1 starting batch 9 with 500 articles\n",
      "22:36:54 - INFO - [Thread-17896] - ‚úÖ Thread 4 completed batch 4: 500 articles, 140461 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-17896] - üöÄ Thread 2 starting batch 10 with 500 articles\n",
      "22:36:54 - INFO - [Thread-17896] - ‚úÖ Thread 4 completed batch 4: 500 articles, 140461 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-17896] - üöÄ Thread 2 starting batch 10 with 500 articles\n",
      "22:36:54 - INFO - [Thread-30668] - ‚úÖ Thread 3 completed batch 3: 500 articles, 137438 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-30668] - üöÄ Thread 3 starting batch 11 with 500 articles\n",
      "22:36:54 - INFO - [Thread-30668] - ‚úÖ Thread 3 completed batch 3: 500 articles, 137438 words, 0 errors\n",
      "22:36:54 - INFO - [Thread-30668] - üöÄ Thread 3 starting batch 11 with 500 articles\n",
      "22:36:55 - INFO - [Thread-33516] - ‚úÖ Thread 2 completed batch 2: 500 articles, 140353 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-33516] - üöÄ Thread 4 starting batch 12 with 500 articles\n",
      "22:36:55 - INFO - [Thread-25780] - üìà Progress: 5/591 batches (0.8%)\n",
      "22:36:55 - INFO - [Thread-25780] -    üìä Articles: 2,500/295,259\n",
      "22:36:55 - INFO - [Thread-25780] -    üí¨ Words extracted: 686,352\n",
      "22:36:55 - INFO - [Thread-25780] -    ‚è±Ô∏è Time: 194.4s (12.9 articles/sec)\n",
      "22:36:55 - INFO - [Thread-25780] -    ‚ùå Errors: 0\n",
      "22:36:55 - INFO - [Thread-33516] - ‚úÖ Thread 2 completed batch 2: 500 articles, 140353 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-33516] - üöÄ Thread 4 starting batch 12 with 500 articles\n",
      "22:36:55 - INFO - [Thread-25780] - üìà Progress: 5/591 batches (0.8%)\n",
      "22:36:55 - INFO - [Thread-25780] -    üìä Articles: 2,500/295,259\n",
      "22:36:55 - INFO - [Thread-25780] -    üí¨ Words extracted: 686,352\n",
      "22:36:55 - INFO - [Thread-25780] -    ‚è±Ô∏è Time: 194.4s (12.9 articles/sec)\n",
      "22:36:55 - INFO - [Thread-25780] -    ‚ùå Errors: 0\n",
      "22:36:55 - INFO - [Thread-30364] - ‚úÖ Thread 0 completed batch 0: 500 articles, 139591 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-30364] - üöÄ Thread 5 starting batch 13 with 500 articles\n",
      "22:36:55 - INFO - [Thread-30364] - ‚úÖ Thread 0 completed batch 0: 500 articles, 139591 words, 0 errors\n",
      "22:36:55 - INFO - [Thread-30364] - üöÄ Thread 5 starting batch 13 with 500 articles\n",
      "22:36:57 - INFO - [Thread-28624] - ‚úÖ Thread 5 completed batch 5: 500 articles, 140752 words, 0 errors\n",
      "22:36:57 - INFO - [Thread-28624] - üöÄ Thread 6 starting batch 14 with 500 articles\n",
      "22:36:57 - INFO - [Thread-28624] - ‚úÖ Thread 5 completed batch 5: 500 articles, 140752 words, 0 errors\n",
      "22:36:57 - INFO - [Thread-28624] - üöÄ Thread 6 starting batch 14 with 500 articles\n",
      "22:36:59 - INFO - [Thread-28564] - ‚úÖ Thread 1 completed batch 1: 500 articles, 149269 words, 0 errors\n",
      "22:36:59 - INFO - [Thread-28564] - üöÄ Thread 7 starting batch 15 with 500 articles\n",
      "22:36:59 - INFO - [Thread-28564] - ‚úÖ Thread 1 completed batch 1: 500 articles, 149269 words, 0 errors\n",
      "22:36:59 - INFO - [Thread-28564] - üöÄ Thread 7 starting batch 15 with 500 articles\n"
     ]
    }
   ],
   "source": [
    "# Process the full dataset with enhanced multi-threading (logging + interrupt handling)\n",
    "print(\"üöÄ Starting ENHANCED MULTI-THREADED processing of ALL articles...\")\n",
    "print(\"‚ö° Features: Detailed logging, interrupt handling, progress monitoring\")\n",
    "print(\"üõë You can interrupt processing safely with Ctrl+C or notebook interrupt\")\n",
    "\n",
    "# Reset interrupt flag\n",
    "reset_interrupt()\n",
    "\n",
    "# Load the full dataset if not already loaded\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "    print(f\"Dataset loaded: {df.shape}\")\n",
    "\n",
    "print(f\"Processing {len(df):,} articles with enhanced multi-threading...\")\n",
    "\n",
    "# Get system recommendations\n",
    "import psutil\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "memory = psutil.virtual_memory()\n",
    "recommended_threads = max(2, min(8, cpu_count - 1))\n",
    "\n",
    "if memory.available / (1024**3) < 4:  # Less than 4GB available\n",
    "    recommended_threads = max(2, recommended_threads - 1)\n",
    "\n",
    "print(f\"üí° System recommendations:\")\n",
    "print(f\"   CPU cores: {cpu_count}\")\n",
    "print(f\"   Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "print(f\"   Recommended threads: {recommended_threads}\")\n",
    "\n",
    "# Run the ENHANCED MULTI-THREADED processing pipeline\n",
    "try:\n",
    "    results = process_articles_multithreaded(\n",
    "        df=df,\n",
    "        nlp_models=nlp,  # Will create copies for each thread\n",
    "        db_path=\"output/dutch_words_full.sqlite\",\n",
    "        batch_size=500,  # Optimized batch size\n",
    "        num_threads=recommended_threads,  # Use system recommendations\n",
    "        log_level=logging.INFO  # Detailed logging\n",
    "    )\n",
    "    \n",
    "    status = \"INTERRUPTED\" if results.get('interrupted', False) else \"COMPLETED\"\n",
    "    print(f\"\\n{'‚ö†Ô∏è' if results.get('interrupted') else '‚úÖ'} FULL PROCESSING {status}!\")\n",
    "    \n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"   Articles processed: {results['articles_processed']:,}\")\n",
    "    print(f\"   Words extracted: {results['words_extracted']:,}\")\n",
    "    print(f\"   Processing time: {results['processing_time']:.1f} seconds\")\n",
    "    print(f\"   Articles per second: {results['articles_per_second']:.1f}\")\n",
    "    print(f\"   Errors encountered: {results['total_errors']}\")\n",
    "    print(f\"   Batches completed: {results['batches_completed']}/{results['batches_total']}\")\n",
    "    \n",
    "    if results.get('interrupted'):\n",
    "        print(f\"\\n‚ö†Ô∏è Processing was interrupted but data has been saved.\")\n",
    "        print(f\"üìÑ You can resume processing by running this cell again.\")\n",
    "        print(f\"üìä Progress monitoring available with: monitor_processing_progress()\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nüõë Processing interrupted by user\")\n",
    "    print(f\"üìÅ Partial results saved to database\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during processing: {e}\")\n",
    "    logger.error(f\"Processing failed: {e}\")\n",
    "\n",
    "# Always run analysis if database exists\n",
    "if os.path.exists(\"output/dutch_words_full.sqlite\"):\n",
    "    print(f\"\\nüìä Database analysis:\")\n",
    "    try:\n",
    "        analyze_word_database(\"output/dutch_words_full.sqlite\")\n",
    "        \n",
    "        # Export results if processing completed successfully\n",
    "        if 'results' in locals() and not results.get('interrupted', False):\n",
    "            print(f\"\\nüìÅ Creating production exports...\")\n",
    "            export_word_lists(\"output/dutch_words_full.sqlite\", \"output/exports\")\n",
    "            print(f\"\\nüéâ COMPLETE! Full Dutch word database created successfully!\")\n",
    "        else:\n",
    "            print(f\"\\nüíæ Partial results available in database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during analysis: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Database file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852156c4",
   "metadata": {},
   "source": [
    "## Step 8: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
