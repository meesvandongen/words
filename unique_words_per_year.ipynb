{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Unique Words Per Year Analysis\n",
    "\n",
    "This notebook analyzes the pre-calculated SQLite database to extract words which are used **relatively more** compared to other words in specific years. It accounts for biases such as total word count per year and identifies words that have anomalous frequency patterns.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Relative Frequency Analysis**: Calculate normalized frequencies accounting for total word counts per year\n",
    "2. **Statistical Significance**: Identify words with statistically significant frequency spikes\n",
    "3. **Bias Correction**: Account for varying article counts and total word volumes per year\n",
    "4. **Temporal Uniqueness**: Find words that are distinctively associated with specific years\n",
    "\n",
    "## Key Metrics Calculated\n",
    "\n",
    "- **Relative Frequency**: Word frequency in year / Total words in year\n",
    "- **Z-Score**: How many standard deviations above/below the word's average frequency\n",
    "- **Lift**: Ratio of year frequency to expected frequency based on overall distribution\n",
    "- **Temporal Specificity**: How concentrated a word's usage is in specific years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Setup and Database Connection\n",
    "\n",
    "Load required libraries and connect to the pre-calculated SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "print(f\"âœ“ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection and validation\n",
    "def connect_to_database(db_path=\"output/words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Connect to the pre-calculated words database and validate its structure.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    # Try multiple possible database locations\n",
    "    possible_paths = [\n",
    "        db_path,\n",
    "        \"output/dutch_words_full.sqlite\",\n",
    "        \"output/test_dutch_words.sqlite\",\n",
    "        \"output/test_words.sqlite\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found database: {path}\")\n",
    "            conn = sqlite3.connect(path)\n",
    "            \n",
    "            # Validate database structure\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Check required tables exist\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "            tables = [row[0] for row in cursor.fetchall()]\n",
    "            \n",
    "            required_tables = ['words', 'word_frequencies']\n",
    "            if all(table in tables for table in required_tables):\n",
    "                print(f\"âœ“ Database validated with tables: {tables}\")\n",
    "                \n",
    "                # Get basic statistics\n",
    "                cursor.execute(\"SELECT COUNT(*) FROM words\")\n",
    "                word_count = cursor.fetchone()[0]\n",
    "                \n",
    "                cursor.execute(\"SELECT COUNT(*) FROM word_frequencies\")\n",
    "                freq_count = cursor.fetchone()[0]\n",
    "                \n",
    "                cursor.execute(\"SELECT MIN(year), MAX(year) FROM word_frequencies\")\n",
    "                year_range = cursor.fetchone()\n",
    "                \n",
    "                print(f\"âœ“ Database contains:\")\n",
    "                print(f\"  - {word_count:,} unique words\")\n",
    "                print(f\"  - {freq_count:,} word-year frequency records\")\n",
    "                print(f\"  - Years: {year_range[0]} to {year_range[1]}\")\n",
    "                \n",
    "                return conn, path\n",
    "            else:\n",
    "                print(f\"âœ— Database {path} missing required tables\")\n",
    "                conn.close()\n",
    "    \n",
    "    raise FileNotFoundError(\"No valid word database found. Please run word_extraction_strategy.ipynb first.\")\n",
    "\n",
    "# Connect to database\n",
    "try:\n",
    "    conn, db_path = connect_to_database()\n",
    "    print(f\"\\nâœ“ Successfully connected to database: {db_path}\")\nexcept FileNotFoundError as e:\n",
    "    print(f\"âœ— Database connection failed: {e}\")\n",
    "    print(\"Please run the word_extraction_strategy.ipynb notebook first to generate the database.\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "Load the word frequency data and prepare it for relative frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_frequency_data(conn):\n",
    "    \"\"\"\n",
    "    Load and prepare word frequency data for analysis.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite database connection\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (word_freq_df, yearly_totals, word_info_df)\n",
    "    \"\"\"\n",
    "    if conn is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    print(\"Loading word frequency data...\")\n",
    "    \n",
    "    # Load word frequencies with word information\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        w.word,\n",
    "        w.lemma,\n",
    "        w.pos_category,\n",
    "        w.total_frequency,\n",
    "        wf.year,\n",
    "        wf.frequency\n",
    "    FROM words w\n",
    "    JOIN word_frequencies wf ON w.id = wf.word_id\n",
    "    WHERE w.total_frequency >= 10  -- Only include words with reasonable frequency\n",
    "    ORDER BY w.word, wf.year\n",
    "    \"\"\"\n",
    "    \n",
    "    word_freq_df = pd.read_sql_query(query, conn)\n",
    "    print(f\"âœ“ Loaded {len(word_freq_df):,} word-year frequency records\")\n",
    "    \n",
    "    # Calculate yearly totals for normalization\n",
    "    yearly_totals = word_freq_df.groupby('year')['frequency'].sum().reset_index()\n",
    "    yearly_totals.columns = ['year', 'total_words']\n",
    "    print(f\"âœ“ Calculated yearly totals for {len(yearly_totals)} years\")\n",
    "    \n",
    "    # Load word information\n",
    "    word_info_query = \"\"\"\n",
    "    SELECT word, lemma, pos_category, total_frequency\n",
    "    FROM words\n",
    "    WHERE total_frequency >= 10\n",
    "    \"\"\"\n",
    "    \n",
    "    word_info_df = pd.read_sql_query(word_info_query, conn)\n",
    "    print(f\"âœ“ Loaded information for {len(word_info_df):,} unique words\")\n",
    "    \n",
    "    return word_freq_df, yearly_totals, word_info_df\n",
    "\n",
    "# Load the data\n",
    "word_freq_df, yearly_totals, word_info_df = load_word_frequency_data(conn)\n",
    "\n",
    "if word_freq_df is not None:\n",
    "    print(\"\\nðŸ“Š Data Overview:\")\n",
    "    print(f\"Years covered: {word_freq_df['year'].min()} to {word_freq_df['year'].max()}\")\n",
    "    print(f\"Unique words: {word_freq_df['word'].nunique():,}\")\n",
    "    print(f\"POS categories: {word_freq_df['pos_category'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Yearly word counts:\")\n",
    "    print(yearly_totals.to_string(index=False))\n",
    "else:\n",
    "    print(\"âŒ Failed to load data. Cannot proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "## Relative Frequency Analysis\n",
    "\n",
    "Calculate relative frequencies and identify words with significant yearly variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative_freq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_relative_frequencies(word_freq_df, yearly_totals):\n",
    "    \"\"\"\n",
    "    Calculate relative frequencies and statistical measures for temporal uniqueness.\n",
    "    \n",
    "    Args:\n",
    "        word_freq_df: DataFrame with word frequencies by year\n",
    "        yearly_totals: DataFrame with total word counts per year\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Enhanced frequency data with relative measures\n",
    "    \"\"\"\n",
    "    if word_freq_df is None or yearly_totals is None:\n",
    "        return None\n",
    "    \n",
    "    print(\"Calculating relative frequencies and statistical measures...\")\n",
    "    \n",
    "    # Merge with yearly totals\n",
    "    df = word_freq_df.merge(yearly_totals, on='year')\n",
    "    \n",
    "    # Calculate relative frequency (normalized by year)\n",
    "    df['relative_frequency'] = df['frequency'] / df['total_words']\n",
    "    \n",
    "    # Calculate expected frequency based on overall distribution\n",
    "    word_stats = df.groupby('word').agg({\n",
    "        'frequency': ['sum', 'mean', 'std'],\n",
    "        'relative_frequency': ['mean', 'std'],\n",
    "        'total_frequency': 'first',\n",
    "        'pos_category': 'first',\n",
    "        'lemma': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    word_stats.columns = ['word', 'total_freq', 'mean_freq', 'std_freq', \n",
    "                         'mean_rel_freq', 'std_rel_freq', 'total_frequency', \n",
    "                         'pos_category', 'lemma']\n",
    "    \n",
    "    # Merge back with main dataframe\n",
    "    df = df.merge(word_stats[['word', 'mean_freq', 'std_freq', 'mean_rel_freq', 'std_rel_freq']], \n",
    "                  on='word')\n",
    "    \n",
    "    # Calculate Z-score for each word-year combination\n",
    "    df['frequency_z_score'] = np.where(\n",
    "        df['std_freq'] > 0,\n",
    "        (df['frequency'] - df['mean_freq']) / df['std_freq'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Calculate relative frequency Z-score\n",
    "    df['rel_freq_z_score'] = np.where(\n",
    "        df['std_rel_freq'] > 0,\n",
    "        (df['relative_frequency'] - df['mean_rel_freq']) / df['std_rel_freq'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Calculate lift (ratio to expected)\n",
    "    df['frequency_lift'] = np.where(\n",
    "        df['mean_freq'] > 0,\n",
    "        df['frequency'] / df['mean_freq'],\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    df['rel_freq_lift'] = np.where(\n",
    "        df['mean_rel_freq'] > 0,\n",
    "        df['relative_frequency'] / df['mean_rel_freq'],\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Calculated relative frequencies for {len(df):,} word-year combinations\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate relative frequencies\n",
    "enhanced_df = calculate_relative_frequencies(word_freq_df, yearly_totals)\n",
    "\n",
    "if enhanced_df is not None:\n",
    "    print(\"\\nðŸ“Š Sample of enhanced frequency data:\")\n",
    "    sample_cols = ['word', 'year', 'frequency', 'relative_frequency', 'rel_freq_z_score', 'rel_freq_lift']\n",
    "    print(enhanced_df[sample_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Summary statistics:\")\n",
    "    print(f\"Mean relative frequency Z-score: {enhanced_df['rel_freq_z_score'].mean():.3f}\")\n",
    "    print(f\"Max relative frequency Z-score: {enhanced_df['rel_freq_z_score'].max():.3f}\")\n",
    "    print(f\"Words with Z-score > 2: {(enhanced_df['rel_freq_z_score'] > 2).sum():,}\")\n",
    "    print(f\"Words with Z-score > 3: {(enhanced_df['rel_freq_z_score'] > 3).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "## Identify Unique Words Per Year\n",
    "\n",
    "Find words that are significantly more frequent in specific years compared to their baseline usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique_words",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_unique_words_per_year(enhanced_df, z_threshold=2.0, min_frequency=20):\n",
    "    \"\"\"\n",
    "    Identify words that are uniquely frequent in specific years.\n",
    "    \n",
    "    Args:\n",
    "        enhanced_df: DataFrame with relative frequency calculations\n",
    "        z_threshold: Minimum Z-score for considering a word \"unique\" to a year\n",
    "        min_frequency: Minimum absolute frequency to avoid noise\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Words with their \"unique\" years and significance measures\n",
    "    \"\"\"\n",
    "    if enhanced_df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"Identifying unique words per year (Z-score >= {z_threshold}, frequency >= {min_frequency})...\")\n",
    "    \n",
    "    # Filter for significant frequency spikes\n",
    "    unique_words = enhanced_df[\n",
    "        (enhanced_df['rel_freq_z_score'] >= z_threshold) & \n",
    "        (enhanced_df['frequency'] >= min_frequency)\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by significance (Z-score)\n",
    "    unique_words = unique_words.sort_values('rel_freq_z_score', ascending=False)\n",
    "    \n",
    "    print(f\"âœ“ Found {len(unique_words):,} word-year combinations with significant frequency spikes\")\n",
    "    \n",
    "    # Group by year to see top unique words per year\n",
    "    top_per_year = unique_words.groupby('year').apply(\n",
    "        lambda x: x.nlargest(20, 'rel_freq_z_score')\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate temporal specificity for each word\n",
    "    word_temporal_stats = unique_words.groupby('word').agg({\n",
    "        'year': ['count', 'nunique'],\n",
    "        'rel_freq_z_score': ['max', 'mean'],\n",
    "        'frequency': 'max',\n",
    "        'pos_category': 'first',\n",
    "        'lemma': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    word_temporal_stats.columns = ['word', 'spike_count', 'unique_years', \n",
    "                                  'max_z_score', 'mean_z_score', 'max_frequency',\n",
    "                                  'pos_category', 'lemma']\n",
    "    \n",
    "    # Calculate temporal specificity (how concentrated usage is)\n",
    "    word_temporal_stats['temporal_specificity'] = 1 / word_temporal_stats['unique_years']\n",
    "    \n",
    "    return unique_words, top_per_year, word_temporal_stats\n",
    "\n",
    "# Identify unique words\n",
    "unique_words, top_per_year, word_temporal_stats = identify_unique_words_per_year(enhanced_df)\n",
    "\n",
    "if unique_words is not None:\n",
    "    print(\"\\nðŸŽ¯ Top 10 most temporally unique words overall:\")\n",
    "    top_overall = unique_words.nlargest(10, 'rel_freq_z_score')\n",
    "    display_cols = ['word', 'year', 'frequency', 'rel_freq_z_score', 'rel_freq_lift', 'pos_category']\n",
    "    print(top_overall[display_cols].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ“… Summary by year:\")\n",
    "    yearly_summary = unique_words.groupby('year').agg({\n",
    "        'word': 'count',\n",
    "        'rel_freq_z_score': 'mean'\n",
    "    }).round(2)\n",
    "    yearly_summary.columns = ['unique_words_count', 'avg_z_score']\n",
    "    print(yearly_summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Create visualizations to show temporal word usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_visualizations(enhanced_df, unique_words, yearly_totals):\n",
    "    \"\"\"\n",
    "    Create visualizations for temporal word usage patterns.\n",
    "    \n",
    "    Args:\n",
    "        enhanced_df: DataFrame with all word frequency data\n",
    "        unique_words: DataFrame with unique words per year\n",
    "        yearly_totals: DataFrame with yearly totals\n",
    "    \"\"\"\n",
    "    if enhanced_df is None or unique_words is None:\n",
    "        print(\"Cannot create visualizations - data not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Creating temporal visualizations...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs('output/visualizations', exist_ok=True)\n",
    "    \n",
    "    # 1. Yearly distribution of unique words\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    yearly_unique_counts = unique_words.groupby('year').size()\n",
    "    plt.bar(yearly_unique_counts.index, yearly_unique_counts.values, alpha=0.7)\n",
    "    plt.title('Number of Unique Words per Year\\n(Z-score >= 2.0)')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count of Unique Words')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Total word volume by year\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(yearly_totals['year'], yearly_totals['total_words'], marker='o', linewidth=2)\n",
    "    plt.title('Total Word Volume by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Total Words')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 3. Z-score distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(unique_words['rel_freq_z_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Distribution of Z-scores\\n(Unique Words Only)')\n",
    "    plt.xlabel('Relative Frequency Z-score')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # 4. Top words by temporal specificity\n",
    "    plt.subplot(2, 2, 4)\n",
    "    top_specific = word_temporal_stats.nlargest(15, 'temporal_specificity')\n",
    "    plt.barh(range(len(top_specific)), top_specific['max_z_score'], alpha=0.7)\n",
    "    plt.yticks(range(len(top_specific)), top_specific['word'])\n",
    "    plt.title('Most Temporally Specific Words\\n(Max Z-score)')\n",
    "    plt.xlabel('Maximum Z-score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/visualizations/temporal_overview.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Detailed timeline for top words\n",
    "    print(\"\\nCreating detailed timeline for top unique words...\")\n",
    "    \n",
    "    # Get top 8 most unique words\n",
    "    top_words = unique_words.nlargest(8, 'rel_freq_z_score')['word'].unique()\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    for i, word in enumerate(top_words):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        \n",
    "        # Get all data for this word\n",
    "        word_data = enhanced_df[enhanced_df['word'] == word].sort_values('year')\n",
    "        \n",
    "        # Plot relative frequency over time\n",
    "        plt.plot(word_data['year'], word_data['relative_frequency'], \n",
    "                marker='o', linewidth=2, markersize=6)\n",
    "        \n",
    "        # Highlight years with high Z-scores\n",
    "        unique_years = word_data[word_data['rel_freq_z_score'] >= 2.0]\n",
    "        if len(unique_years) > 0:\n",
    "            plt.scatter(unique_years['year'], unique_years['relative_frequency'], \n",
    "                       color='red', s=100, alpha=0.7, zorder=5)\n",
    "        \n",
    "        plt.title(f\"'{word}'\\n(POS: {word_data.iloc[0]['pos_category']})\")\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Relative Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Format y-axis in scientific notation if very small\n",
    "        if word_data['relative_frequency'].max() < 0.001:\n",
    "            plt.ticklabel_format(axis='y', style='scientific', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/visualizations/top_words_timeline.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Visualizations saved to output/visualizations/\")\n",
    "\n",
    "# Create visualizations\n",
    "create_temporal_visualizations(enhanced_df, unique_words, yearly_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export the unique words analysis results in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_unique_words_analysis(unique_words, top_per_year, word_temporal_stats, enhanced_df):\n",
    "    \"\"\"\n",
    "    Export the unique words analysis results.\n",
    "    \n",
    "    Args:\n",
    "        unique_words: DataFrame with all unique word-year combinations\n",
    "        top_per_year: DataFrame with top unique words per year\n",
    "        word_temporal_stats: DataFrame with temporal statistics per word\n",
    "        enhanced_df: Complete enhanced frequency data\n",
    "    \"\"\"\n",
    "    if unique_words is None:\n",
    "        print(\"Cannot export - analysis data not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Exporting unique words analysis results...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = 'output/unique_words_analysis'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Complete unique words dataset\n",
    "    print(\"\\n1. Exporting complete unique words dataset...\")\n",
    "    export_cols = ['word', 'lemma', 'pos_category', 'year', 'frequency', \n",
    "                   'relative_frequency', 'rel_freq_z_score', 'rel_freq_lift']\n",
    "    unique_words[export_cols].to_csv(\n",
    "        f'{output_dir}/unique_words_complete.csv', \n",
    "        index=False, encoding='utf-8'\n",
    "    )\n",
    "    print(f\"   Exported {len(unique_words):,} unique word-year combinations\")\n",
    "    \n",
    "    # 2. Top unique words per year\n",
    "    print(\"\\n2. Exporting top unique words per year...\")\n",
    "    top_per_year[export_cols].to_csv(\n",
    "        f'{output_dir}/top_unique_words_per_year.csv', \n",
    "        index=False, encoding='utf-8'\n",
    "    )\n",
    "    print(f\"   Exported top 20 unique words for each year\")\n",
    "    \n",
    "    # 3. Word temporal statistics\n",
    "    print(\"\\n3. Exporting word temporal statistics...\")\n",
    "    word_temporal_stats.to_csv(\n",
    "        f'{output_dir}/word_temporal_statistics.csv', \n",
    "        index=False, encoding='utf-8'\n",
    "    )\n",
    "    print(f\"   Exported temporal statistics for {len(word_temporal_stats):,} words\")\n",
    "    \n",
    "    # 4. Summary by year\n",
    "    print(\"\\n4. Creating yearly summary...\")\n",
    "    yearly_summary = unique_words.groupby('year').agg({\n",
    "        'word': 'count',\n",
    "        'rel_freq_z_score': ['mean', 'max'],\n",
    "        'frequency': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    yearly_summary.columns = ['unique_words_count', 'avg_z_score', 'max_z_score', 'total_frequency']\n",
    "    yearly_summary.to_csv(f'{output_dir}/yearly_summary.csv', encoding='utf-8')\n",
    "    print(f\"   Exported yearly summary for {len(yearly_summary)} years\")\n",
    "    \n",
    "    # 5. Simple text lists for each year\n",
    "    print(\"\\n5. Creating simple word lists per year...\")\n",
    "    year_lists_dir = f'{output_dir}/word_lists_by_year'\n",
    "    os.makedirs(year_lists_dir, exist_ok=True)\n",
    "    \n",
    "    for year in sorted(unique_words['year'].unique()):\n",
    "        year_words = unique_words[unique_words['year'] == year].nlargest(50, 'rel_freq_z_score')\n",
    "        \n",
    "        with open(f'{year_lists_dir}/unique_words_{year}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Unique words for {year} (Top 50 by Z-score)\\n\\n\")\n",
    "            for _, row in year_words.iterrows():\n",
    "                f.write(f\"{row['word']}\\t{row['frequency']}\\t{row['rel_freq_z_score']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"   Created word lists for {len(unique_words['year'].unique())} years\")\n",
    "    \n",
    "    # 6. Analysis summary report\n",
    "    print(\"\\n6. Creating analysis summary report...\")\n",
    "    with open(f'{output_dir}/analysis_summary.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# Unique Words Per Year Analysis Summary\\n\\n\")\n",
    "        f.write(f\"Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Database: {db_path}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Methodology\\n\")\n",
    "        f.write(\"- Relative frequency analysis accounting for yearly word count variations\\n\")\n",
    "        f.write(\"- Z-score calculation to identify statistically significant frequency spikes\\n\")\n",
    "        f.write(\"- Minimum Z-score threshold: 2.0 (2 standard deviations above mean)\\n\")\n",
    "        f.write(\"- Minimum absolute frequency: 20 occurrences\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Key Statistics\\n\")\n",
    "        f.write(f\"- Total unique word-year combinations analyzed: {len(enhanced_df):,}\\n\")\n",
    "        f.write(f\"- Significant temporal spikes identified: {len(unique_words):,}\\n\")\n",
    "        f.write(f\"- Unique words with temporal spikes: {unique_words['word'].nunique():,}\\n\")\n",
    "        f.write(f\"- Years covered: {unique_words['year'].min()} to {unique_words['year'].max()}\\n\")\n",
    "        f.write(f\"- Average Z-score of significant spikes: {unique_words['rel_freq_z_score'].mean():.2f}\\n\")\n",
    "        f.write(f\"- Maximum Z-score observed: {unique_words['rel_freq_z_score'].max():.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Top 10 Most Temporally Unique Words\\n\")\n",
    "        top_10 = unique_words.nlargest(10, 'rel_freq_z_score')\n",
    "        for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "            f.write(f\"{i:2d}. {row['word']} ({row['year']}) - Z-score: {row['rel_freq_z_score']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"   Created analysis summary report\")\n",
    "    \n",
    "    print(f\"\\nâœ… All exports completed successfully in: {output_dir}\")\n",
    "    print(f\"\\nðŸ“ Generated files:\")\n",
    "    print(f\"   - unique_words_complete.csv: Complete dataset\")\n",
    "    print(f\"   - top_unique_words_per_year.csv: Top words per year\")\n",
    "    print(f\"   - word_temporal_statistics.csv: Temporal statistics\")\n",
    "    print(f\"   - yearly_summary.csv: Summary by year\")\n",
    "    print(f\"   - word_lists_by_year/: Simple text lists per year\")\n",
    "    print(f\"   - analysis_summary.txt: Methodology and key findings\")\n",
    "\n",
    "# Export results\n",
    "export_unique_words_analysis(unique_words, top_per_year, word_temporal_stats, enhanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "## Key Findings Summary\n",
    "\n",
    "Summarize the key findings from the unique words per year analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key_findings(unique_words, word_temporal_stats, yearly_totals):\n",
    "    \"\"\"\n",
    "    Generate and display key findings from the analysis.\n",
    "    \n",
    "    Args:\n",
    "        unique_words: DataFrame with unique word-year combinations\n",
    "        word_temporal_stats: DataFrame with temporal statistics\n",
    "        yearly_totals: DataFrame with yearly totals\n",
    "    \"\"\"\n",
    "    if unique_words is None:\n",
    "        print(\"Cannot generate findings - analysis data not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"ðŸ” KEY FINDINGS: Unique Words Per Year Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Overall statistics\n",
    "    print(\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
    "    print(f\"   â€¢ Significant temporal word spikes identified: {len(unique_words):,}\")\n",
    "    print(f\"   â€¢ Unique words with temporal patterns: {unique_words['word'].nunique():,}\")\n",
    "    print(f\"   â€¢ Average Z-score of spikes: {unique_words['rel_freq_z_score'].mean():.2f}\")\n",
    "    print(f\"   â€¢ Strongest temporal spike (Z-score): {unique_words['rel_freq_z_score'].max():.2f}\")\n",
    "    \n",
    "    # 2. Year with most unique words\n",
    "    yearly_counts = unique_words.groupby('year').size()\n",
    "    peak_year = yearly_counts.idxmax()\n",
    "    peak_count = yearly_counts.max()\n",
    "    \n",
    "    print(f\"\\nðŸ“… TEMPORAL PATTERNS:\")\n",
    "    print(f\"   â€¢ Year with most unique words: {peak_year} ({peak_count} words)\")\n",
    "    print(f\"   â€¢ Years analyzed: {unique_words['year'].min()} to {unique_words['year'].max()}\")\n",
    "    \n",
    "    # Word volume correlation\n",
    "    if yearly_totals is not None:\n",
    "        corr_data = yearly_counts.to_frame('unique_count').merge(\n",
    "            yearly_totals.set_index('year'), left_index=True, right_index=True\n",
    "        )\n",
    "        correlation = corr_data['unique_count'].corr(corr_data['total_words'])\n",
    "        print(f\"   â€¢ Correlation between unique words and total volume: {correlation:.3f}\")\n",
    "    \n",
    "    # 3. Top temporally specific words\n",
    "    print(f\"\\nðŸŽ¯ MOST TEMPORALLY SPECIFIC WORDS:\")\n",
    "    most_specific = word_temporal_stats.nlargest(5, 'temporal_specificity')\n",
    "    for i, (_, row) in enumerate(most_specific.iterrows(), 1):\n",
    "        print(f\"   {i}. '{row['word']}' (POS: {row['pos_category']}) - \"\n",
    "              f\"Max Z-score: {row['max_z_score']:.2f}\")\n",
    "    \n",
    "    # 4. Most frequent unique words\n",
    "    print(f\"\\nðŸ“ˆ HIGHEST FREQUENCY UNIQUE WORDS:\")\n",
    "    high_freq = unique_words.nlargest(5, 'frequency')\n",
    "    for i, (_, row) in enumerate(high_freq.iterrows(), 1):\n",
    "        print(f\"   {i}. '{row['word']}' in {row['year']} - \"\n",
    "              f\"Frequency: {row['frequency']:,}, Z-score: {row['rel_freq_z_score']:.2f}\")\n",
    "    \n",
    "    # 5. POS category distribution\n",
    "    print(f\"\\nðŸ“ PART-OF-SPEECH DISTRIBUTION:\")\n",
    "    pos_dist = unique_words['pos_category'].value_counts().head(5)\n",
    "    total_unique = len(unique_words)\n",
    "    for pos, count in pos_dist.items():\n",
    "        percentage = (count / total_unique) * 100\n",
    "        print(f\"   â€¢ {pos}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 6. Sample unique words by year\n",
    "    print(f\"\\nðŸ“‹ SAMPLE UNIQUE WORDS BY YEAR:\")\n",
    "    for year in sorted(unique_words['year'].unique())[-3:]:  # Last 3 years\n",
    "        year_top = unique_words[unique_words['year'] == year].nlargest(3, 'rel_freq_z_score')\n",
    "        words_list = \", \".join([f\"'{row['word']}'\" for _, row in year_top.iterrows()])\n",
    "        print(f\"   â€¢ {year}: {words_list}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ“ Detailed results exported to: output/unique_words_analysis/\")\n",
    "    print(f\"ðŸ“ˆ Visualizations saved to: output/visualizations/\")\n",
    "\n",
    "# Generate key findings\n",
    "generate_key_findings(unique_words, word_temporal_stats, yearly_totals)\n",
    "\n",
    "# Close database connection\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"\\nâœ“ Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis successfully identified words that are used relatively more in specific years compared to their baseline usage across the entire corpus. The methodology accounts for:\n",
    "\n",
    "1. **Yearly volume bias**: Normalizes frequencies by total word count per year\n",
    "2. **Statistical significance**: Uses Z-scores to identify truly anomalous usage patterns\n",
    "3. **Temporal specificity**: Measures how concentrated a word's usage is in particular time periods\n",
    "\n",
    "### Key Outputs Generated:\n",
    "\n",
    "- **Complete dataset** of words with significant temporal spikes\n",
    "- **Year-by-year analysis** showing unique words for each time period\n",
    "- **Statistical measures** including Z-scores and lift ratios\n",
    "- **Visualizations** showing temporal patterns and trends\n",
    "- **Export files** in CSV and text formats for further analysis\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Historical analysis**: Understanding language evolution and current events\n",
    "- **Content categorization**: Identifying time-specific terminology\n",
    "- **Trend detection**: Spotting emerging or declining word usage\n",
    "- **Research**: Supporting linguistic and sociological studies\n",
    "\n",
    "This analysis provides a robust foundation for understanding how language usage varies over time and which words serve as temporal markers in the Dutch news corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}