{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a1e8d6",
   "metadata": {},
   "source": [
    "# Word Extraction (Single-Threaded) from Cleaned Articles\n",
    "\n",
    "This notebook builds Dutch word frequencies from the cleaned article texts stored by the text extraction pipeline. It reads from `output/articles_text_export.sqlite` (table `articles`) and writes a word database to `output/words_database.sqlite`.\n",
    "\n",
    "Key points:\n",
    "- Single-threaded batch processing (no multi-threading)\n",
    "- Uses spaCy Dutch model for tokenization and POS\n",
    "- Efficient SQLite upserts and yearly frequencies\n",
    "- Exports multiple word lists for downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b075868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports ready\n"
     ]
    }
   ],
   "source": [
    "# 1) Import Required Libraries\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Minimal display for VS Code\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print(\"‚úÖ Imports ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c4020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  INPUT_DB=output/articles_text_export.sqlite\n",
      "  INPUT_TABLE=articles\n",
      "  OUTPUT_DB=output/words_database.sqlite\n",
      "  BATCH_SIZE=2000\n",
      "  MIN_TEXT_LENGTH=20\n",
      "  EXCLUDE_STOPWORDS=True\n",
      "  COMMON_WORD_THRESHOLD=10\n",
      "  OUT_EXPORT_DIR=output/exports\n"
     ]
    }
   ],
   "source": [
    "# 2) Configure Paths and Parameters\n",
    "INPUT_DB = os.getenv('WORDS_INPUT_DB', 'output/articles_text_export.sqlite')\n",
    "INPUT_TABLE = os.getenv('WORDS_INPUT_TABLE', 'articles')\n",
    "OUTPUT_DB = os.getenv('WORDS_OUTPUT_DB', 'output/words_database.sqlite')\n",
    "BATCH_SIZE = int(os.getenv('WORDS_BATCH_SIZE', '2000'))\n",
    "MIN_TEXT_LENGTH = int(os.getenv('WORDS_MIN_TEXT_LENGTH', '20'))\n",
    "EXCLUDE_STOPWORDS = os.getenv('WORDS_EXCLUDE_STOPWORDS', '1') == '1'\n",
    "COMMON_WORD_THRESHOLD = int(os.getenv('WORDS_COMMON_WORD_THRESHOLD', '10'))\n",
    "\n",
    "OUT_EXPORT_DIR = os.getenv('WORDS_EXPORT_DIR', 'output/exports')\n",
    "\n",
    "print('Configuration:')\n",
    "print(f'  INPUT_DB={INPUT_DB}')\n",
    "print(f'  INPUT_TABLE={INPUT_TABLE}')\n",
    "print(f'  OUTPUT_DB={OUTPUT_DB}')\n",
    "print(f'  BATCH_SIZE={BATCH_SIZE}')\n",
    "print(f'  MIN_TEXT_LENGTH={MIN_TEXT_LENGTH}')\n",
    "print(f'  EXCLUDE_STOPWORDS={EXCLUDE_STOPWORDS}')\n",
    "print(f'  COMMON_WORD_THRESHOLD={COMMON_WORD_THRESHOLD}')\n",
    "print(f'  OUT_EXPORT_DIR={OUT_EXPORT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0765c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy available\n",
      "‚úÖ Loaded spaCy model: nl_core_news_sm\n",
      "‚úÖ Loaded spaCy model: nl_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "# 3) Install and Load spaCy Dutch Model\n",
    "try:\n",
    "    import spacy  # type: ignore\n",
    "    print('spaCy available')\n",
    "except ImportError:\n",
    "    spacy = None\n",
    "    print('spaCy not installed. Install with: pip install spacy; python -m spacy download nl_core_news_sm')\n",
    "\n",
    "nlp = None\n",
    "if 'spacy' in globals() and spacy is not None:\n",
    "    try:\n",
    "        nlp = spacy.load('nl_core_news_sm')\n",
    "        print('‚úÖ Loaded spaCy model: nl_core_news_sm')\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è Could not load nl_core_news_sm: {e}')\n",
    "        print('Install it via: python -m spacy download nl_core_news_sm')\n",
    "else:\n",
    "    print('‚ö†Ô∏è spaCy unavailable; tokenization will not run until installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55ded67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Articles table 'articles' OK, usable rows (>=MIN_TEXT_LENGTH): 295,097\n"
     ]
    }
   ],
   "source": [
    "# 4) Connect to Articles SQLite Database (read-only) and validate\n",
    "\n",
    "def connect_readonly(db_path: str) -> sqlite3.Connection:\n",
    "    uri = f\"file:{Path(db_path).as_posix()}?mode=ro\"\n",
    "    conn = sqlite3.connect(uri, uri=True)\n",
    "    conn.text_factory = str  # slightly faster conversions\n",
    "    return conn\n",
    "\n",
    "required_cols = {'article_id', 'published_time', 'published_timestamp', 'content', 'text_length'}\n",
    "\n",
    "def validate_articles_table(conn: sqlite3.Connection, table: str) -> None:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info({table})\")\n",
    "    cols = {row[1] for row in cur.fetchall()}\n",
    "    missing = required_cols - cols\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing required columns in {table}: {sorted(missing)}\")\n",
    "    # Quick row count of usable rows\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {table} WHERE content IS NOT NULL AND text_length >= ?\", (MIN_TEXT_LENGTH,))\n",
    "    total = cur.fetchone()[0]\n",
    "    print(f\"‚úÖ Articles table '{table}' OK, usable rows (>=MIN_TEXT_LENGTH): {total:,}\")\n",
    "\n",
    "# Try connecting\n",
    "if os.path.exists(INPUT_DB):\n",
    "    try:\n",
    "        art_conn = connect_readonly(INPUT_DB)\n",
    "        validate_articles_table(art_conn, INPUT_TABLE)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to open/validate articles DB: {e}\")\n",
    "        art_conn = None\n",
    "else:\n",
    "    print(f\"‚ùå Input DB not found: {INPUT_DB}\")\n",
    "    art_conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f112a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Helper: Stream Articles in Batches\n",
    "\n",
    "def stream_article_batches(conn: sqlite3.Connection, table: str, batch_size: int, min_text_len: int):\n",
    "    \"\"\"Yield batches of rows as lists of tuples (article_id, published_time, published_timestamp, content).\"\"\"\n",
    "    cur = conn.cursor()\n",
    "    # Prefer rowid pagination for stability\n",
    "    # Select only needed fields and filter early\n",
    "    last_rowid = 0\n",
    "    fetched = 0\n",
    "    while True:\n",
    "        cur.execute(\n",
    "            f\"\"\"\n",
    "            SELECT rowid, article_id, published_time, published_timestamp, content\n",
    "            FROM {table}\n",
    "            WHERE rowid > ? AND content IS NOT NULL AND text_length >= ?\n",
    "            ORDER BY rowid\n",
    "            LIMIT ?\n",
    "            \"\"\",\n",
    "            (last_rowid, min_text_len, batch_size)\n",
    "        )\n",
    "        rows = cur.fetchall()\n",
    "        if not rows:\n",
    "            break\n",
    "        last_rowid = rows[-1][0]\n",
    "        fetched += len(rows)\n",
    "        yield [(r[1], r[2], r[3], r[4]) for r in rows]\n",
    "    print(f\"üì¶ Finished streaming articles. Total yielded: {fetched:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ff7995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization helpers ready\n"
     ]
    }
   ],
   "source": [
    "# 6) Tokenization and Filtering Functions (spaCy)\n",
    "\n",
    "def should_include_token(token, exclude_stop: bool = EXCLUDE_STOPWORDS) -> bool:\n",
    "    if not token.text or not token.text.strip():\n",
    "        return False\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    if len(token.text) < 2 or len(token.text) > 25:\n",
    "        return False\n",
    "    if token.pos_ in {'PUNCT', 'SPACE', 'X'}:\n",
    "        return False\n",
    "    # Exclude long ALLCAPS acronyms\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    if exclude_stop and token.is_stop:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_words_from_text(text: str, nlp_model) -> List[Dict[str, object]]:\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    doc = nlp_model(text)\n",
    "    out: List[Dict[str, object]] = []\n",
    "    for tok in doc:\n",
    "        if should_include_token(tok):\n",
    "            out.append({\n",
    "                'word': tok.text.lower(),\n",
    "                'lemma': tok.lemma_.lower() if tok.lemma_ else tok.text.lower(),\n",
    "                'pos': tok.pos_,\n",
    "                'is_alpha': tok.is_alpha,\n",
    "                'is_stop': tok.is_stop,\n",
    "                'length': len(tok.text),\n",
    "            })\n",
    "    return out\n",
    "\n",
    "print(\"‚úÖ Tokenization helpers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb882b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ POS category mapping ready\n"
     ]
    }
   ],
   "source": [
    "# 7) POS Category Mapping\n",
    "\n",
    "def get_pos_category(pos_tag: str) -> str:\n",
    "    mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary',\n",
    "    }\n",
    "    return mapping.get(pos_tag, 'other')\n",
    "\n",
    "print('‚úÖ POS category mapping ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a612046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Word Database Schema (SQLite)\n",
    "\n",
    "def setup_word_db(db_path: str) -> sqlite3.Connection:\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Performance PRAGMAs for faster bulk upserts\n",
    "    try:\n",
    "        cur.execute(\"PRAGMA journal_mode = WAL;\")              # better concurrency and throughput\n",
    "        cur.execute(\"PRAGMA synchronous = NORMAL;\")            # safer than OFF, still faster\n",
    "        cur.execute(\"PRAGMA temp_store = MEMORY;\")\n",
    "        cur.execute(\"PRAGMA cache_size = -200000;\")            # ~200MB page cache\n",
    "        cur.execute(\"PRAGMA page_size = 4096;\")                # default; only affects new DBs\n",
    "        cur.execute(\"PRAGMA mmap_size = 3000000000;\")          # ~3GB, if supported\n",
    "        cur.execute(\"PRAGMA wal_autocheckpoint = 1000;\")\n",
    "        cur.execute(\"PRAGMA busy_timeout = 5000;\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen TEXT,\n",
    "            last_seen TEXT,\n",
    "            created_at TEXT DEFAULT (datetime('now')),\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER NOT NULL,\n",
    "            year INTEGER NOT NULL,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TEXT DEFAULT (datetime('now')),\n",
    "            notes TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_words_lookup ON words(word, lemma, pos_tag)\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_words_category ON words(pos_category)\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_yearly_word ON word_frequencies(word_id)\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_yearly_year ON word_frequencies(year)\")\n",
    "    conn.commit()\n",
    "    print(f\"‚úÖ Word DB ready at {db_path}\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ffdad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Upsert helpers ready\n"
     ]
    }
   ],
   "source": [
    "# 9) Upsert Helpers\n",
    "from itertools import groupby\n",
    "\n",
    "def ensure_words_exist(conn: sqlite3.Connection, items: List[Tuple[str, str, str, str, int]]) -> Dict[Tuple[str,str,str], int]:\n",
    "    \"\"\"\n",
    "    Ensure (word, lemma, pos_tag) exist; return mapping to ids.\n",
    "    items: list of (word, lemma, pos_tag, pos_category, year) ‚Äì year used to set first/last seen.\n",
    "    Uses a TEMP table + join to fetch IDs in one query for speed.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    # Deduplicate keys\n",
    "    to_insert: Dict[Tuple[str,str,str], Tuple[str,str,str,str,int]] = {}\n",
    "    for w, l, p, c, y in items:\n",
    "        key = (w, l, p)\n",
    "        if key not in to_insert:\n",
    "            to_insert[key] = (w, l, p, c, y)\n",
    "    data = list(to_insert.values())\n",
    "\n",
    "    # Insert new words (IGNORE on duplicates)\n",
    "    cur.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        [(w, l, p, c, f\"{y}-01-01\", f\"{y}-12-31\") for (w,l,p,c,y) in data]\n",
    "    )\n",
    "\n",
    "    # Update last_seen if needed (keep earliest first_seen as inserted)\n",
    "    cur.executemany(\n",
    "        \"\"\"\n",
    "        UPDATE words SET last_seen = CASE WHEN last_seen < ? THEN ? ELSE last_seen END\n",
    "        WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        \"\"\",\n",
    "        [(f\"{y}-12-31\", f\"{y}-12-31\", w, l, p) for (w,l,p,c,y) in data]\n",
    "    )\n",
    "\n",
    "    # Use a TEMP table to fetch IDs for all keys at once\n",
    "    cur.execute(\"CREATE TEMP TABLE IF NOT EXISTS tmp_keys (word TEXT, lemma TEXT, pos_tag TEXT)\")\n",
    "    cur.execute(\"DELETE FROM tmp_keys\")\n",
    "    cur.executemany(\"INSERT INTO tmp_keys (word, lemma, pos_tag) VALUES (?, ?, ?)\", [(w,l,p) for (w,l,p,c,y) in data])\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT w.word, w.lemma, w.pos_tag, w.id\n",
    "        FROM words w\n",
    "        JOIN tmp_keys k ON k.word = w.word AND k.lemma = w.lemma AND k.pos_tag = w.pos_tag\n",
    "        \"\"\"\n",
    "    )\n",
    "    ids: Dict[Tuple[str,str,str], int] = {}\n",
    "    for w, l, p, wid in cur.fetchall():\n",
    "        ids[(w,l,p)] = wid\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def bump_word_counts(conn: sqlite3.Connection, counts_by_id: Dict[int, int]) -> None:\n",
    "    if not counts_by_id:\n",
    "        return\n",
    "    cur = conn.cursor()\n",
    "    cur.executemany(\n",
    "        \"UPDATE words SET total_frequency = total_frequency + ? WHERE id = ?\",\n",
    "        [(cnt, wid) for wid, cnt in counts_by_id.items()]\n",
    "    )\n",
    "\n",
    "\n",
    "def bump_yearly_counts(conn: sqlite3.Connection, yearly_items: List[Tuple[int, int, int]]) -> None:\n",
    "    \"\"\"yearly_items: list of (word_id, year, count). Performs a single UPSERT per row.\"\"\"\n",
    "    if not yearly_items:\n",
    "        return\n",
    "    # Aggregate duplicates within the batch to minimize upserts\n",
    "    agg: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    for wid, yr, cnt in yearly_items:\n",
    "        agg[(wid, yr)] += cnt\n",
    "    rows = [(wid, yr, cnt) for (wid, yr), cnt in agg.items()]\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    cur.executemany(\n",
    "        \"\"\"\n",
    "        INSERT INTO word_frequencies (word_id, year, frequency)\n",
    "        VALUES (?, ?, ?)\n",
    "        ON CONFLICT(word_id, year) DO UPDATE SET\n",
    "            frequency = frequency + excluded.frequency\n",
    "        \"\"\",\n",
    "        rows\n",
    "    )\n",
    "\n",
    "print('‚úÖ Upsert helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c512f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Processing Pipeline (Single-Threaded, Batch)\n",
    "\n",
    "def parse_year(published_time: Optional[str], published_ts: Optional[float], default_year: int = 2015) -> int:\n",
    "    if published_time:\n",
    "        try:\n",
    "            return pd.to_datetime(published_time).year\n",
    "        except Exception:\n",
    "            pass\n",
    "    if published_ts is not None:\n",
    "        try:\n",
    "            return pd.to_datetime(published_ts, unit='s').year\n",
    "        except Exception:\n",
    "            pass\n",
    "    return default_year\n",
    "\n",
    "\n",
    "def process_articles_to_words(\n",
    "    articles_conn: sqlite3.Connection,\n",
    "    words_db_path: str,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    min_text_len: int = MIN_TEXT_LENGTH,\n",
    "    nlp_model = nlp,\n",
    ") -> Dict[str, int]:\n",
    "    if nlp_model is None:\n",
    "        print('‚ùå spaCy model not loaded')\n",
    "        return {}\n",
    "\n",
    "    words_conn = setup_word_db(words_db_path)\n",
    "    stats = {\n",
    "        'batches': 0,\n",
    "        'articles_processed': 0,\n",
    "        'words_extracted': 0,\n",
    "        'skipped': 0,\n",
    "        'errors': 0,\n",
    "    }\n",
    "\n",
    "    # Prepare a cursor for occasional optimize\n",
    "    wcur = words_conn.cursor()\n",
    "\n",
    "    batch_iter = stream_article_batches(articles_conn, INPUT_TABLE, batch_size, min_text_len)\n",
    "    for batch in tqdm(batch_iter, desc='Processing article batches'):\n",
    "        stats['batches'] += 1\n",
    "\n",
    "        # Aggregate counts: key = (word, lemma, pos, pos_category, year)\n",
    "        key_counter: Counter = Counter()\n",
    "\n",
    "        # Build once, reuse inner variables for speed\n",
    "        get_cat = get_pos_category\n",
    "        for (article_id, published_time, published_ts, content) in batch:\n",
    "            try:\n",
    "                year = parse_year(published_time, published_ts)\n",
    "                tokens = extract_words_from_text(content, nlp_model)\n",
    "                if not tokens:\n",
    "                    stats['skipped'] += 1\n",
    "                    continue\n",
    "                # Extend counter\n",
    "                for t in tokens:\n",
    "                    key_counter[(t['word'], t['lemma'], t['pos'], get_cat(str(t['pos'])), year)] += 1\n",
    "                stats['articles_processed'] += 1\n",
    "                stats['words_extracted'] += len(tokens)\n",
    "            except Exception:\n",
    "                stats['errors'] += 1\n",
    "                continue\n",
    "\n",
    "        if not key_counter:\n",
    "            continue\n",
    "\n",
    "        # Prepare items for upsert\n",
    "        items: List[Tuple[str,str,str,str,int]] = [(w,l,p,c,y) for (w,l,p,c,y), cnt in key_counter.items()]\n",
    "\n",
    "        with words_conn:  # single transaction per batch\n",
    "            ids_map = ensure_words_exist(words_conn, items)\n",
    "            # Aggregate by word_id\n",
    "            counts_by_id: Dict[int, int] = defaultdict(int)\n",
    "            yearly_items: List[Tuple[int,int,int]] = []\n",
    "            for (w,l,p,c,y), cnt in key_counter.items():\n",
    "                wid = ids_map.get((w,l,p))\n",
    "                if wid is None:\n",
    "                    words_conn.execute(\n",
    "                        \"INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category) VALUES (?,?,?,?)\",\n",
    "                        (w,l,p,c)\n",
    "                    )\n",
    "                    wid = words_conn.execute(\n",
    "                        \"SELECT id FROM words WHERE word=? AND lemma=? AND pos_tag=?\",\n",
    "                        (w,l,p)\n",
    "                    ).fetchone()[0]\n",
    "                counts_by_id[wid] += cnt\n",
    "                yearly_items.append((wid, y, cnt))\n",
    "\n",
    "            bump_word_counts(words_conn, counts_by_id)\n",
    "            bump_yearly_counts(words_conn, yearly_items)\n",
    "\n",
    "        # Periodic housekeeping\n",
    "        if stats['batches'] % 25 == 0:\n",
    "            try:\n",
    "                wcur.execute(\"PRAGMA optimize;\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Periodic log row\n",
    "        if stats['batches'] % 10 == 0:\n",
    "            with words_conn:\n",
    "                words_conn.execute(\n",
    "                    \"INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)\",\n",
    "                    (stats['articles_processed'], stats['words_extracted'], f\"After batch {stats['batches']}\")\n",
    "                )\n",
    "\n",
    "    # Final optimize\n",
    "    try:\n",
    "        wcur.execute(\"PRAGMA optimize;\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Final log row\n",
    "    with words_conn:\n",
    "        words_conn.execute(\n",
    "            \"INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)\",\n",
    "            (stats['articles_processed'], stats['words_extracted'], 'Final summary')\n",
    "        )\n",
    "\n",
    "    print('‚úÖ Processing complete')\n",
    "    print(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51896e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stats helper ready\n"
     ]
    }
   ],
   "source": [
    "# 11) Progress Logging and Basic Stats\n",
    "\n",
    "def print_basic_stats(db_path: str):\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"No DB at {db_path}\")\n",
    "        return\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT COUNT(*) FROM words\")\n",
    "    words_count = cur.fetchone()[0]\n",
    "    cur.execute(\"SELECT SUM(total_frequency) FROM words\")\n",
    "    total_freq = cur.fetchone()[0]\n",
    "    cur.execute(\"SELECT COUNT(*) FROM word_frequencies\")\n",
    "    yearly_rows = cur.fetchone()[0]\n",
    "    print(f\"DB stats - unique words: {words_count:,}, total freq: {total_freq:,}, yearly rows: {yearly_rows:,}\")\n",
    "    conn.close()\n",
    "\n",
    "print('‚úÖ Stats helper ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a2778b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis helpers ready\n"
     ]
    }
   ],
   "source": [
    "# 12) Analysis Queries (Top Words, POS Breakdown, Yearly Trends)\n",
    "\n",
    "def top_words(db_path: str, limit: int = 20):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT word, lemma, pos_category, total_frequency\n",
    "        FROM words\n",
    "        ORDER BY total_frequency DESC\n",
    "        LIMIT ?\n",
    "        \"\"\",\n",
    "        (limit,)\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    df = pd.DataFrame(rows, columns=['word','lemma','pos_category','total_frequency'])\n",
    "    print(df)\n",
    "\n",
    "\n",
    "def pos_breakdown(db_path: str):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"SELECT pos_category, COUNT(*), AVG(total_frequency) FROM words GROUP BY pos_category ORDER BY 2 DESC\"\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    df = pd.DataFrame(rows, columns=['pos_category','unique_words','avg_total_freq'])\n",
    "    print(df)\n",
    "\n",
    "\n",
    "def yearly_trends(db_path: str):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"SELECT year, COUNT(*), SUM(frequency) FROM word_frequencies GROUP BY year ORDER BY year\"\n",
    "    )\n",
    "    rows = cur.fetchall()\n",
    "    conn.close()\n",
    "    df = pd.DataFrame(rows, columns=['year','unique_words','total_instances'])\n",
    "    print(df)\n",
    "\n",
    "print('‚úÖ Analysis helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0acb2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Export helpers ready\n"
     ]
    }
   ],
   "source": [
    "# 13) Export Word Lists (TXT/CSV)\n",
    "import csv\n",
    "\n",
    "def export_word_lists(db_path: str = OUTPUT_DB, out_dir: str = OUT_EXPORT_DIR, common_threshold: int = COMMON_WORD_THRESHOLD):\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # All words\n",
    "    cur.execute(\"SELECT DISTINCT word FROM words ORDER BY word\")\n",
    "    words = [r[0] for r in cur.fetchall()]\n",
    "    with open(Path(out_dir)/'all_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for w in words:\n",
    "            f.write(w + '\\n')\n",
    "    print(f\"Exported all_words.txt ({len(words):,} words)\")\n",
    "\n",
    "    # Common words\n",
    "    cur.execute(\"SELECT word, total_frequency FROM words WHERE total_frequency >= ? ORDER BY total_frequency DESC\", (common_threshold,))\n",
    "    rows = cur.fetchall()\n",
    "    with open(Path(out_dir)/'common_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for w, freq in rows:\n",
    "            f.write(f\"{w}\\t{freq}\\n\")\n",
    "    print(f\"Exported common_words.txt ({len(rows):,} rows)\")\n",
    "\n",
    "    # POS specific\n",
    "    for pos in ['noun','verb','adjective','adverb']:\n",
    "        cur.execute(\"SELECT word, total_frequency FROM words WHERE pos_category=? ORDER BY total_frequency DESC\", (pos,))\n",
    "        rows = cur.fetchall()\n",
    "        with open(Path(out_dir)/(f\"{pos}_words.txt\"), 'w', encoding='utf-8') as f:\n",
    "            for w, freq in rows:\n",
    "                f.write(f\"{w}\\t{freq}\\n\")\n",
    "        print(f\"Exported {pos}_words.txt ({len(rows):,} rows)\")\n",
    "\n",
    "    # Full CSV\n",
    "    cur.execute(\"SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen FROM words ORDER BY total_frequency DESC\")\n",
    "    rows = cur.fetchall()\n",
    "    with open(Path(out_dir)/'words_full_data.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['word','lemma','pos_tag','pos_category','total_frequency','first_seen','last_seen'])\n",
    "        writer.writerows(rows)\n",
    "    print(\"Exported words_full_data.csv\")\n",
    "\n",
    "    # Game words\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT word FROM words\n",
    "        WHERE LENGTH(word) BETWEEN 4 AND 8\n",
    "          AND total_frequency >= ?\n",
    "          AND pos_category IN ('noun','verb','adjective')\n",
    "        ORDER BY total_frequency DESC\n",
    "        \"\"\",\n",
    "        (max(5, common_threshold//2),)\n",
    "    )\n",
    "    game = [r[0] for r in cur.fetchall()]\n",
    "    with open(Path(out_dir)/'game_words.txt', 'w', encoding='utf-8') as f:\n",
    "        for w in game:\n",
    "            f.write(w + '\\n')\n",
    "    print(f\"Exported game_words.txt ({len(game):,} words)\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "print('‚úÖ Export helpers ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d293bd8",
   "metadata": {},
   "source": [
    "## 14) Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available rows >= MIN_TEXT_LENGTH: 295,097\n",
      "\n",
      "üöÄ Running smoke test...\n",
      "‚úÖ Word DB ready at output/words_database.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing article batches: 591it [6:38:37, 40.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Finished streaming articles. Total yielded: 295,097\n",
      "‚úÖ Processing complete\n",
      "{'batches': 591, 'articles_processed': 295097, 'words_extracted': 53549626, 'skipped': 0, 'errors': 0}\n",
      "DB stats - unique words: 841,764, total freq: 53,549,626, yearly rows: 2,408,652\n",
      "\n",
      "üîé Top 10 words:\n",
      "        word      lemma pos_category  total_frequency\n",
      "0       jaar       jaar         noun           330465\n",
      "1       zegt     zeggen         verb           267452\n",
      "2     mensen       mens         noun           231235\n",
      "3       twee       twee       number           219437\n",
      "4       gaat       gaan         verb           188659\n",
      "5  nederland  nederland  proper_noun           149891\n",
      "6       gaan       gaan         verb           146466\n",
      "7       goed       goed    adjective           127884\n",
      "8        uur        uur         noun           126045\n",
      "9     nieuwe      nieuw    adjective           124807\n",
      "\n",
      "üìä POS breakdown:\n",
      "    pos_category  unique_words  avg_total_freq\n",
      "0           noun        351506       59.013300\n",
      "1    proper_noun        250295       42.270381\n",
      "2           verb        111027      103.682573\n",
      "3      adjective         73014       96.774523\n",
      "4          other         29876       14.050341\n",
      "5         adverb         13128      118.792276\n",
      "6        pronoun          3757       48.074261\n",
      "7     determiner          2495       35.341082\n",
      "8    preposition          2193      179.862289\n",
      "9         number          1905      462.440420\n",
      "10     auxiliary          1066       23.751407\n",
      "11   conjunction           959       86.944734\n",
      "12  interjection           543       30.243094\n",
      "\n",
      "üìà Yearly trends:\n",
      "    year  unique_words  total_instances\n",
      "0   2015        244575          5504670\n",
      "1   2016        246954          5573371\n",
      "2   2017        237036          5240988\n",
      "3   2018        242689          5196295\n",
      "4   2019        224436          4803635\n",
      "5   2020        217802          5058766\n",
      "6   2021        229222          5627297\n",
      "7   2022        228267          5150319\n",
      "8   2023        219664          4960059\n",
      "9   2024        220868          5178023\n",
      "10  2025         97139          1256203\n",
      "\n",
      "üíæ Exports:\n",
      "Exported all_words.txt (548,963 words)\n",
      "Exported common_words.txt (158,361 rows)\n",
      "Exported noun_words.txt (351,506 rows)\n",
      "Exported verb_words.txt (111,027 rows)\n",
      "Exported adjective_words.txt (73,014 rows)\n",
      "Exported adverb_words.txt (13,128 rows)\n",
      "Exported words_full_data.csv\n",
      "Exported game_words.txt (52,763 words)\n"
     ]
    }
   ],
   "source": [
    "if art_conn is None:\n",
    "    try:\n",
    "        art_conn = connect_readonly(INPUT_DB)\n",
    "        validate_articles_table(art_conn, INPUT_TABLE)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cannot open input DB for extraction: {e}\")\n",
    "        art_conn = None\n",
    "\n",
    "if art_conn and nlp is not None:\n",
    "    # Create a temporary view to limit rows\n",
    "    cur = art_conn.cursor()\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM {INPUT_TABLE} WHERE content IS NOT NULL AND text_length >= ?\", (MIN_TEXT_LENGTH,))\n",
    "    available = cur.fetchone()[0]\n",
    "    print(f\"Available rows >= MIN_TEXT_LENGTH: {available:,}\")\n",
    "\n",
    "    print(\"\\nüöÄ Running extraction...\")\n",
    "    stats = process_articles_to_words(art_conn, OUTPUT_DB, batch_size=max(100, BATCH_SIZE), min_text_len=MIN_TEXT_LENGTH, nlp_model=nlp)\n",
    "    print_basic_stats(OUTPUT_DB)\n",
    "\n",
    "    print(\"\\nüîé Top 10 words:\")\n",
    "    top_words(OUTPUT_DB, limit=10)\n",
    "\n",
    "    print(\"\\nüìä POS breakdown:\")\n",
    "    pos_breakdown(OUTPUT_DB)\n",
    "\n",
    "    print(\"\\nüìà Yearly trends:\")\n",
    "    yearly_trends(OUTPUT_DB)\n",
    "\n",
    "    print(\"\\nüíæ Exports:\")\n",
    "    export_word_lists(OUTPUT_DB, OUT_EXPORT_DIR, COMMON_WORD_THRESHOLD)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping extraction (missing articles connection or spaCy model)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
