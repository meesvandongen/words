{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ad0834",
   "metadata": {},
   "source": [
    "# Text Extraction from Dutch News Articles\n",
    "\n",
    "This notebook extracts clean, readable text from the NOS Dutch news articles feather dataset. The main focus is on properly cleaning HTML content and storing articles in a single SQLite database with native columns for fast querying.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. Improved HTML Cleaning: Proper spacing between HTML elements to avoid word concatenation\n",
    "2. SQLite Storage: One compact database file with a row per article\n",
    "3. Database-Native Schema: Explicit columns for core fields\n",
    "4. Batch Processing: Efficient processing with transaction-per-batch\n",
    "5. Indexes: Fast filtering and sorting by publication time\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "1. Load the feather dataset and examine structure\n",
    "2. Implement improved HTML cleaning with proper text separation\n",
    "3. Create text preprocessing and validation functions\n",
    "4. Store cleaned articles directly in SQLite with native columns\n",
    "5. Process the full dataset in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e076f",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for dataset processing, HTML cleaning, file operations, and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2eb5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "📊 Pandas version: 2.3.1\n",
      "🧹 BeautifulSoup available for HTML cleaning\n",
      "📁 Path and file operations ready\n",
      "⏰ DateTime handling configured\n",
      "🗄️ SQLite ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import sqlite3\n",
    "from contextlib import closing\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🧹 BeautifulSoup available for HTML cleaning\")\n",
    "print(f\"📁 Path and file operations ready\")\n",
    "print(f\"⏰ DateTime handling configured\")\n",
    "print(f\"🗄️ SQLite ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aec87f",
   "metadata": {},
   "source": [
    "## Load the Feather Dataset\n",
    "\n",
    "Load the NOS Dutch news articles dataset and examine its structure, fields, and content types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8ba8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "📁 File exists: True\n",
      "💾 File size: 503.98 MB\n",
      "🔄 Loading data...\n",
      "\n",
      "✅ Dataset loaded successfully!\n",
      "📊 Shape: (295259, 11) (rows, columns)\n",
      "💾 Memory usage: 1361.08 MB\n",
      "\n",
      "📋 Dataset Info:\n",
      "   Columns: ['channel', 'url', 'type', 'title', 'keywords', 'section', 'description', 'published_time', 'modified_time', 'image', 'content']\n",
      "   Date range: 2015-01-01 00:32:52 to 2025-03-31 23:45:01\n",
      "\n",
      "🔍 Sample data:\n",
      "                                                         title  \\\n",
      "1948                                   Euro nu ook in Litouwen   \n",
      "1949  Start 2015 vol vreugde maar ook met gewonden en inzet ME   \n",
      "1950            Letland nieuwe voorzitter van de Europese Unie   \n",
      "\n",
      "                                                                                              description  \\\n",
      "1948  Vanaf vandaag betalen ze in Litouwen met de euro. Alle Baltische landen hebben nu de Europese munt.   \n",
      "1949  Nederland is met oliebollen en vuurwerk het nieuwe jaar ingegaan, maar niet overal was de jaarwi...   \n",
      "1950                                              Vanaf vandaag neemt Letland het stokje over van Italië.   \n",
      "\n",
      "          published_time  \n",
      "1948 2015-01-01 00:32:52  \n",
      "1949 2015-01-01 01:05:57  \n",
      "1950 2015-01-01 02:32:34  \n",
      "\n",
      "🌐 HTML Content Analysis:\n",
      "   Articles with content: 295,259\n",
      "   Sample content (first 500 chars): <h1>Euro nu ook in Litouwen</h1><p>In Litouwen wordt vanaf vandaag betaald met de euro in plaats van met de litas. Hiermee is Litouwen het 19e land dat toetreedt tot de eurozone en de laatste Baltische staat die zich bij de munteenheid voegt. Estland en Letland traden al eerder toe, in 2011 en 2014.</p><p>President Dalia Grybauskaite zei gisteren: \"We zijn trots dat we aan alle voorwaarden van het Verdrag van Maastricht voldoen; dat onze economie sterk genoeg is.\" Het land, dat zich de laatste j...\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"📂 Loading dataset from: {file_path}\")\n",
    "print(f\"📁 File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"💾 File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    print(\"🔄 Loading data...\")\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "    print(f\"📊 Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"💾 Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"\\n📋 Dataset Info:\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    print(f\"   Date range: {df['published_time'].min()} to {df['published_time'].max()}\")\n",
    "    \n",
    "    # Sample data\n",
    "    print(f\"\\n🔍 Sample data:\")\n",
    "    print(df[['title', 'description', 'published_time']].head(3))\n",
    "    \n",
    "    # Check content field specifically\n",
    "    if 'content' in df.columns:\n",
    "        print(f\"\\n🌐 HTML Content Analysis:\")\n",
    "        content_with_html = df['content'].notna().sum()\n",
    "        print(f\"   Articles with content: {content_with_html:,}\")\n",
    "        \n",
    "        # Show sample HTML content\n",
    "        sample_content = df[df['content'].notna()]['content'].iloc[0][:500]\n",
    "        print(f\"   Sample content (first 500 chars): {sample_content}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ File not found! Please check the file path.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52099e",
   "metadata": {},
   "source": [
    "## HTML Content Cleaning with Proper Text Separation\n",
    "\n",
    "Create improved HTML cleaning functions that prevent word concatenation by ensuring proper spacing between HTML elements, especially headings and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef75b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing HTML cleaning with improved spacing:\n",
      "============================================================\n",
      "\n",
      "🔍 Test case 1:\n",
      "   Input:  <h2>Bloedneus</h2><h2>Adele</h2><p>Kirsten Sokol:</p>\n",
      "   Output: 'Bloedneus Adele Kirsten Sokol:'\n",
      "   Words:  ['Bloedneus', 'Adele', 'Kirsten', 'Sokol:']\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Test case 2:\n",
      "   Input:  <h1>Titel</h1><h2>Subtitel</h2><p>Dit is de inhoud van het artikel.</p><p>Tweede paragraaf.</p>\n",
      "   Output: 'Titel Subtitel Dit is de inhoud van het artikel. Tweede paragraaf.'\n",
      "   Words:  ['Titel', 'Subtitel', 'Dit', 'is', 'de', 'inhoud', 'van', 'het', 'artikel.', 'Tweede', 'paragraaf.']\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Test case 3:\n",
      "   Input:  <div><h3>Sectie</h3><ul><li>Item 1</li><li>Item 2</li></ul><p>Tekst na lijst.</p></div>\n",
      "   Output: 'Sectie Item 1 Item 2 Tekst na lijst.'\n",
      "   Words:  ['Sectie', 'Item', '1', 'Item', '2', 'Tekst', 'na', 'lijst.']\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔍 Test case 4:\n",
      "   Input:  <article><header><h1>Hoofdtitel</h1></header><section><h2>Sectie 1</h2><p>Tekst hier.</p></section></article>\n",
      "   Output: 'Hoofdtitel Sectie 1 Tekst hier.'\n",
      "   Words:  ['Hoofdtitel', 'Sectie', '1', 'Tekst', 'hier.']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content_improved(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content with proper text separation to prevent word concatenation.\n",
    "    \n",
    "    This addresses issues like <h2>Bloedneus</h2><h2>Adele</h2><p>Kirsten Sokol:</p>\n",
    "    becoming \"bloadneusadelekirsten\" by ensuring proper spacing between elements.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text with proper word separation\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements completely\n",
    "        for script in soup([\"script\", \"style\", \"noscript\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Add spacing around block-level elements to prevent concatenation\n",
    "        block_elements = [\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6',  # Headings\n",
    "            'p', 'div', 'article', 'section',     # Paragraphs and containers\n",
    "            'blockquote', 'pre', 'address',       # Text blocks\n",
    "            'li', 'dd', 'dt',                     # List items\n",
    "            'tr', 'td', 'th',                     # Table elements\n",
    "            'header', 'footer', 'nav', 'aside'    # Semantic elements\n",
    "        ]\n",
    "        \n",
    "        # Add separators around block elements\n",
    "        for tag_name in block_elements:\n",
    "            for tag in soup.find_all(tag_name):\n",
    "                # Add space before the tag content\n",
    "                if tag.string:\n",
    "                    tag.string = f\" {tag.string.strip()} \"\n",
    "                elif tag.get_text():\n",
    "                    # For tags with mixed content, wrap in spaces\n",
    "                    tag.insert_before(\" \")\n",
    "                    tag.insert_after(\" \")\n",
    "        \n",
    "        # Get text content with separator\n",
    "        text = soup.get_text(separator=' ')\n",
    "        \n",
    "        # Clean up whitespace but preserve sentence structure\n",
    "        lines = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Replace multiple spaces with single space\n",
    "                line = re.sub(r'\\s+', ' ', line)\n",
    "                lines.append(line)\n",
    "        \n",
    "        # Join lines with space and clean up\n",
    "        text = ' '.join(lines)\n",
    "        \n",
    "        # Final cleanup - remove excessive spacing\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        # Fallback to basic text extraction\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            return soup.get_text(separator=' ').strip()\n",
    "        except:\n",
    "            return str(html_content)\n",
    "\n",
    "def test_html_cleaning():\n",
    "    \"\"\"Test the HTML cleaning function with problematic examples.\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        # Original problem case\n",
    "        \"<h2>Bloedneus</h2><h2>Adele</h2><p>Kirsten Sokol:</p>\",\n",
    "        \n",
    "        # Multiple headings with content\n",
    "        \"<h1>Titel</h1><h2>Subtitel</h2><p>Dit is de inhoud van het artikel.</p><p>Tweede paragraaf.</p>\",\n",
    "        \n",
    "        # Mixed content with lists\n",
    "        \"<div><h3>Sectie</h3><ul><li>Item 1</li><li>Item 2</li></ul><p>Tekst na lijst.</p></div>\",\n",
    "        \n",
    "        # Complex nested structure\n",
    "        \"<article><header><h1>Hoofdtitel</h1></header><section><h2>Sectie 1</h2><p>Tekst hier.</p></section></article>\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Testing HTML cleaning with improved spacing:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, test_html in enumerate(test_cases, 1):\n",
    "        print(f\"\\n🔍 Test case {i}:\")\n",
    "        print(f\"   Input:  {test_html}\")\n",
    "        \n",
    "        cleaned = clean_html_content_improved(test_html)\n",
    "        print(f\"   Output: '{cleaned}'\")\n",
    "        \n",
    "        # Check for word concatenation issues\n",
    "        words = cleaned.split()\n",
    "        if len(words) > 0:\n",
    "            print(f\"   Words:  {words}\")\n",
    "            \n",
    "            # Flag potential concatenation (words longer than typical)\n",
    "            long_words = [w for w in words if len(w) > 15]\n",
    "            if long_words:\n",
    "                print(f\"   ⚠️ Long words (potential concatenation): {long_words}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Run the test\n",
    "test_html_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14b1c3",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Validation\n",
    "\n",
    "Implement text cleaning and validation functions to remove URLs, email addresses, and normalize text while preserving readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5083a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing text preprocessing functions:\n",
      "Original text:\n",
      "'Dit is een test artikel met http://example.com en contact@example.nl.\\n\\nHier is een nieuwe paragraaf     met veel spaties.\\n\\n\\nEn hier is nog een paragraaf na veel lege regels.'\n",
      "\n",
      "Processed text:\n",
      "'Dit is een test artikel met [URL] en [EMAIL].\\nHier is een nieuwe paragraaf met veel spaties.\\nEn hier is nog een paragraaf na veel lege regels.'\n",
      "\n",
      "Validation: True - Valid text content\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_for_export(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text for export while maintaining readability.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for storage\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs but keep the text readable\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '[URL]', text)\n",
    "    \n",
    "    # Remove email addresses but keep structure\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "    \n",
    "    # Clean up excessive whitespace but preserve paragraphs\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs to single space\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple newlines to double newline\n",
    "    \n",
    "    # Remove leading/trailing whitespace from lines\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    text = '\\n'.join(line for line in lines if line)  # Remove empty lines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def validate_text_content(text, min_length=20, max_length=50000):\n",
    "    \"\"\"\n",
    "    Validate text content for storage.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to validate\n",
    "        min_length (int): Minimum text length\n",
    "        max_length (int): Maximum text length\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_valid, reason)\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return False, \"Empty or whitespace-only text\"\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if len(text) < min_length:\n",
    "        return False, f\"Text too short ({len(text)} < {min_length} characters)\"\n",
    "    \n",
    "    if len(text) > max_length:\n",
    "        return False, f\"Text too long ({len(text)} > {max_length} characters)\"\n",
    "    \n",
    "    # Check for reasonable text content (not just repeated characters)\n",
    "    unique_chars = len(set(text.lower()))\n",
    "    if unique_chars < 10:\n",
    "        return False, f\"Text lacks diversity ({unique_chars} unique characters)\"\n",
    "    \n",
    "    return True, \"Valid text content\"\n",
    "\n",
    "# Test the preprocessing functions\n",
    "print(\"🧪 Testing text preprocessing functions:\")\n",
    "\n",
    "test_text = \"\"\"Dit is een test artikel met http://example.com en contact@example.nl.\n",
    "\n",
    "Hier is een nieuwe paragraaf     met veel spaties.\n",
    "\n",
    "\n",
    "En hier is nog een paragraaf na veel lege regels.\"\"\"\n",
    "\n",
    "print(f\"Original text:\\n{repr(test_text)}\")\n",
    "\n",
    "processed = preprocess_text_for_export(test_text)\n",
    "print(f\"\\nProcessed text:\\n{repr(processed)}\")\n",
    "\n",
    "is_valid, reason = validate_text_content(processed)\n",
    "print(f\"\\nValidation: {is_valid} - {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcc526",
   "metadata": {},
   "source": [
    "## SQLite Storage (Native Schema)\n",
    "\n",
    "Store each article's cleaned text in a single SQLite database using explicit columns. The database has one row per article with columns:\n",
    "\n",
    "- article_id (dataset index)\n",
    "- title\n",
    "- published_time (ISO8601), published_timestamp (epoch seconds)\n",
    "- content (cleaned text), text_length\n",
    "- processing_extracted_date, extraction_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c26201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing SQLite write for a single article:\n",
      "Testing with article: Euro nu ook in Litouwen\n",
      "Publication date: 2015-01-01 00:32:52\n",
      "✅ Saved: 1948 (1577 chars)\n",
      "\n",
      "Test Result:\n",
      "  Success: True\n",
      "  Article ID: 1948\n",
      "  Text length: 1577\n",
      "  Row in DB: ✅ (count=1)\n"
     ]
    }
   ],
   "source": [
    "def _to_py(value):\n",
    "    \"\"\"Convert numpy/pandas scalars to native Python types for SQLite.\"\"\"\n",
    "    if isinstance(value, (np.integer, )):\n",
    "        return int(value)\n",
    "    if isinstance(value, (np.floating, )):\n",
    "        return float(value)\n",
    "    if isinstance(value, (pd.Timestamp, datetime)):\n",
    "        return value.isoformat()\n",
    "    return None if pd.isna(value) else value\n",
    "\n",
    "\n",
    "def init_database(db_path):\n",
    "    \"\"\"\n",
    "    Initialize SQLite database and return a connection.\n",
    "    Creates table and indexes if they do not exist.\n",
    "    Applies performance PRAGMAs suitable for bulk inserts.\n",
    "    \"\"\"\n",
    "    db_path = str(db_path)\n",
    "    Path(db_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Performance-related PRAGMAs\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")          # Better concurrency and write throughput\n",
    "    conn.execute(\"PRAGMA synchronous=NORMAL;\")         # Good balance of safety/speed for WAL\n",
    "    conn.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "    conn.execute(\"PRAGMA cache_size = -100000;\")       # ~100MB page cache (negative => KB)\n",
    "    conn.execute(\"PRAGMA page_size = 4096;\")           # Default page size; effective on new DBs\n",
    "    conn.execute(\"PRAGMA mmap_size = 3000000000;\")     # Use memory-mapped I/O when available (~3GB)\n",
    "    conn.execute(\"PRAGMA busy_timeout = 5000;\")        # Wait for locks for up to 5s\n",
    "    # conn.execute(\"PRAGMA threads = 4;\")              # Optional (depends on SQLite build)\n",
    "\n",
    "    conn.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS articles (\n",
    "            article_id TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            published_time TEXT,\n",
    "            published_timestamp REAL,\n",
    "            content TEXT,\n",
    "            text_length INTEGER,\n",
    "            processing_extracted_date TEXT,\n",
    "            extraction_method TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_articles_published_time ON articles(published_time);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_articles_published_ts ON articles(published_timestamp);\")\n",
    "    return conn\n",
    "\n",
    "\n",
    "def _upsert_sql():\n",
    "    return (\n",
    "        \"\"\"\n",
    "        INSERT INTO articles (\n",
    "            article_id, title, published_time, published_timestamp,\n",
    "            content, text_length, processing_extracted_date, extraction_method\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(article_id) DO UPDATE SET\n",
    "            title=excluded.title,\n",
    "            published_time=excluded.published_time,\n",
    "            published_timestamp=excluded.published_timestamp,\n",
    "            content=excluded.content,\n",
    "            text_length=excluded.text_length,\n",
    "            processing_extracted_date=excluded.processing_extracted_date,\n",
    "            extraction_method=excluded.extraction_method\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _prepare_values(row, clean_text, processing_date=None):\n",
    "    if processing_date is None:\n",
    "        processing_date = datetime.now().isoformat()\n",
    "    title = row.get('title', 'untitled')\n",
    "    published_time = row.get('published_time')\n",
    "    if pd.notna(published_time):\n",
    "        if isinstance(published_time, str):\n",
    "            pub_dt = pd.to_datetime(published_time)\n",
    "        else:\n",
    "            pub_dt = published_time\n",
    "        published_time_iso = pub_dt.isoformat()\n",
    "        published_ts = pub_dt.timestamp()\n",
    "    else:\n",
    "        published_time_iso = None\n",
    "        published_ts = None\n",
    "\n",
    "    article_id = str(row.name)\n",
    "    return (\n",
    "        article_id,\n",
    "        _to_py(title) if title is not None else None,\n",
    "        published_time_iso,\n",
    "        published_ts,\n",
    "        clean_text,\n",
    "        int(len(clean_text)),\n",
    "        processing_date,\n",
    "        'improved_html_cleaning',\n",
    "    )\n",
    "\n",
    "\n",
    "def save_article_to_db(row, clean_text, conn):\n",
    "    \"\"\"\n",
    "    Insert or update a single article row into the SQLite database (native columns).\n",
    "    Returns: (success: bool, article_id: str, message: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        values = _prepare_values(row, clean_text)\n",
    "        with closing(conn.cursor()) as cur:\n",
    "            cur.execute(_upsert_sql(), values)\n",
    "        return True, str(row.name), \"Success\"\n",
    "    except Exception as e:\n",
    "        return False, \"\", str(e)\n",
    "\n",
    "\n",
    "def process_single_article(row, conn, verbose=False):\n",
    "    \"\"\"\n",
    "    Process a single article and write into SQLite DB (native schema).\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with article data\n",
    "        conn: SQLite connection\n",
    "        verbose (bool): Whether to print detailed output\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processing results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'article_id': '',\n",
    "        'text_length': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Clean HTML content only (title and description are already in the HTML)\n",
    "        final_text = \"\"\n",
    "        if pd.notna(row.get('content')):\n",
    "            clean_content = clean_html_content_improved(row['content'])\n",
    "            if clean_content:\n",
    "                final_text = preprocess_text_for_export(clean_content)\n",
    "        \n",
    "        # Validate text\n",
    "        is_valid, validation_reason = validate_text_content(final_text)\n",
    "        if not is_valid:\n",
    "            result['error'] = f\"Text validation failed: {validation_reason}\"\n",
    "            return result\n",
    "        \n",
    "        # Save to DB\n",
    "        success, article_id, message = save_article_to_db(row, final_text, conn)\n",
    "        \n",
    "        if success:\n",
    "            result['success'] = True\n",
    "            result['article_id'] = article_id\n",
    "            result['text_length'] = len(final_text)\n",
    "            if verbose:\n",
    "                print(f\"✅ Saved: {article_id} ({len(final_text)} chars)\")\n",
    "        else:\n",
    "            result['error'] = f\"DB save failed: {message}\"\n",
    "            if verbose:\n",
    "                print(f\"❌ Failed to save: {message}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        if verbose:\n",
    "            print(f\"❌ Error processing article: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the DB functions on a single row\n",
    "print(\"🧪 Testing SQLite write for a single article:\")\n",
    "\n",
    "if 'df' in locals() and df is not None and len(df) > 0:\n",
    "    test_row = df.iloc[0]\n",
    "    test_db_path = Path(\"output\") / \"test_articles.sqlite\"\n",
    "    \n",
    "    conn = init_database(test_db_path)\n",
    "    try:\n",
    "        print(f\"Testing with article: {test_row.get('title', 'No title')}\")\n",
    "        print(f\"Publication date: {test_row.get('published_time', 'No date')}\")\n",
    "        \n",
    "        result = process_single_article(test_row, conn, verbose=True)\n",
    "        \n",
    "        print(f\"\\nTest Result:\")\n",
    "        print(f\"  Success: {result['success']}\")\n",
    "        print(f\"  Article ID: {result['article_id']}\")\n",
    "        print(f\"  Text length: {result['text_length']}\")\n",
    "        if result['error']:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "        \n",
    "        # Verify row exists in DB\n",
    "        if result['success']:\n",
    "            with closing(conn.cursor()) as cur:\n",
    "                cur.execute(\"SELECT COUNT(1) FROM articles WHERE article_id = ?\", (result['article_id'],))\n",
    "                count = cur.fetchone()[0]\n",
    "                print(f\"  Row in DB: {'✅' if count == 1 else '❌'} (count={count})\")\n",
    "    finally:\n",
    "        conn.commit()\n",
    "        conn.execute(\"PRAGMA optimize;\")\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"⚠️ No dataset loaded - cannot test DB write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d6853",
   "metadata": {},
   "source": [
    "## Batch Processing Pipeline (SQLite)\n",
    "\n",
    "Implement efficient batch processing to handle large datasets while writing directly to a SQLite database with transaction-per-batch for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b0ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing batch processing pipeline (SQLite):\n",
      "Testing batch processing with 10 articles -> output\\test_batch_articles.sqlite...\n",
      "🚀 Starting batch processing of 10 articles -> output\\test_batch_articles.sqlite\n",
      "📊 Configuration:\n",
      "   Batch size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2/2 [00:00<00:00, 181.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 1: 5 successful, 0 failed\n",
      "   Total progress: 5/10 (263.2 articles/sec)\n",
      "\n",
      "✅ Batch processing to SQLite completed!\n",
      "📊 Final Statistics:\n",
      "   Total articles: 10\n",
      "   Successfully processed: 10\n",
      "   Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total text extracted: 13,979 characters\n",
      "   Processing time: 0.0 seconds\n",
      "   Average rate: 10.0 articles/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_articles_batch_sqlite(df, db_path, batch_size=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    Process articles in batches and save as rows in a SQLite database (native schema).\n",
    "    One transaction per batch for performance.\n",
    "    \n",
    "    Returns stats dict.\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting batch processing of {len(df):,} articles -> {db_path}\")\n",
    "    print(f\"📊 Configuration:\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    conn = init_database(db_path)\n",
    "    stats = {\n",
    "        'total_articles': len(df),\n",
    "        'processed': 0,\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'skipped': 0,\n",
    "        'total_text_length': 0,\n",
    "        'processing_time': 0,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Precompile SQL and use a buffer for executemany\n",
    "    upsert_sql = _upsert_sql()\n",
    "    values_buffer = []\n",
    "    buffer_target = 1000  # tuneable: number of rows per executemany\n",
    "    \n",
    "    try:\n",
    "        for batch_start in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "            batch_end = min(batch_start + batch_size, len(df))\n",
    "            batch_df = df.iloc[batch_start:batch_end]\n",
    "            batch_success = 0\n",
    "            batch_failed = 0\n",
    "            \n",
    "            with conn:  # implicit transaction\n",
    "                cur = conn.cursor()\n",
    "                try:\n",
    "                    for idx, row in batch_df.iterrows():\n",
    "                        try:\n",
    "                            # Clean & validate\n",
    "                            final_text = \"\"\n",
    "                            if pd.notna(row.get('content')):\n",
    "                                clean_content = clean_html_content_improved(row['content'])\n",
    "                                if clean_content:\n",
    "                                    final_text = preprocess_text_for_export(clean_content)\n",
    "                            is_valid, validation_reason = validate_text_content(final_text)\n",
    "                            stats['processed'] += 1\n",
    "                            if not is_valid:\n",
    "                                stats['failed'] += 1\n",
    "                                batch_failed += 1\n",
    "                                stats['errors'].append(f\"Article {idx}: {validation_reason}\")\n",
    "                                continue\n",
    "                            # Prepare values and buffer\n",
    "                            values = _prepare_values(row, final_text)\n",
    "                            values_buffer.append(values)\n",
    "                            # Flush buffer when large enough\n",
    "                            if len(values_buffer) >= buffer_target:\n",
    "                                cur.executemany(upsert_sql, values_buffer)\n",
    "                                values_buffer.clear()\n",
    "                            stats['successful'] += 1\n",
    "                            stats['total_text_length'] += len(final_text)\n",
    "                            batch_success += 1\n",
    "                        except Exception as e:\n",
    "                            stats['failed'] += 1\n",
    "                            batch_failed += 1\n",
    "                            stats['errors'].append(f\"Article {idx}: {str(e)}\")\n",
    "                    # Flush remaining in buffer at the end of the batch\n",
    "                    if values_buffer:\n",
    "                        cur.executemany(upsert_sql, values_buffer)\n",
    "                        values_buffer.clear()\n",
    "                finally:\n",
    "                    cur.close()\n",
    "            \n",
    "            if verbose and (batch_start // batch_size) % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = stats['processed'] / elapsed if elapsed > 0 else 0\n",
    "                print(f\"📊 Batch {batch_start//batch_size + 1}: {batch_success} successful, {batch_failed} failed\")\n",
    "                print(f\"   Total progress: {stats['processed']:,}/{stats['total_articles']:,} ({rate:.1f} articles/sec)\")\n",
    "\n",
    "            # Periodically run optimize to update stats\n",
    "            if (batch_start // batch_size) % 50 == 0:\n",
    "                try:\n",
    "                    conn.execute(\"PRAGMA optimize;\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "    finally:\n",
    "        try:\n",
    "            conn.execute(\"PRAGMA optimize;\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        conn.close()\n",
    "    \n",
    "    stats['processing_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Batch processing to SQLite completed!\")\n",
    "    print(f\"📊 Final Statistics:\")\n",
    "    print(f\"   Total articles: {stats['total_articles']:,}\")\n",
    "    print(f\"   Successfully processed: {stats['successful']:,}\")\n",
    "    print(f\"   Failed: {stats['failed']:,}\")\n",
    "    print(f\"   Success rate: {(stats['successful']/max(1, stats['total_articles'])*100):.1f}%\")\n",
    "    print(f\"   Total text extracted: {stats['total_text_length']:,} characters\")\n",
    "    print(f\"   Processing time: {stats['processing_time']:.1f} seconds\")\n",
    "    print(f\"   Average rate: {stats['processed']/max(1, stats['processing_time']):.1f} articles/second\")\n",
    "    \n",
    "    if stats['errors']:\n",
    "        print(f\"\\n⚠️ Error Summary (first 10):\")\n",
    "        for error in stats['errors'][:10]:\n",
    "            print(f\"   {error}\")\n",
    "        if len(stats['errors']) > 10:\n",
    "            print(f\"   ... and {len(stats['errors']) - 10} more errors\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test batch processing with a small sample into SQLite\n",
    "print(\"🧪 Testing batch processing pipeline (SQLite):\")\n",
    "\n",
    "if 'df' in locals() and df is not None:\n",
    "    test_sample = df.head(10)\n",
    "    test_db = Path(\"output\") / \"test_batch_articles.sqlite\"\n",
    "    print(f\"Testing batch processing with {len(test_sample)} articles -> {test_db}...\")\n",
    "    test_stats = process_articles_batch_sqlite(\n",
    "        df=test_sample,\n",
    "        db_path=test_db,\n",
    "        batch_size=5,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ No dataset loaded - cannot test batch processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7322b8f",
   "metadata": {},
   "source": [
    "## Export All Articles to SQLite\n",
    "\n",
    "Execute the complete pipeline to process all articles from the dataset and store them as rows in a SQLite database with clean text and core fields.\n",
    "\n",
    "⚠️ WARNING: This may process 295k+ articles and take considerable time and disk space. Only run when ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bfab478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FULL DATASET TEXT EXTRACTION TO SQLITE\n",
      "==================================================\n",
      "📊 Dataset ready: 295,259 articles\n",
      "📅 Date range: 2015-01-01 00:32:52 to 2025-03-31 23:45:01\n",
      "\n",
      "⚙️ Configuration:\n",
      "   Database path: output\\articles_text_export.sqlite\n",
      "   Batch size: 1000\n",
      "\n",
      "💾 Estimated storage requirements:\n",
      "   Estimated text size: 443.5 MB\n",
      "   Number of rows: 295,259\n",
      "\n",
      "🏁 Starting full dataset processing -> SQLite...\n",
      "🚀 Starting batch processing of 295,259 articles -> output\\articles_text_export.sqlite\n",
      "📊 Configuration:\n",
      "   Batch size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 1/296 [00:01<05:08,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 1: 1000 successful, 0 failed\n",
      "   Total progress: 1,000/295,259 (956.7 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▎         | 11/296 [00:09<04:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 11: 1000 successful, 0 failed\n",
      "   Total progress: 11,000/295,259 (1162.7 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|▋         | 21/296 [00:17<03:38,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 21: 1000 successful, 0 failed\n",
      "   Total progress: 21,000/295,259 (1193.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 31/296 [00:25<03:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 31: 1000 successful, 0 failed\n",
      "   Total progress: 31,000/295,259 (1206.1 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 41/296 [00:33<03:29,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 41: 1000 successful, 0 failed\n",
      "   Total progress: 41,000/295,259 (1208.3 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 51/296 [00:41<03:17,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 51: 1000 successful, 0 failed\n",
      "   Total progress: 51,000/295,259 (1216.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 61/296 [00:50<03:14,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 61: 1000 successful, 0 failed\n",
      "   Total progress: 61,000/295,259 (1211.8 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|██▍       | 71/296 [00:58<03:15,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 71: 1000 successful, 0 failed\n",
      "   Total progress: 71,000/295,259 (1203.4 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 81/296 [01:07<03:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 81: 1000 successful, 0 failed\n",
      "   Total progress: 81,000/295,259 (1198.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 91/296 [01:16<03:10,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 91: 1000 successful, 0 failed\n",
      "   Total progress: 91,000/295,259 (1190.9 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 101/296 [01:25<02:54,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 101: 1000 successful, 0 failed\n",
      "   Total progress: 101,000/295,259 (1179.4 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 111/296 [01:34<02:54,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 111: 1000 successful, 0 failed\n",
      "   Total progress: 111,000/295,259 (1172.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|████      | 121/296 [01:43<02:33,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 121: 1000 successful, 0 failed\n",
      "   Total progress: 121,000/295,259 (1167.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|████▍     | 131/296 [01:53<02:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 131: 1000 successful, 0 failed\n",
      "   Total progress: 131,000/295,259 (1158.3 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|████▊     | 141/296 [02:03<02:41,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 141: 1000 successful, 0 failed\n",
      "   Total progress: 141,000/295,259 (1143.8 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|█████     | 151/296 [02:13<02:29,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 151: 1000 successful, 0 failed\n",
      "   Total progress: 151,000/295,259 (1127.2 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|█████▍    | 161/296 [02:24<02:28,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 161: 1000 successful, 0 failed\n",
      "   Total progress: 161,000/295,259 (1112.5 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 171/296 [02:35<02:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 171: 1000 successful, 0 failed\n",
      "   Total progress: 171,000/295,259 (1100.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|██████    | 181/296 [02:46<02:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 181: 1000 successful, 0 failed\n",
      "   Total progress: 181,000/295,259 (1088.8 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  65%|██████▍   | 191/296 [02:57<01:58,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 191: 1000 successful, 0 failed\n",
      "   Total progress: 191,000/295,259 (1073.8 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  68%|██████▊   | 201/296 [03:10<01:56,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 201: 1000 successful, 0 failed\n",
      "   Total progress: 201,000/295,259 (1056.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  71%|███████▏  | 211/296 [03:22<01:47,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 211: 1000 successful, 0 failed\n",
      "   Total progress: 211,000/295,259 (1040.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|███████▍  | 221/296 [03:35<01:30,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 221: 1000 successful, 0 failed\n",
      "   Total progress: 221,000/295,259 (1026.6 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|███████▊  | 231/296 [03:48<01:25,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 231: 1000 successful, 0 failed\n",
      "   Total progress: 231,000/295,259 (1012.9 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  81%|████████▏ | 241/296 [04:00<01:08,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 241: 1000 successful, 0 failed\n",
      "   Total progress: 241,000/295,259 (1002.7 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  85%|████████▍ | 251/296 [04:12<00:54,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 251: 997 successful, 3 failed\n",
      "   Total progress: 251,000/295,259 (993.8 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  88%|████████▊ | 261/296 [04:24<00:42,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 261: 999 successful, 1 failed\n",
      "   Total progress: 261,000/295,259 (986.0 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|█████████▏| 271/296 [04:37<00:30,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 271: 999 successful, 1 failed\n",
      "   Total progress: 271,000/295,259 (977.4 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|█████████▍| 281/296 [04:50<00:20,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 281: 997 successful, 3 failed\n",
      "   Total progress: 281,000/295,259 (967.2 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  98%|█████████▊| 291/296 [05:03<00:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Batch 291: 997 successful, 3 failed\n",
      "   Total progress: 291,000/295,259 (959.1 articles/sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 296/296 [05:08<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Batch processing to SQLite completed!\n",
      "📊 Final Statistics:\n",
      "   Total articles: 295,259\n",
      "   Successfully processed: 295,097\n",
      "   Failed: 162\n",
      "   Success rate: 99.9%\n",
      "   Total text extracted: 713,883,339 characters\n",
      "   Processing time: 308.5 seconds\n",
      "   Average rate: 957.2 articles/second\n",
      "\n",
      "⚠️ Error Summary (first 10):\n",
      "   Article 65832: Text too long (59172 > 50000 characters)\n",
      "   Article 184098: Text too long (50351 > 50000 characters)\n",
      "   Article 184168: Text too long (52950 > 50000 characters)\n",
      "   Article 184244: Text too long (76183 > 50000 characters)\n",
      "   Article 184308: Text too long (66483 > 50000 characters)\n",
      "   Article 184346: Text too long (53228 > 50000 characters)\n",
      "   Article 184840: Text too long (52559 > 50000 characters)\n",
      "   Article 185219: Text too long (64243 > 50000 characters)\n",
      "   Article 185334: Text too long (73335 > 50000 characters)\n",
      "   Article 185456: Text too long (59028 > 50000 characters)\n",
      "   ... and 152 more errors\n",
      "\n",
      "🎉 FULL PROCESSING COMPLETED SUCCESSFULLY!\n",
      "🗄️ Database saved to: output\\articles_text_export.sqlite\n",
      "\n",
      "✅ Text extraction notebook execution complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Full dataset processing -> SQLite - EXECUTE WITH CAUTION\n",
    "print(\"🚀 FULL DATASET TEXT EXTRACTION TO SQLITE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"❌ Dataset not loaded. Please run the dataset loading cell first.\")\n",
    "else:\n",
    "    print(f\"📊 Dataset ready: {len(df):,} articles\")\n",
    "    print(f\"📅 Date range: {df['published_time'].min()} to {df['published_time'].max()}\")\n",
    "    \n",
    "    # Configuration\n",
    "    FULL_DB_PATH = Path(\"output\") / \"articles_text_export.sqlite\"\n",
    "    BATCH_SIZE = 1000  # Process 1000 articles per batch\n",
    "    \n",
    "    print(f\"\\n⚙️ Configuration:\")\n",
    "    print(f\"   Database path: {FULL_DB_PATH}\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    \n",
    "    # Estimate disk space\n",
    "    avg_text_per_article = 1500  # Rough estimate in characters\n",
    "    total_estimated_text = len(df) * avg_text_per_article\n",
    "    overhead_factor = 1.05  # index + row overhead\n",
    "    estimated_size_mb = (total_estimated_text * overhead_factor) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\n💾 Estimated storage requirements:\")\n",
    "    print(f\"   Estimated text size: {estimated_size_mb:.1f} MB\")\n",
    "    print(f\"   Number of rows: {len(df):,}\")\n",
    "    \n",
    "    # Confirmation flag\n",
    "    CONFIRM_PROCESSING = True\n",
    "    \n",
    "    if CONFIRM_PROCESSING:\n",
    "        print(f\"\\n🏁 Starting full dataset processing -> SQLite...\")\n",
    "        try:\n",
    "            final_stats = process_articles_batch_sqlite(\n",
    "                df=df,\n",
    "                db_path=FULL_DB_PATH,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=True\n",
    "            )\n",
    "            print(f\"\\n🎉 FULL PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"🗄️ Database saved to: {FULL_DB_PATH}\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n⚠️ Processing interrupted by user\")\n",
    "            print(f\"💾 Partial results available in: {FULL_DB_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during processing: {e}\")\n",
    "            print(f\"💾 Check database for partial results: {FULL_DB_PATH}\")\n",
    "    else:\n",
    "        print(f\"\\n⏸️ Processing skipped (CONFIRM_PROCESSING = False)\")\n",
    "        print(f\"💡 Set CONFIRM_PROCESSING = True to run the full export\")\n",
    "\n",
    "print(f\"\\n✅ Text extraction notebook execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648a2f5",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook extracts clean text from the Dutch news articles dataset and stores it efficiently in a SQLite database with a simple, database-native schema.\n",
    "\n",
    "### ✅ Key Features Implemented\n",
    "\n",
    "1. Improved HTML cleaning with proper spacing\n",
    "2. Clean text stored in a single SQLite file\n",
    "3. Native schema with explicit columns\n",
    "4. Fast querying using index on published_time and primary key\n",
    "5. Batch processing with transaction-per-batch for performance\n",
    "\n",
    "### 🗄️ SQLite Schema\n",
    "\n",
    "Table: articles\n",
    "- article_id TEXT PRIMARY KEY (DataFrame index)\n",
    "- title TEXT\n",
    "- published_time TEXT (ISO8601)\n",
    "- published_timestamp REAL (epoch seconds)\n",
    "- content TEXT (cleaned)\n",
    "- text_length INTEGER\n",
    "- processing_extracted_date TEXT (ISO8601)\n",
    "- extraction_method TEXT\n",
    "\n",
    "### 🔍 Example Queries\n",
    "\n",
    "- Count rows: SELECT COUNT(*) FROM articles;\n",
    "- Latest articles: SELECT article_id, title, published_time FROM articles ORDER BY published_timestamp DESC LIMIT 20;\n",
    "- Year filter: SELECT COUNT(*) FROM articles WHERE published_time LIKE '2015%';\n",
    "- Search word: SELECT article_id, title FROM articles WHERE content LIKE '%verkiezing%';\n",
    "\n",
    "### 🚀 Optional\n",
    "\n",
    "- Add full-text search with FTS5 for large-scale content search:\n",
    "  - CREATE VIRTUAL TABLE articles_fts USING fts5(article_id, content, content='articles', content_rowid='rowid');\n",
    "  - INSERT INTO articles_fts(article_id, content) SELECT article_id, content FROM articles;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
