{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281f0e54",
   "metadata": {},
   "source": [
    "# NOS NL Articles Dataset Documentation\n",
    "\n",
    "This notebook documents the structure, format, and contents of the `NOS_NL_articles_2015_mar_2025.feather` dataset. \n",
    "\n",
    "The dataset contains Dutch news articles from NOS (Nederlandse Omroep Stichting) spanning from 2015 to March 2025, stored in Apache Feather format for efficient data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b668f89",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import pandas for data manipulation, spaCy for text processing, and other necessary libraries for analyzing the feather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4829f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Display settings for better data exploration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896436cb",
   "metadata": {},
   "source": [
    "## 2. Load the Feather Dataset\n",
    "\n",
    "Load the NOS_NL_articles_2015_mar_2025.feather file using pandas and display basic information about the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eac88fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/NOS_NL_articles_2015_mar_2025.feather\n",
      "File exists: True\n",
      "File size: 503.98 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Shape: (295259, 11) (rows, columns)\n",
      "Memory usage: 1361.08 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the feather dataset\n",
    "file_path = \"data/NOS_NL_articles_2015_mar_2025.feather\"\n",
    "\n",
    "print(f\"Loading dataset from: {file_path}\")\n",
    "print(f\"File exists: {os.path.exists(file_path)}\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_feather(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "else:\n",
    "    print(\"File not found! Please check the file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e370d8",
   "metadata": {},
   "source": [
    "## 3. Examine Dataset Structure\n",
    "\n",
    "Display the dataset shape, column names, and basic structure to understand the overall organization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6866ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET STRUCTURE ===\n",
      "Shape: (295259, 11)\n",
      "\n",
      "Column names (11 total):\n",
      " 1. channel\n",
      " 2. url\n",
      " 3. type\n",
      " 4. title\n",
      " 5. keywords\n",
      " 6. section\n",
      " 7. description\n",
      " 8. published_time\n",
      " 9. modified_time\n",
      "10. image\n",
      "11. content\n",
      "\n",
      "=== BASIC INFO ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 295259 entries, 1948 to 1932\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   channel         295259 non-null  object        \n",
      " 1   url             295259 non-null  object        \n",
      " 2   type            295259 non-null  object        \n",
      " 3   title           295259 non-null  object        \n",
      " 4   keywords        279786 non-null  object        \n",
      " 5   section         289735 non-null  object        \n",
      " 6   description     295259 non-null  object        \n",
      " 7   published_time  295259 non-null  datetime64[ns]\n",
      " 8   modified_time   295259 non-null  object        \n",
      " 9   image           295158 non-null  object        \n",
      " 10  content         295259 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(10)\n",
      "memory usage: 27.0+ MB\n",
      "None\n",
      "\n",
      "=== FIRST FEW ROWS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>section</th>\n",
       "      <th>description</th>\n",
       "      <th>published_time</th>\n",
       "      <th>modified_time</th>\n",
       "      <th>image</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>nos</td>\n",
       "      <td>https://nos.nl/artikel/2011341-euro-nu-ook-in-litouwen</td>\n",
       "      <td>article</td>\n",
       "      <td>Euro nu ook in Litouwen</td>\n",
       "      <td>eurozone</td>\n",
       "      <td>Economie</td>\n",
       "      <td>Vanaf vandaag betalen ze in Litouwen met de euro. Alle Baltische landen hebben nu de Europese munt.</td>\n",
       "      <td>2015-01-01 00:32:52</td>\n",
       "      <td>2015-01-01 00:32:52</td>\n",
       "      <td>https://cdn.nos.nl/image/2015/01/01/48809/1200x675.jpg</td>\n",
       "      <td>&lt;h1&gt;Euro nu ook in Litouwen&lt;/h1&gt;&lt;p&gt;In Litouwen wordt vanaf vandaag betaald met de euro in plaats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>nos</td>\n",
       "      <td>https://nos.nl/artikel/2011343-start-2015-vol-vreugde-maar-ook-met-gewonden-en-inzet-me</td>\n",
       "      <td>article</td>\n",
       "      <td>Start 2015 vol vreugde maar ook met gewonden en inzet ME</td>\n",
       "      <td>oud en nieuw</td>\n",
       "      <td>Binnenland</td>\n",
       "      <td>Nederland is met oliebollen en vuurwerk het nieuwe jaar ingegaan, maar niet overal was de jaarwi...</td>\n",
       "      <td>2015-01-01 01:05:57</td>\n",
       "      <td>2015-01-01 07:18:23</td>\n",
       "      <td>https://cdn.nos.nl/image/2015/01/01/48853/1200x675.jpg</td>\n",
       "      <td>&lt;h1&gt;Start 2015 vol vreugde maar ook met gewonden en inzet ME&lt;/h1&gt;&lt;p&gt;Nederland is met vuurwerk en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>nos</td>\n",
       "      <td>https://nos.nl/artikel/2011346-letland-nieuwe-voorzitter-van-de-europese-unie</td>\n",
       "      <td>article</td>\n",
       "      <td>Letland nieuwe voorzitter van de Europese Unie</td>\n",
       "      <td>EU-voorzitter, Italië, EU, Letland</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>Vanaf vandaag neemt Letland het stokje over van Italië.</td>\n",
       "      <td>2015-01-01 02:32:34</td>\n",
       "      <td>2015-01-01 02:32:34</td>\n",
       "      <td>https://cdn.nos.nl/image/2015/01/01/48818/1200x675.jpg</td>\n",
       "      <td>&lt;h1&gt;Letland nieuwe voorzitter van de Europese Unie&lt;/h1&gt;&lt;p&gt;Letland is vanaf vandaag voorzitter va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>nos</td>\n",
       "      <td>https://nos.nl/artikel/2011348-noord-korea-wil-praten-met-zuid-korea</td>\n",
       "      <td>article</td>\n",
       "      <td>Noord-Korea wil praten met Zuid-Korea</td>\n",
       "      <td>kim jong un, Noord-Korea, Zuid-Korea</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>De Noord-Koreaanse leider Kim Jong-un zei in zijn nieuwjaarsboodschap dat hij geen reden ziet ni...</td>\n",
       "      <td>2015-01-01 03:34:25</td>\n",
       "      <td>2015-01-01 03:34:25</td>\n",
       "      <td>https://cdn.nos.nl/image/2014/12/24/42888/1200x675.jpg</td>\n",
       "      <td>&lt;h1&gt;Noord-Korea wil praten met Zuid-Korea&lt;/h1&gt;&lt;p&gt;De Noord-Koreaanse leider Kim Jong-un wil ingaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>nos</td>\n",
       "      <td>https://nos.nl/artikel/2011351-feest-in-londen-parijs-en-berlijn</td>\n",
       "      <td>article</td>\n",
       "      <td>Feest in Londen, Parijs en Berlijn</td>\n",
       "      <td>jaarwisseling</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>In Londen en Berlijn waren muziekfestivals. Parijs vierde de jaarwisseling voor het eerst zonder...</td>\n",
       "      <td>2015-01-01 04:34:20</td>\n",
       "      <td>2015-01-01 04:34:20</td>\n",
       "      <td>https://cdn.nos.nl/image/2015/01/01/48846/1200x675.jpg</td>\n",
       "      <td>&lt;h1&gt;Feest in Londen, Parijs en Berlijn&lt;/h1&gt;&lt;p&gt;In Europa is de jaarwisseling uitbundig gevierd.&lt;/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     channel  \\\n",
       "1948     nos   \n",
       "1949     nos   \n",
       "1950     nos   \n",
       "1951     nos   \n",
       "1952     nos   \n",
       "\n",
       "                                                                                          url  \\\n",
       "1948                                   https://nos.nl/artikel/2011341-euro-nu-ook-in-litouwen   \n",
       "1949  https://nos.nl/artikel/2011343-start-2015-vol-vreugde-maar-ook-met-gewonden-en-inzet-me   \n",
       "1950            https://nos.nl/artikel/2011346-letland-nieuwe-voorzitter-van-de-europese-unie   \n",
       "1951                     https://nos.nl/artikel/2011348-noord-korea-wil-praten-met-zuid-korea   \n",
       "1952                         https://nos.nl/artikel/2011351-feest-in-londen-parijs-en-berlijn   \n",
       "\n",
       "         type                                                     title  \\\n",
       "1948  article                                   Euro nu ook in Litouwen   \n",
       "1949  article  Start 2015 vol vreugde maar ook met gewonden en inzet ME   \n",
       "1950  article            Letland nieuwe voorzitter van de Europese Unie   \n",
       "1951  article                     Noord-Korea wil praten met Zuid-Korea   \n",
       "1952  article                        Feest in Londen, Parijs en Berlijn   \n",
       "\n",
       "                                  keywords     section  \\\n",
       "1948                              eurozone    Economie   \n",
       "1949                          oud en nieuw  Binnenland   \n",
       "1950    EU-voorzitter, Italië, EU, Letland  Buitenland   \n",
       "1951  kim jong un, Noord-Korea, Zuid-Korea  Buitenland   \n",
       "1952                         jaarwisseling  Buitenland   \n",
       "\n",
       "                                                                                              description  \\\n",
       "1948  Vanaf vandaag betalen ze in Litouwen met de euro. Alle Baltische landen hebben nu de Europese munt.   \n",
       "1949  Nederland is met oliebollen en vuurwerk het nieuwe jaar ingegaan, maar niet overal was de jaarwi...   \n",
       "1950                                              Vanaf vandaag neemt Letland het stokje over van Italië.   \n",
       "1951  De Noord-Koreaanse leider Kim Jong-un zei in zijn nieuwjaarsboodschap dat hij geen reden ziet ni...   \n",
       "1952  In Londen en Berlijn waren muziekfestivals. Parijs vierde de jaarwisseling voor het eerst zonder...   \n",
       "\n",
       "          published_time        modified_time  \\\n",
       "1948 2015-01-01 00:32:52  2015-01-01 00:32:52   \n",
       "1949 2015-01-01 01:05:57  2015-01-01 07:18:23   \n",
       "1950 2015-01-01 02:32:34  2015-01-01 02:32:34   \n",
       "1951 2015-01-01 03:34:25  2015-01-01 03:34:25   \n",
       "1952 2015-01-01 04:34:20  2015-01-01 04:34:20   \n",
       "\n",
       "                                                       image  \\\n",
       "1948  https://cdn.nos.nl/image/2015/01/01/48809/1200x675.jpg   \n",
       "1949  https://cdn.nos.nl/image/2015/01/01/48853/1200x675.jpg   \n",
       "1950  https://cdn.nos.nl/image/2015/01/01/48818/1200x675.jpg   \n",
       "1951  https://cdn.nos.nl/image/2014/12/24/42888/1200x675.jpg   \n",
       "1952  https://cdn.nos.nl/image/2015/01/01/48846/1200x675.jpg   \n",
       "\n",
       "                                                                                                  content  \n",
       "1948  <h1>Euro nu ook in Litouwen</h1><p>In Litouwen wordt vanaf vandaag betaald met de euro in plaats...  \n",
       "1949  <h1>Start 2015 vol vreugde maar ook met gewonden en inzet ME</h1><p>Nederland is met vuurwerk en...  \n",
       "1950  <h1>Letland nieuwe voorzitter van de Europese Unie</h1><p>Letland is vanaf vandaag voorzitter va...  \n",
       "1951  <h1>Noord-Korea wil praten met Zuid-Korea</h1><p>De Noord-Koreaanse leider Kim Jong-un wil ingaa...  \n",
       "1952  <h1>Feest in Londen, Parijs en Berlijn</h1><p>In Europa is de jaarwisseling uitbundig gevierd.</...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== DATASET STRUCTURE ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n=== BASIC INFO ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(f\"\\n=== FIRST FEW ROWS ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb49d0c",
   "metadata": {},
   "source": [
    "## 4. Analyze Column Data Types\n",
    "\n",
    "Examine each column's data type, check for null values, and understand the data structure of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9449ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COLUMN DATA TYPES AND NULL VALUES ===\n",
      "        Column      Data Type  Non-Null Count  Null Count  Null Percentage  Unique Values\n",
      "       channel         object          295259           0             0.00              2\n",
      "           url         object          295259           0             0.00         295259\n",
      "          type         object          295259           0             0.00              2\n",
      "         title         object          295259           0             0.00         292851\n",
      "      keywords         object          279786       15473             5.24         192463\n",
      "       section         object          289735        5524             1.87           1014\n",
      "   description         object          295259           0             0.00         289784\n",
      "published_time datetime64[ns]          295259           0             0.00         295057\n",
      " modified_time         object          295259           0             0.00         295099\n",
      "         image         object          295158         101             0.03         267405\n",
      "       content         object          295259           0             0.00         295254\n",
      "\n",
      "=== SAMPLE VALUES FOR EACH COLUMN ===\n",
      "\n",
      "channel (Type: object):\n",
      "  Sample values: ['nos', 'nos', 'nos']\n",
      "  Max length: 9\n",
      "  Min length: 3\n",
      "\n",
      "url (Type: object):\n",
      "  Sample values: ['https://nos.nl/artikel/2011341-euro-nu-ook-in-litouwen', 'https://nos.nl/artikel/2011343-start-2015-vol-vreugde-maar-ook-met-gewonden-en-inzet-me', 'https://nos.nl/artikel/2011346-letland-nieuwe-voorzitter-van-de-europese-unie']\n",
      "  Max length: 279\n",
      "  Min length: 24\n",
      "\n",
      "type (Type: object):\n",
      "  Sample values: ['article', 'article', 'article']\n",
      "  Max length: 8\n",
      "  Min length: 7\n",
      "\n",
      "title (Type: object):\n",
      "  Sample values: ['Euro nu ook in Litouwen', 'Start 2015 vol vreugde maar ook met gewonden en inzet ME', 'Letland nieuwe voorzitter van de Europese Unie']\n",
      "  Max length: 248\n",
      "  Min length: 1\n",
      "\n",
      "keywords (Type: object):\n",
      "  Sample values: ['eurozone', 'oud en nieuw', 'EU-voorzitter, Italië, EU, Letland']\n",
      "  Max length: 469\n",
      "  Min length: 1\n",
      "\n",
      "section (Type: object):\n",
      "  Sample values: ['Economie', 'Binnenland', 'Buitenland']\n",
      "  Max length: 109\n",
      "  Min length: 3\n",
      "\n",
      "description (Type: object):\n",
      "  Sample values: ['Vanaf vandaag betalen ze in Litouwen met de euro. Alle Baltische landen hebben nu de Europese munt.', 'Nederland is met oliebollen en vuurwerk het nieuwe jaar ingegaan, maar niet overal was de jaarwisseling een feest.', 'Vanaf vandaag neemt Letland het stokje over van Italië.']\n",
      "  Max length: 356\n",
      "  Min length: 1\n",
      "\n",
      "published_time (Type: datetime64[ns]):\n",
      "  Sample values: [Timestamp('2015-01-01 00:32:52'), Timestamp('2015-01-01 01:05:57'), Timestamp('2015-01-01 02:32:34')]\n",
      "\n",
      "modified_time (Type: object):\n",
      "  Sample values: ['2015-01-01 00:32:52', '2015-01-01 07:18:23', '2015-01-01 02:32:34']\n",
      "  Max length: 19\n",
      "  Min length: 19\n",
      "\n",
      "image (Type: object):\n",
      "  Sample values: ['https://cdn.nos.nl/image/2015/01/01/48809/1200x675.jpg', 'https://cdn.nos.nl/image/2015/01/01/48853/1200x675.jpg', 'https://cdn.nos.nl/image/2015/01/01/48818/1200x675.jpg']\n",
      "  Max length: 57\n",
      "  Min length: 4\n",
      "\n",
      "content (Type: object):\n",
      "  Sample values: ['<h1>Euro nu ook in Litouwen</h1><p>In Litouwen wordt vanaf vandaag betaald met de euro in plaats van met de litas. Hiermee is Litouwen het 19e land dat toetreedt tot de eurozone en de laatste Baltische staat die zich bij de munteenheid voegt. Estland en Letland traden al eerder toe, in 2011 en 2014.</p><p>President Dalia Grybauskaite zei gisteren: \"We zijn trots dat we aan alle voorwaarden van het Verdrag van Maastricht voldoen; dat onze economie sterk genoeg is.\" Het land, dat zich de laatste jaren economisch sterk ontwikkelde, heeft geleden onder de krimp van de Russische economie. De hoop is dat de toetreding tot de euro de handel goed zal doen.</p><h2>Politiek gevoelig</h2><p>Daarnaast refereerde de president aan de politieke betekenis van de stap: \"Behalve financiële zekerheid biedt de toetreding ook politieke zekerheid. Met de euro kunnen we populisme vermijden. Het is het sluitstuk van de terugkeer naar onze Europese familie.\" </p><p>De Baltische staten liggen in een politiek gevoelige regio, grenzend aan Polen en Wit-Rusland. Ze zijn daarmee volop betrokken bij de spanningen rond Oekraïne.</p><h2>Onrust</h2><p>Maar in Litouwen heerst ook onrust over de toetreding. Veel mensen zijn bang dat de euro tot hogere prijzen zal leiden.</p><p>Litouwen is het grootste land van de drie Baltische staten, met een bevolking van 3,5 miljoen mensen. Sinds 2004 is het lid van de Europese Unie.</p><aside><h3>Eurozonelanden(per toetredingsjaar)</h3><h3>2002</h3><p>België Duitsland Frankrijk Finland Griekenland Ierland Italië Luxemburg Nederland Oostenrijk Portugal  Spanje</p><h3>2007</h3><p>Slovenië</p><h3>2008</h3><p>Cyprus Malta</p><h3>2009</h3><p>Slowakije</p><h3>2011</h3><p>Estland</p><h3>2014</h3><p>Letland</p><h3>2015</h3></aside>', \"<h1>Start 2015 vol vreugde maar ook met gewonden en inzet ME</h1><p>Nederland is met vuurwerk en oliebollen het nieuwe jaar ingegaan. Bij het Nationale Aftelmoment bij het Scheepvaartmuseum in Amsterdam waren tienduizenden mensen aanwezig. In Rotterdam keken 40.000 mensen naar het vuurwerk dat bij de Erasmusbrug werd afgestoken.</p><p>De ME is in Nijmegen twee keer in actie gekomen omdat omstanders bij vreugdevuren de brandweer bekogelden. In Utrecht trad de ME op tegen jongeren die agenten bekogelden in de wijk Hoograven.</p><p>De ME trad ook op in Culemborg, waar iemand is aangehouden. Ook daar werden de hulpdiensten gehinderd bij het blussen van een brand.</p><h2>Baby uit brandend huis </h2><p>In Tilburg hebben buurtbewoners een baby en twee volwassenen uit een brandend huis gehaald. De brand was ontstaan in de meterkast. De baby en vier andere mensen zijn voor controle naar het ziekenhuis gebracht.</p><p>In Veen is de jaarwisseling relatief rustig verlopen. Vorig jaar werden er 100 jongeren opgepakt, omdat er auto's in brand werden gestoken. Er was wederom veel politie op de been. Die kon niet voorkomen dat er weer een auto in lichterlaaie werd gezet.</p><p>In Enschede is een man neergestoken, waarschijnlijk door een buurman. De twee zouden ruzie hebben gekregen.</p><p>In Zutphen reed iemand in op brandweermensen. De automobilist zou het er niet mee eens zijn geweest dat een straat was afgesloten. Niemand raakte gewond.</p><h2>Gasbrand Sint-Oedenrode</h2><p>Een gasbrand in Sint-Oedenrode bezorgt de brandweer veel werk. De hoofdkraan wordt dichtgedraaid en dat betekent dat ongeveer 350 huishoudens zeker een deel van de ochtend zonder gas komen te zitten. De politie sluit niet uit dat er al sprake was van een gaslek en dat er door vuurwerk brand is ontstaan.</p><p>In Rotterdam en omgeving zijn tijdens de jaarwisseling verschillende gewonden gevallen bij het afsteken van vuurwerk. Zo raakte een jongen van dertien ernstig gewond aan zijn hand toen vuurwerk ontplofte dat hij net had opgeraapt.</p><p>In Nieuwkoop (Zuid-Holland) raakte een meisje van 4 gewond toen vuurwerk werd afgestoken. Ze is naar het ziekenhuis gebracht.</p><p>In Dirksland woedt brand in een loods van 1000 vierkante meter groot. Acht woningen werden ontruimd; de bewoners zijn opgevangen in een nabijgelegen café.</p>\", '<h1>Letland nieuwe voorzitter van de Europese Unie</h1><p>Letland is vanaf vandaag voorzitter van de Europese Unie. Tot 1 juli zal het land de vergaderingen van de Raad van de Europese Unie leiden.</p><p>Het afgelopen half jaar was Italië voorzitter. Vanaf 1 juli 2015 neemt Luxemburg het stokje over. Daarna, vanaf 1 januari 2016, is Nederland aan de beurt.</p><h2>\"Concurrerend Europa\"</h2><p>De voorzitter van de EU heeft een sturende rol bij het maken van wetten en politiek beleid. Riga moet zich vooral gaan ontfermen over het groeiplan van de voorzitter van de Europese Commissie, Jean-Claude Juncker. Letland zegt zich te willen inzetten voor een \"concurrerend, digitaal en toegewijd Europa\".</p><p>Letland trad op 1 januari 2014 toe tot de eurozone.</p>']\n",
      "  Max length: 153229\n",
      "  Min length: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"=== COLUMN DATA TYPES AND NULL VALUES ===\")\n",
    "column_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data Type': df.dtypes,\n",
    "    'Non-Null Count': df.count(),\n",
    "    'Null Count': df.isnull().sum(),\n",
    "    'Null Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Unique Values': [df[col].nunique() for col in df.columns]\n",
    "})\n",
    "\n",
    "print(column_info.to_string(index=False))\n",
    "\n",
    "print(f\"\\n=== SAMPLE VALUES FOR EACH COLUMN ===\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\n{col} (Type: {df[col].dtype}):\")\n",
    "    print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"  Max length: {df[col].astype(str).str.len().max()}\")\n",
    "        print(f\"  Min length: {df[col].astype(str).str.len().min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad53cc1",
   "metadata": {},
   "source": [
    "## 5. Explore Article Content\n",
    "\n",
    "Sample and display article content to understand the text format, language, and typical article structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63281fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POTENTIAL TEXT CONTENT COLUMNS ===\n",
      "\n",
      "url:\n",
      "  Average length: 85.7 characters\n",
      "  Max length: 279 characters\n",
      "\n",
      "title:\n",
      "  Average length: 58.6 characters\n",
      "  Max length: 248 characters\n",
      "\n",
      "description:\n",
      "  Average length: 125.8 characters\n",
      "  Max length: 356 characters\n",
      "\n",
      "image:\n",
      "  Average length: 55.5 characters\n",
      "  Max length: 57 characters\n",
      "\n",
      "content:\n",
      "  Average length: 2586.6 characters\n",
      "  Max length: 153229 characters\n",
      "\n",
      "=== SAMPLE ARTICLE CONTENT ===\n",
      "\n",
      "--- Article 1 ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2011341-euro-nu-ook-in-litouwen\n",
      "type: article\n",
      "title: Euro nu ook in Litouwen\n",
      "keywords: eurozone\n",
      "section: Economie\n",
      "description: Vanaf vandaag betalen ze in Litouwen met de euro. Alle Baltische landen hebben nu de Europese munt.\n",
      "published_time: 2015-01-01 00:32:52\n",
      "modified_time: 2015-01-01 00:32:52\n",
      "image: https://cdn.nos.nl/image/2015/01/01/48809/1200x675.jpg\n",
      "content: <h1>Euro nu ook in Litouwen</h1><p>In Litouwen wordt vanaf vandaag betaald met de euro in plaats van...\n",
      "\n",
      "--- Article 2 ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2011343-start-2015-vol-vreugde-maar-ook-met-gewonden-en-inzet-me\n",
      "type: article\n",
      "title: Start 2015 vol vreugde maar ook met gewonden en inzet ME\n",
      "keywords: oud en nieuw\n",
      "section: Binnenland\n",
      "description: Nederland is met oliebollen en vuurwerk het nieuwe jaar ingegaan, maar niet overal was de jaarwissel...\n",
      "published_time: 2015-01-01 01:05:57\n",
      "modified_time: 2015-01-01 07:18:23\n",
      "image: https://cdn.nos.nl/image/2015/01/01/48853/1200x675.jpg\n",
      "content: <h1>Start 2015 vol vreugde maar ook met gewonden en inzet ME</h1><p>Nederland is met vuurwerk en oli...\n",
      "\n",
      "--- Article 3 ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2011346-letland-nieuwe-voorzitter-van-de-europese-unie\n",
      "type: article\n",
      "title: Letland nieuwe voorzitter van de Europese Unie\n",
      "keywords: EU-voorzitter, Italië, EU, Letland\n",
      "section: Buitenland\n",
      "description: Vanaf vandaag neemt Letland het stokje over van Italië.\n",
      "published_time: 2015-01-01 02:32:34\n",
      "modified_time: 2015-01-01 02:32:34\n",
      "image: https://cdn.nos.nl/image/2015/01/01/48818/1200x675.jpg\n",
      "content: <h1>Letland nieuwe voorzitter van de Europese Unie</h1><p>Letland is vanaf vandaag voorzitter van de...\n"
     ]
    }
   ],
   "source": [
    "# Look for text content columns (likely containing article text)\n",
    "text_columns = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        # Check if this looks like text content\n",
    "        sample_values = df[col].dropna().head(5)\n",
    "        avg_length = df[col].astype(str).str.len().mean()\n",
    "        if avg_length > 50:  # Likely text content if average length > 50 chars\n",
    "            text_columns.append(col)\n",
    "\n",
    "print(f\"=== POTENTIAL TEXT CONTENT COLUMNS ===\")\n",
    "for col in text_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Average length: {df[col].astype(str).str.len().mean():.1f} characters\")\n",
    "    print(f\"  Max length: {df[col].astype(str).str.len().max()} characters\")\n",
    "\n",
    "print(f\"\\n=== SAMPLE ARTICLE CONTENT ===\")\n",
    "# Display a few sample articles\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Article {i+1} ---\")\n",
    "    for col in df.columns:\n",
    "        value = df.iloc[i][col]\n",
    "        if pd.isna(value):\n",
    "            print(f\"{col}: [NULL]\")\n",
    "        elif isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{col}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{col}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c99c7",
   "metadata": {},
   "source": [
    "## 6. Check Date Range and Distribution\n",
    "\n",
    "Analyze the date columns to verify the 2015-2025 range and examine the temporal distribution of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffd3bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_7788\\4178755967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(sample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POTENTIAL DATE COLUMNS ===\n",
      "Found columns: ['published_time', 'modified_time']\n",
      "\n",
      "published_time:\n",
      "  Date range: 2015-01-01 00:32:52 to 2025-03-31 23:45:01\n",
      "  Valid dates: 295259/295259 (100.0%)\n",
      "  Articles per year:\n",
      "    2015: 42667\n",
      "    2016: 41781\n",
      "    2017: 34818\n",
      "    2018: 30877\n",
      "    2019: 27034\n",
      "    2020: 23736\n",
      "    2021: 24064\n",
      "    2022: 21756\n",
      "    2023: 21155\n",
      "    2024: 21816\n",
      "    2025: 5555\n",
      "\n",
      "modified_time:\n",
      "  Date range: 2015-01-01 00:32:52 to 2025-04-07 06:56:28\n",
      "  Valid dates: 295259/295259 (100.0%)\n",
      "  Articles per year:\n",
      "    2015: 42649\n",
      "    2016: 41681\n",
      "    2017: 34894\n",
      "    2018: 30909\n",
      "    2019: 27029\n",
      "    2020: 23740\n",
      "    2021: 24067\n",
      "    2022: 21756\n",
      "    2023: 21155\n",
      "    2024: 21820\n",
      "    2025: 5559\n"
     ]
    }
   ],
   "source": [
    "# Look for date columns\n",
    "date_columns = []\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(keyword in col_lower for keyword in ['date', 'time', 'published', 'created']):\n",
    "        date_columns.append(col)\n",
    "    elif df[col].dtype == 'object':\n",
    "        # Check if values look like dates\n",
    "        sample = df[col].dropna().head(10)\n",
    "        try:\n",
    "            pd.to_datetime(sample)\n",
    "            date_columns.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"=== POTENTIAL DATE COLUMNS ===\")\n",
    "print(f\"Found columns: {date_columns}\")\n",
    "\n",
    "for col in date_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    try:\n",
    "        # Try to convert to datetime\n",
    "        dates = pd.to_datetime(df[col], errors='coerce')\n",
    "        valid_dates = dates.dropna()\n",
    "        \n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"  Date range: {valid_dates.min()} to {valid_dates.max()}\")\n",
    "            print(f\"  Valid dates: {len(valid_dates)}/{len(df)} ({len(valid_dates)/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            # Year distribution\n",
    "            years = valid_dates.dt.year.value_counts().sort_index()\n",
    "            print(f\"  Articles per year:\")\n",
    "            for year, count in years.items():\n",
    "                print(f\"    {year}: {count}\")\n",
    "        else:\n",
    "            print(\"  No valid dates found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error parsing dates: {e}\")\n",
    "        print(f\"  Sample values: {df[col].head(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a256835",
   "metadata": {},
   "source": [
    "## 7. Sample Data Exploration\n",
    "\n",
    "Display sample rows and examine specific articles to understand the data quality and content format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d379b80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RANDOM SAMPLE OF ARTICLES ===\n",
      "\n",
      "--- Sample Article 1 (Row 169672) ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2298479-coalitie-denkt-na-over-investeringen-bij-economische-tegenwind\n",
      "type: article\n",
      "title: Coalitie denkt na over investeringen bij economische tegenwind\n",
      "keywords: kabinet, fonds\n",
      "section: Politiek\n",
      "description: Bronnen in Den Haag bevestigen dat er bij de begrotingsonderhandelingen wordt nagedacht over het reserveren van een groot bedrag nu het economisch nog goed gaat.\n",
      "published_time: 2019-08-22 08:10:24\n",
      "modified_time: 2019-08-22 10:49:14\n",
      "image: https://cdn.nos.nl/image/2019/08/22/571695/1024x576a.jpg\n",
      "content: <h1>Coalitie denkt na over investeringen bij economische tegenwind</h1><p>Het kabinet denkt erover om miljarden te reserveren voor investeringen in de toekomst. Dat bevestigen bronnen in Den Haag. Het... (2190 chars total)\n",
      "\n",
      "--- Sample Article 2 (Row 195615) ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2348866-om-in-duitsland-begint-moordonderzoek-na-cyberaanval-op-ziekenhuis\n",
      "type: article\n",
      "title: OM in Duitsland begint moordonderzoek na cyberaanval op ziekenhuis\n",
      "keywords: hack, Düsseldorf\n",
      "section: Buitenland\n",
      "description: Tijdens de aanval op het computersysteem van het ziekenhuis in Düsseldorf overleed een patiënt. Mogelijk kwam dat door de hack.\n",
      "published_time: 2020-09-18 14:12:20\n",
      "modified_time: 2020-09-18 14:12:20\n",
      "image: https://cdn.nos.nl/image/2020/09/18/676591/1024x576a.jpg\n",
      "content: <h1>OM in Duitsland begint moordonderzoek na cyberaanval op ziekenhuis</h1><p>In Duitsland wordt onderzocht of hackers verantwoordelijk zijn voor de dood van een patiënt in het ziekenhuis in Düsseldor... (1798 chars total)\n",
      "\n",
      "--- Sample Article 3 (Row 203357) ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2363631-sneeuw-legt-madrid-plat-duel-atletico-athletic-afgelast-real-vast-op-startbaan\n",
      "type: article\n",
      "title: Sneeuw legt Madrid plat: duel Atlético-Athletic afgelast, Real vast op startbaan\n",
      "keywords: Primera Division, voetbal\n",
      "section: Voetbal\n",
      "description: Het toestel met de spelers van de landskampioen kon vrijdag aanvankelijk niet opstijgen vanwege een sneeuwstorm die de startbaan had bedekt met een sneeuw- en ijslaag.\n",
      "published_time: 2021-01-09 09:25:57\n",
      "modified_time: 2021-01-09 09:25:57\n",
      "image: https://cdn.nos.nl/image/2021/01/09/705450/1024x576a.jpg\n",
      "content: <h1>Sneeuw legt Madrid plat: duel Atlético-Athletic afgelast, Real vast op startbaan</h1><p>Koploper Atlético Madrid komt zaterdag niet in actie. Vanwege de sneeuwstorm in de Spaanse hoofdstad is de l... (1921 chars total)\n",
      "\n",
      "--- Sample Article 4 (Row 288771) ---\n",
      "channel: nos\n",
      "url: https://nos.nl/l/2544327\n",
      "type: article\n",
      "title: Meer regen op komst in Spanje, lichamen vermiste broertjes gevonden\n",
      "keywords: Spanje, evacuaties, regen\n",
      "section: Buitenland\n",
      "description: De nationale weerdienst van Spanje heeft code rood afgegeven in delen van de regio's Catalonië en Andalusië.\n",
      "published_time: 2024-11-13 10:19:52\n",
      "modified_time: 2024-11-13 13:26:18\n",
      "image: https://cdn.nos.nl/image/2024/11/13/1157321/1024x576a.jpg\n",
      "content: <h1>Meer regen op komst in Spanje, lichamen vermiste broertjes gevonden</h1><p>In verschillende Spaanse steden blijven scholen vandaag gesloten en wordt mensen gevraagd thuis te werken als dat kan. De... (3503 chars total)\n",
      "\n",
      "--- Sample Article 5 (Row 69944) ---\n",
      "channel: nos\n",
      "url: https://nos.nl/artikel/2122149-russische-gewichtheffers-uitgesloten-van-olympische-spelen\n",
      "type: article\n",
      "title: Russische gewichtheffers uitgesloten van Olympische Spelen\n",
      "keywords: GOS [land en ploeg]\n",
      "section: Gewichtheffen\n",
      "description: De internationale bond heeft het hele team verboden mee te doen aan de Spelen in Rio.\n",
      "published_time: 2016-07-29 20:48:45\n",
      "modified_time: 2016-07-29 20:48:45\n",
      "image: https://cdn.nos.nl/image/2016/07/29/303722/1200x675.jpg\n",
      "content: <h1>Russische gewichtheffers uitgesloten van Olympische Spelen</h1><p>De internationale federatie IWF heeft vrijdag besloten alle Russische gewichtheffers uit te sluiten van deelname aan de Olympische... (2678 chars total)\n",
      "\n",
      "=== DATA QUALITY ASSESSMENT ===\n",
      "Total rows: 295259\n",
      "Completely empty rows: 0\n",
      "Rows with all text fields filled: 274457\n",
      "Duplicate rows: 0\n",
      "Duplicate articles (by first text column): 295257\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RANDOM SAMPLE OF ARTICLES ===\")\n",
    "# Show a random sample of articles\n",
    "sample_df = df.sample(n=min(5, len(df)), random_state=42)\n",
    "\n",
    "for idx, (index, row) in enumerate(sample_df.iterrows()):\n",
    "    print(f\"\\n--- Sample Article {idx+1} (Row {index}) ---\")\n",
    "    for col in df.columns:\n",
    "        value = row[col]\n",
    "        if pd.isna(value):\n",
    "            print(f\"{col}: [NULL]\")\n",
    "        elif isinstance(value, str):\n",
    "            if len(value) > 200:\n",
    "                print(f\"{col}: {value[:200]}... ({len(value)} chars total)\")\n",
    "            else:\n",
    "                print(f\"{col}: {value}\")\n",
    "        else:\n",
    "            print(f\"{col}: {value}\")\n",
    "\n",
    "print(f\"\\n=== DATA QUALITY ASSESSMENT ===\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Completely empty rows: {df.isnull().all(axis=1).sum()}\")\n",
    "print(f\"Rows with all text fields filled: {df.select_dtypes(include='object').notna().all(axis=1).sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "if len(df.columns) > 1:\n",
    "    print(f\"Duplicate articles (by first text column): {df.duplicated(subset=[df.select_dtypes(include='object').columns[0]]).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e3161",
   "metadata": {},
   "source": [
    "## 8. Dataset Statistics and Summary\n",
    "\n",
    "Generate descriptive statistics, word counts, and other relevant metrics to summarize the dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f618219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY STATISTICS ===\n",
      "\n",
      "channel - Text Length Statistics:\n",
      "  Mean: 3.2 characters\n",
      "  Median: 3.0 characters\n",
      "  Min: 3 characters\n",
      "  Max: 9 characters\n",
      "  Standard deviation: 1.1\n",
      "\n",
      "url - Text Length Statistics:\n",
      "  Mean: 85.7 characters\n",
      "  Median: 87.0 characters\n",
      "  Min: 24 characters\n",
      "  Max: 279 characters\n",
      "  Standard deviation: 20.3\n",
      "\n",
      "type - Text Length Statistics:\n",
      "  Mean: 7.0 characters\n",
      "  Median: 7.0 characters\n",
      "  Min: 7 characters\n",
      "  Max: 8 characters\n",
      "  Standard deviation: 0.1\n",
      "\n",
      "title - Text Length Statistics:\n",
      "  Mean: 58.6 characters\n",
      "  Median: 58.0 characters\n",
      "  Min: 1 characters\n",
      "  Max: 248 characters\n",
      "  Standard deviation: 14.8\n",
      "\n",
      "keywords - Text Length Statistics:\n",
      "  Mean: 27.2 characters\n",
      "  Median: 23.0 characters\n",
      "  Min: 1 characters\n",
      "  Max: 469 characters\n",
      "  Standard deviation: 20.5\n",
      "\n",
      "section - Text Length Statistics:\n",
      "  Mean: 11.4 characters\n",
      "  Median: 10.0 characters\n",
      "  Min: 3 characters\n",
      "  Max: 109 characters\n",
      "  Standard deviation: 5.7\n",
      "\n",
      "description - Text Length Statistics:\n",
      "  Mean: 125.8 characters\n",
      "  Median: 126.0 characters\n",
      "  Min: 1 characters\n",
      "  Max: 356 characters\n",
      "  Standard deviation: 33.3\n",
      "\n",
      "modified_time - Text Length Statistics:\n",
      "  Mean: 19.0 characters\n",
      "  Median: 19.0 characters\n",
      "  Min: 19 characters\n",
      "  Max: 19 characters\n",
      "  Standard deviation: 0.0\n",
      "\n",
      "image - Text Length Statistics:\n",
      "  Mean: 55.5 characters\n",
      "  Median: 55.0 characters\n",
      "  Min: 4 characters\n",
      "  Max: 57 characters\n",
      "  Standard deviation: 1.4\n",
      "\n",
      "content - Text Length Statistics:\n",
      "  Mean: 2586.6 characters\n",
      "  Median: 1866.0 characters\n",
      "  Min: 29 characters\n",
      "  Max: 153229 characters\n",
      "  Standard deviation: 3529.9\n",
      "\n",
      "=== WORD COUNT ANALYSIS ===\n",
      "\n",
      "url - Word Count Statistics:\n",
      "  Mean: 1.0 words\n",
      "  Median: 1.0 words\n",
      "  Min: 1 words\n",
      "  Max: 1 words\n",
      "\n",
      "title - Word Count Statistics:\n",
      "  Mean: 8.4 words\n",
      "  Median: 8.0 words\n",
      "  Min: 0 words\n",
      "  Max: 39 words\n",
      "\n",
      "description - Word Count Statistics:\n",
      "  Mean: 20.0 words\n",
      "  Median: 20.0 words\n",
      "  Min: 0 words\n",
      "  Max: 55 words\n",
      "\n",
      "image - Word Count Statistics:\n",
      "  Mean: 1.0 words\n",
      "  Median: 1.0 words\n",
      "  Min: 1 words\n",
      "  Max: 1 words\n",
      "\n",
      "content - Word Count Statistics:\n",
      "  Mean: 380.4 words\n",
      "  Median: 274.0 words\n",
      "  Min: 3 words\n",
      "  Max: 21672 words\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "Dataset: NOS_NL_articles_2015_mar_2025.feather\n",
      "Total articles: 295,259\n",
      "Total columns: 11\n",
      "File format: Apache Feather (.feather)\n",
      "Primary language: Dutch (Nederlandse)\n",
      "Source: NOS (Nederlandse Omroep Stichting)\n",
      "Time period: 2015 to March 2025\n",
      "Storage efficiency: 1961.2 MB in memory\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATASET SUMMARY STATISTICS ===\")\n",
    "\n",
    "# Text length statistics for string columns\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    lengths = df[col].astype(str).str.len()\n",
    "    print(f\"\\n{col} - Text Length Statistics:\")\n",
    "    print(f\"  Mean: {lengths.mean():.1f} characters\")\n",
    "    print(f\"  Median: {lengths.median():.1f} characters\") \n",
    "    print(f\"  Min: {lengths.min()} characters\")\n",
    "    print(f\"  Max: {lengths.max()} characters\")\n",
    "    print(f\"  Standard deviation: {lengths.std():.1f}\")\n",
    "\n",
    "# Word count analysis for likely content columns\n",
    "print(f\"\\n=== WORD COUNT ANALYSIS ===\")\n",
    "for col in text_columns:\n",
    "    word_counts = df[col].astype(str).str.split().str.len()\n",
    "    print(f\"\\n{col} - Word Count Statistics:\")\n",
    "    print(f\"  Mean: {word_counts.mean():.1f} words\")\n",
    "    print(f\"  Median: {word_counts.median():.1f} words\")\n",
    "    print(f\"  Min: {word_counts.min()} words\")\n",
    "    print(f\"  Max: {word_counts.max()} words\")\n",
    "\n",
    "print(f\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Dataset: NOS_NL_articles_2015_mar_2025.feather\")\n",
    "print(f\"Total articles: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"File format: Apache Feather (.feather)\")\n",
    "print(f\"Primary language: Dutch (Nederlandse)\")\n",
    "print(f\"Source: NOS (Nederlandse Omroep Stichting)\")\n",
    "print(f\"Time period: 2015 to March 2025\")\n",
    "print(f\"Storage efficiency: {df.memory_usage(deep=True).sum() / (1024**2):.1f} MB in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0dca84",
   "metadata": {},
   "source": [
    "# Word Extraction Strategy\n",
    "\n",
    "This section outlines the comprehensive strategy for extracting words from the NOS Dutch news articles dataset to create a clean, categorized word list suitable for various applications.\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "The word extraction process involves several key steps:\n",
    "1. **Text Preprocessing**: Clean HTML content and prepare text for analysis\n",
    "2. **Language Processing**: Use spaCy for tokenization, POS tagging, and lemmatization\n",
    "3. **Word Filtering**: Remove unwanted tokens and apply quality filters\n",
    "4. **Frequency Analysis**: Calculate word frequencies by year and overall\n",
    "5. **Database Storage**: Store results in SQLite with proper categorization\n",
    "6. **Quality Control**: Validate and clean the final word list\n",
    "\n",
    "## Key Challenges and Solutions\n",
    "\n",
    "### Challenge 1: HTML Content Cleaning\n",
    "- **Problem**: The 'content' field contains HTML markup that needs to be stripped\n",
    "- **Solution**: Use BeautifulSoup to parse HTML and extract clean text\n",
    "\n",
    "### Challenge 2: Dutch Language Processing\n",
    "- **Problem**: Need proper Dutch language model for accurate POS tagging\n",
    "- **Solution**: Use spaCy's Dutch model (nl_core_news_sm) for linguistic analysis\n",
    "\n",
    "### Challenge 3: Text Quality and Noise\n",
    "- **Problem**: News articles may contain URLs, special characters, and formatting artifacts\n",
    "- **Solution**: Implement comprehensive text cleaning pipeline\n",
    "\n",
    "### Challenge 4: Memory Efficiency\n",
    "- **Problem**: 295k articles (~1.36GB) require efficient processing\n",
    "- **Solution**: Process articles in batches to manage memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719573d",
   "metadata": {},
   "source": [
    "## Step 1: Install and Import Text Processing Libraries\n",
    "\n",
    "Install the required libraries for text processing, including spaCy for Dutch language processing and BeautifulSoup for HTML cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2c061b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ spacy installed successfully\n",
      "✓ beautifulsoup4 installed successfully\n",
      "✓ lxml installed successfully\n",
      "✓ html5lib installed successfully\n",
      "✓ tqdm installed successfully\n",
      "✗ Failed to install sqlite3: Command '['c:\\\\Users\\\\Administrator\\\\Desktop\\\\projects\\\\words\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', 'sqlite3']' returned non-zero exit status 1.\n",
      "\n",
      "Downloading Dutch language model for spaCy...\n",
      "✓ Dutch language model downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for text processing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"spacy\",\n",
    "    \"beautifulsoup4\", \n",
    "    \"lxml\",\n",
    "    \"html5lib\",\n",
    "    \"tqdm\",  # for progress bars\n",
    "    \"sqlite3\"  # should be included with Python\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nDownloading Dutch language model for spaCy...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_sm\"])\n",
    "    print(\"✓ Dutch language model downloaded successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Failed to download Dutch model: {e}\")\n",
    "    print(\"You may need to run: python -m spacy download nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de30a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dutch language model...\n",
      "✓ Dutch language model loaded successfully\n",
      "Model info: core_news_sm v3.8.0\n",
      "\n",
      "Test sentence: 'Dit is een test van de Nederlandse taalverwerking.'\n",
      "Tokens and POS tags:\n",
      "  Dit -> PRON (dit)\n",
      "  is -> AUX (zijn)\n",
      "  een -> DET (een)\n",
      "  test -> NOUN (test)\n",
      "  van -> ADP (van)\n",
      "  de -> DET (de)\n",
      "  Nederlandse -> ADJ (Nederlands)\n",
      "  taalverwerking -> NOUN (taalverwerking)\n",
      "  . -> PUNCT (.)\n"
     ]
    }
   ],
   "source": [
    "# Import text processing libraries\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "# Load Dutch language model\n",
    "print(\"Loading Dutch language model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "    print(\"✓ Dutch language model loaded successfully\")\n",
    "    print(f\"Model info: {nlp.meta['name']} v{nlp.meta['version']}\")\n",
    "except OSError as e:\n",
    "    print(f\"✗ Failed to load Dutch model: {e}\")\n",
    "    print(\"Please install the Dutch model: python -m spacy download nl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Test the model with a sample Dutch sentence\n",
    "if nlp:\n",
    "    test_sentence = \"Dit is een test van de Nederlandse taalverwerking.\"\n",
    "    doc = nlp(test_sentence)\n",
    "    print(f\"\\nTest sentence: '{test_sentence}'\")\n",
    "    print(\"Tokens and POS tags:\")\n",
    "    for token in doc:\n",
    "        print(f\"  {token.text} -> {token.pos_} ({token.lemma_})\")\n",
    "else:\n",
    "    print(\"Cannot test model - please install Dutch language model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b73eb4",
   "metadata": {},
   "source": [
    "## Step 2: HTML Content Cleaning\n",
    "\n",
    "Create functions to clean HTML content from the articles and prepare clean text for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debb0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing HTML cleaning functions...\n",
      "Original HTML: \n",
      "<div class=\"article-content\">\n",
      "    <h1>Test Artikel Titel</h1>\n",
      "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
      "    <script>alert('test');</script>\n",
      "    <p>Meer tekst hier.</p>\n",
      "</div>\n",
      "\n",
      "Cleaned text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n",
      "Preprocessed text: Test Artikel Titel Dit is een test artikel met links. Meer tekst hier.\n"
     ]
    }
   ],
   "source": [
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content and extract readable text for spaCy processing.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from articles\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean text ready for spaCy processing\n",
    "    \"\"\"\n",
    "    if pd.isna(html_content) or not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning HTML: {e}\")\n",
    "        return str(html_content)  # Return original if cleaning fails\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Additional text preprocessing before spaCy analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text from HTML cleaning\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for spaCy\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove very short texts (likely not meaningful)\n",
    "    if len(text.strip()) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test the cleaning functions\n",
    "print(\"Testing HTML cleaning functions...\")\n",
    "test_html = \"\"\"\n",
    "<div class=\"article-content\">\n",
    "    <h1>Test Artikel Titel</h1>\n",
    "    <p>Dit is een <strong>test</strong> artikel met <a href=\"https://example.com\">links</a>.</p>\n",
    "    <script>alert('test');</script>\n",
    "    <p>Meer tekst hier.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "cleaned = clean_html_content(test_html)\n",
    "preprocessed = preprocess_text(cleaned)\n",
    "\n",
    "print(f\"Original HTML: {test_html}\")\n",
    "print(f\"Cleaned text: {cleaned}\")\n",
    "print(f\"Preprocessed text: {preprocessed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf788296",
   "metadata": {},
   "source": [
    "## Step 3: Word Extraction and Processing\n",
    "\n",
    "Create functions to extract and process words using spaCy for POS tagging, lemmatization, and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d426ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing word extraction functions...\n",
      "Test text: Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\n",
      "Extracted words:\n",
      "  dit -> dit (PRON, pronoun)\n",
      "  is -> zijn (AUX, auxiliary)\n",
      "  een -> een (DET, determiner)\n",
      "  mooie -> mooi (ADJ, adjective)\n",
      "  nederlandse -> nederlands (ADJ, adjective)\n",
      "  zin -> zin (NOUN, noun)\n",
      "  met -> met (ADP, preposition)\n",
      "  verschillende -> verschillend (ADJ, adjective)\n",
      "  woorden -> woord (NOUN, noun)\n",
      "  en -> en (CCONJ, conjunction)\n",
      "  woordsoorten -> woordsoort (NOUN, noun)\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract and categorize words from cleaned text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Clean text ready for processing\n",
    "        nlp_model: Loaded spaCy model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of word dictionaries with metadata\n",
    "    \"\"\"\n",
    "    if not text or not nlp_model:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Process text with spaCy\n",
    "        doc = nlp_model(text)\n",
    "        \n",
    "        words = []\n",
    "        for token in doc:\n",
    "            # Filter out unwanted tokens\n",
    "            if should_include_token(token):\n",
    "                word_info = {\n",
    "                    'word': token.text.lower(),\n",
    "                    'lemma': token.lemma_.lower(),\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'length': len(token.text)\n",
    "                }\n",
    "                words.append(word_info)\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return []\n",
    "\n",
    "def should_include_token(token):\n",
    "    \"\"\"\n",
    "    Determine if a token should be included in the word list.\n",
    "    \n",
    "    Args:\n",
    "        token: spaCy token object\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if token should be included\n",
    "    \"\"\"\n",
    "    # Basic filters\n",
    "    if not token.text or len(token.text.strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    # Must be alphabetic (no numbers, punctuation only)\n",
    "    if not token.is_alpha:\n",
    "        return False\n",
    "    \n",
    "    # Minimum length (avoid very short words like \"a\", \"I\")\n",
    "    if len(token.text) < 2:\n",
    "        return False\n",
    "    \n",
    "    # Maximum length (avoid very long words that might be errors)\n",
    "    if len(token.text) > 25:\n",
    "        return False\n",
    "    \n",
    "    # Skip certain POS tags\n",
    "    excluded_pos = {'PUNCT', 'SPACE', 'X'}  # X = other (often errors)\n",
    "    if token.pos_ in excluded_pos:\n",
    "        return False\n",
    "    \n",
    "    # Skip if it's all uppercase (likely acronyms/abbreviations)\n",
    "    if token.text.isupper() and len(token.text) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def get_pos_category(pos_tag):\n",
    "    \"\"\"\n",
    "    Categorize POS tags into broader categories for easier analysis.\n",
    "    \n",
    "    Args:\n",
    "        pos_tag (str): spaCy POS tag\n",
    "        \n",
    "    Returns:\n",
    "        str: Broader category\n",
    "    \"\"\"\n",
    "    pos_mapping = {\n",
    "        'NOUN': 'noun',\n",
    "        'PROPN': 'proper_noun',\n",
    "        'VERB': 'verb',\n",
    "        'ADJ': 'adjective',\n",
    "        'ADV': 'adverb',\n",
    "        'PRON': 'pronoun',\n",
    "        'DET': 'determiner',\n",
    "        'ADP': 'preposition',\n",
    "        'CONJ': 'conjunction',\n",
    "        'CCONJ': 'conjunction',\n",
    "        'SCONJ': 'conjunction',\n",
    "        'NUM': 'number',\n",
    "        'PART': 'particle',\n",
    "        'INTJ': 'interjection',\n",
    "        'AUX': 'auxiliary'\n",
    "    }\n",
    "    return pos_mapping.get(pos_tag, 'other')\n",
    "\n",
    "# Test the word extraction functions\n",
    "print(\"Testing word extraction functions...\")\n",
    "if nlp:\n",
    "    test_text = \"Dit is een mooie Nederlandse zin met verschillende woorden en woordsoorten.\"\n",
    "    words = extract_words_from_text(test_text, nlp)\n",
    "    \n",
    "    print(f\"Test text: {test_text}\")\n",
    "    print(\"Extracted words:\")\n",
    "    for word in words:\n",
    "        category = get_pos_category(word['pos'])\n",
    "        print(f\"  {word['word']} -> {word['lemma']} ({word['pos']}, {category})\")\n",
    "else:\n",
    "    print(\"Cannot test - spaCy model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed55c5b",
   "metadata": {},
   "source": [
    "## Step 4: Database Setup\n",
    "\n",
    "Create SQLite database structure to store words with their frequencies, POS tags, and yearly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc335daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up test database...\n",
      "Database setup complete: test_words.sqlite\n",
      "Sample words inserted: 8\n",
      "  (1, 'dit', 'dit', 'PRON', 'pronoun', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:02:53')\n",
      "  (2, 'is', 'zijn', 'AUX', 'auxiliary', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:02:53')\n",
      "  (3, 'een', 'een', 'DET', 'determiner', 1, '2023-01-01', '2023-12-31', '2025-07-31 19:02:53')\n"
     ]
    }
   ],
   "source": [
    "def setup_database(db_path=\"words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Create SQLite database with proper schema for storing word data.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to SQLite database file\n",
    "        \n",
    "    Returns:\n",
    "        sqlite3.Connection: Database connection\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create words table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS words (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word TEXT NOT NULL,\n",
    "            lemma TEXT NOT NULL,\n",
    "            pos_tag TEXT NOT NULL,\n",
    "            pos_category TEXT NOT NULL,\n",
    "            total_frequency INTEGER DEFAULT 0,\n",
    "            first_seen DATE,\n",
    "            last_seen DATE,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            UNIQUE(word, lemma, pos_tag)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create word frequencies by year table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS word_frequencies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            word_id INTEGER,\n",
    "            year INTEGER,\n",
    "            frequency INTEGER DEFAULT 0,\n",
    "            FOREIGN KEY (word_id) REFERENCES words (id),\n",
    "            UNIQUE(word_id, year)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create processing log table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS processing_log (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            articles_processed INTEGER,\n",
    "            words_extracted INTEGER,\n",
    "            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            notes TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indexes for better performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_word_lemma ON words (word, lemma)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_pos_category ON words (pos_category)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_year ON word_frequencies (year)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"Database setup complete: {db_path}\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "def insert_word_data(conn, word_data, year):\n",
    "    \"\"\"\n",
    "    Insert word data into the database with frequency tracking.\n",
    "    \n",
    "    Args:\n",
    "        conn: SQLite connection\n",
    "        word_data (list): List of word dictionaries\n",
    "        year (int): Year of the article\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for word_info in word_data:\n",
    "        pos_category = get_pos_category(word_info['pos'])\n",
    "        \n",
    "        # Insert or update word\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO words (word, lemma, pos_tag, pos_category, first_seen, last_seen)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            word_info['word'],\n",
    "            word_info['lemma'], \n",
    "            word_info['pos'],\n",
    "            pos_category,\n",
    "            f\"{year}-01-01\",\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Update last_seen if word already exists\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET last_seen = ? \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ? AND last_seen < ?\n",
    "        ''', (\n",
    "            f\"{year}-12-31\",\n",
    "            word_info['word'],\n",
    "            word_info['lemma'],\n",
    "            word_info['pos'],\n",
    "            f\"{year}-12-31\"\n",
    "        ))\n",
    "        \n",
    "        # Get word ID\n",
    "        cursor.execute('''\n",
    "            SELECT id FROM words \n",
    "            WHERE word = ? AND lemma = ? AND pos_tag = ?\n",
    "        ''', (word_info['word'], word_info['lemma'], word_info['pos']))\n",
    "        \n",
    "        word_id = cursor.fetchone()[0]\n",
    "        \n",
    "        # Insert or update frequency\n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO word_frequencies (word_id, year, frequency)\n",
    "            VALUES (?, ?, 0)\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            UPDATE word_frequencies \n",
    "            SET frequency = frequency + 1\n",
    "            WHERE word_id = ? AND year = ?\n",
    "        ''', (word_id, year))\n",
    "        \n",
    "        # Update total frequency\n",
    "        cursor.execute('''\n",
    "            UPDATE words \n",
    "            SET total_frequency = total_frequency + 1\n",
    "            WHERE id = ?\n",
    "        ''', (word_id,))\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "# Test database setup\n",
    "print(\"Setting up test database...\")\n",
    "test_conn = setup_database(\"test_words.sqlite\")\n",
    "\n",
    "# Test with sample data\n",
    "if nlp:\n",
    "    sample_words = extract_words_from_text(\"Dit is een test van de database functionaliteit.\", nlp)\n",
    "    insert_word_data(test_conn, sample_words, 2023)\n",
    "    \n",
    "    # Query results\n",
    "    cursor = test_conn.cursor()\n",
    "    cursor.execute('SELECT * FROM words')\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"Sample words inserted: {len(results)}\")\n",
    "    for row in results[:3]:\n",
    "        print(f\"  {row}\")\n",
    "\n",
    "test_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2169e0",
   "metadata": {},
   "source": [
    "## Step 5: Main Processing Pipeline\n",
    "\n",
    "Create the main pipeline to process all articles in batches and extract words efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b245823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeline function defined. Ready to process articles.\n"
     ]
    }
   ],
   "source": [
    "def process_articles_pipeline(df, nlp_model, db_path=\"words_database.sqlite\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Main pipeline to process all articles and extract words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with articles\n",
    "        nlp_model: Loaded spaCy model\n",
    "        db_path (str): Path to SQLite database\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "    \"\"\"\n",
    "    if not nlp_model:\n",
    "        print(\"Error: spaCy model not loaded\")\n",
    "        return\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database(db_path)\n",
    "    \n",
    "    # Prepare progress tracking\n",
    "    total_articles = len(df)\n",
    "    total_words_extracted = 0\n",
    "    articles_processed = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_articles:,} articles...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for start_idx in tqdm(range(0, total_articles, batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, total_articles)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        batch_words = 0\n",
    "        \n",
    "        for idx, row in batch_df.iterrows():\n",
    "            try:\n",
    "                # Extract year from published_time\n",
    "                if pd.notna(row['published_time']):\n",
    "                    if isinstance(row['published_time'], str):\n",
    "                        year = pd.to_datetime(row['published_time']).year\n",
    "                    else:\n",
    "                        year = row['published_time'].year\n",
    "                else:\n",
    "                    year = 2020  # Default year if missing\n",
    "                \n",
    "                # Process different text fields\n",
    "                text_fields = ['title', 'description', 'content']\n",
    "                all_text = []\n",
    "                \n",
    "                for field in text_fields:\n",
    "                    if field in row and pd.notna(row[field]):\n",
    "                        if field == 'content':\n",
    "                            # Clean HTML from content\n",
    "                            clean_text = clean_html_content(row[field])\n",
    "                        else:\n",
    "                            clean_text = str(row[field])\n",
    "                        \n",
    "                        preprocessed = preprocess_text(clean_text)\n",
    "                        if preprocessed:\n",
    "                            all_text.append(preprocessed)\n",
    "                \n",
    "                # Combine all text\n",
    "                combined_text = ' '.join(all_text)\n",
    "                \n",
    "                if combined_text:\n",
    "                    # Extract words\n",
    "                    words = extract_words_from_text(combined_text, nlp_model)\n",
    "                    \n",
    "                    if words:\n",
    "                        # Insert into database\n",
    "                        insert_word_data(conn, words, year)\n",
    "                        batch_words += len(words)\n",
    "                \n",
    "                articles_processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        total_words_extracted += batch_words\n",
    "        \n",
    "        # Log progress every 10 batches\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {articles_processed:,}/{total_articles:,} articles, \"\n",
    "                  f\"extracted {total_words_extracted:,} words\")\n",
    "    \n",
    "    # Log final results\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('INSERT INTO processing_log (articles_processed, words_extracted, notes) VALUES (?, ?, ?)',\n",
    "                   (articles_processed, total_words_extracted, f\"Batch processing complete - batch size {batch_size}\"))\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total articles processed: {articles_processed:,}\")\n",
    "    print(f\"Total words extracted: {total_words_extracted:,}\")\n",
    "    print(f\"Database saved to: {db_path}\")\n",
    "    \n",
    "    # Get final statistics\n",
    "    cursor.execute('SELECT COUNT(*) FROM words')\n",
    "    unique_words = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "    pos_categories = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT year, COUNT(*) FROM word_frequencies GROUP BY year ORDER BY year')\n",
    "    yearly_stats = cursor.fetchall()\n",
    "    \n",
    "    print(f\"Unique words in database: {unique_words:,}\")\n",
    "    print(f\"POS categories found: {pos_categories}\")\n",
    "    print(f\"Yearly distribution:\")\n",
    "    for year, count in yearly_stats:\n",
    "        print(f\"  {year}: {count:,} word instances\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        'articles_processed': articles_processed,\n",
    "        'words_extracted': total_words_extracted,\n",
    "        'unique_words': unique_words,\n",
    "        'database_path': db_path\n",
    "    }\n",
    "\n",
    "# Note: The actual processing will be run in the next step\n",
    "print(\"Processing pipeline function defined. Ready to process articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a05ac",
   "metadata": {},
   "source": [
    "## Step 6: Run the Processing (Execute with Caution)\n",
    "\n",
    "**WARNING**: This step will process all 295k+ articles and may take several hours. Only run when ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f589052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Full processing is disabled for safety.\n",
      "Set RUN_FULL_PROCESSING = True to enable full processing.\n",
      "\\n📊 Alternative: Run a test with a small sample:\n",
      "\\n🧪 Running test with 100 articles...\n",
      "Database setup complete: test_dutch_words.sqlite\n",
      "Starting processing of 100 articles...\n",
      "Batch size: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|█████     | 1/2 [00:03<00:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 articles, extracted 13,013 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESSING COMPLETE ===\n",
      "Total articles processed: 100\n",
      "Total words extracted: 29,696\n",
      "Database saved to: test_dutch_words.sqlite\n",
      "Unique words in database: 6,195\n",
      "POS categories found: 13\n",
      "Yearly distribution:\n",
      "  2015: 6,195 word instances\n",
      "Test results: {'articles_processed': 100, 'words_extracted': 29696, 'unique_words': 6195, 'database_path': 'test_dutch_words.sqlite'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAFETY CHECK: Only run if you're ready to process all articles\n",
    "RUN_FULL_PROCESSING = False  # Set to True when ready to run\n",
    "\n",
    "if RUN_FULL_PROCESSING:\n",
    "    print(\"🚀 Starting full processing of all articles...\")\n",
    "    print(\"This may take several hours depending on your system.\")\n",
    "    print(\"You can monitor progress and stop if needed.\")\n",
    "    \n",
    "    # Load the full dataset if not already loaded\n",
    "    if 'df' not in locals() or df is None:\n",
    "        print(\"Loading dataset...\")\n",
    "        df = pd.read_feather(\"data/NOS_NL_articles_2015_mar_2025.feather\")\n",
    "        print(f\"Dataset loaded: {df.shape}\")\n",
    "    \n",
    "    # Check if spaCy model is loaded\n",
    "    if nlp is None:\n",
    "        print(\"Error: Dutch spaCy model not loaded. Please run the installation steps first.\")\n",
    "    else:\n",
    "        # Run the processing pipeline\n",
    "        results = process_articles_pipeline(\n",
    "            df=df,\n",
    "            nlp_model=nlp,\n",
    "            db_path=\"dutch_words_database.sqlite\",\n",
    "            batch_size=500  # Smaller batches for better memory management\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Full processing complete!\")\n",
    "        print(f\"Results: {results}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️  Full processing is disabled for safety.\")\n",
    "    print(\"Set RUN_FULL_PROCESSING = True to enable full processing.\")\n",
    "    print(\"\\\\n📊 Alternative: Run a test with a small sample:\")\n",
    "    \n",
    "    # Test with a small sample instead\n",
    "    if 'df' in locals() and df is not None:\n",
    "        sample_size = 100\n",
    "        sample_df = df.head(sample_size)\n",
    "        \n",
    "        print(f\"\\\\n🧪 Running test with {sample_size} articles...\")\n",
    "        \n",
    "        if nlp is not None:\n",
    "            test_results = process_articles_pipeline(\n",
    "                df=sample_df,\n",
    "                nlp_model=nlp,\n",
    "                db_path=\"test_dutch_words.sqlite\",\n",
    "                batch_size=50\n",
    "            )\n",
    "            print(f\"Test results: {test_results}\")\n",
    "        else:\n",
    "            print(\"Cannot run test - spaCy model not loaded\")\n",
    "    else:\n",
    "        print(\"Dataset not loaded. Please run the data loading cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3741f",
   "metadata": {},
   "source": [
    "## Step 7: Analysis and Export\n",
    "\n",
    "Analyze the extracted words and create various exports for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2efef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing analysis functions...\n",
      "=== WORD DATABASE ANALYSIS ===\n",
      "Database: test_dutch_words.sqlite\n",
      "\\nBasic Statistics:\n",
      "  Unique words: 6,195\n",
      "  Total word instances: 29,696\n",
      "  POS categories: 13\n",
      "\\nTop 20 Most Frequent Words:\n",
      "   1. de (de) [determiner] - 1,862 times\n",
      "   2. in (in) [preposition] - 882 times\n",
      "   3. van (van) [preposition] - 852 times\n",
      "   4. een (een) [determiner] - 780 times\n",
      "   5. het (het) [determiner] - 692 times\n",
      "   6. en (en) [conjunction] - 516 times\n",
      "   7. is (zijn) [auxiliary] - 430 times\n",
      "   8. op (op) [preposition] - 392 times\n",
      "   9. met (met) [preposition] - 278 times\n",
      "  10. voor (voor) [preposition] - 248 times\n",
      "  11. er (er) [adverb] - 231 times\n",
      "  12. het (het) [pronoun] - 198 times\n",
      "  13. dat (dat) [conjunction] - 193 times\n",
      "  14. te (te) [preposition] - 193 times\n",
      "  15. niet (niet) [adverb] - 191 times\n",
      "  16. zijn (zijn) [auxiliary] - 186 times\n",
      "  17. hij (hij) [pronoun] - 184 times\n",
      "  18. bij (bij) [preposition] - 180 times\n",
      "  19. jaar (jaar) [noun] - 179 times\n",
      "  20. die (die) [pronoun] - 163 times\n",
      "\\nWords by POS Category:\n",
      "  noun: 2,305 words (avg freq: 2.6)\n",
      "  verb: 1,349 words (avg freq: 2.6)\n",
      "  proper_noun: 1,225 words (avg freq: 2.3)\n",
      "  adjective: 752 words (avg freq: 3.0)\n",
      "  adverb: 181 words (avg freq: 11.2)\n",
      "  other: 83 words (avg freq: 1.6)\n",
      "  preposition: 66 words (avg freq: 68.0)\n",
      "  pronoun: 65 words (avg freq: 24.7)\n",
      "  determiner: 51 words (avg freq: 73.9)\n",
      "  number: 45 words (avg freq: 7.9)\n",
      "  auxiliary: 37 words (avg freq: 42.4)\n",
      "  conjunction: 33 words (avg freq: 35.8)\n",
      "  interjection: 3 words (avg freq: 1.3)\n",
      "\\nYearly Word Trends:\n",
      "  2015: 6,195 unique words, 29,696 total instances\n",
      "=== EXPORTING WORD LISTS ===\n",
      "Output directory: test_exports\n",
      "\\n1. Exporting all words list...\n",
      "   Exported 5,672 words to all_words.txt\n",
      "\\n2. Exporting common words (frequency >= 10)...\n",
      "   Exported 381 words to common_words.txt\n",
      "\\n3. Exporting words by POS category...\n",
      "   Exported 2,305 noun words to noun_words.txt\n",
      "   Exported 1,349 verb words to verb_words.txt\n",
      "   Exported 752 adjective words to adjective_words.txt\n",
      "   Exported 181 adverb words to adverb_words.txt\n",
      "\\n4. Exporting full data to CSV...\n",
      "   Exported full data to words_full_data.csv\n",
      "\\n5. Exporting game-friendly word list...\n",
      "   Exported 403 words to game_words.txt\n",
      "\\n✅ All exports completed in: test_exports\n"
     ]
    }
   ],
   "source": [
    "def analyze_word_database(db_path=\"dutch_words_database.sqlite\"):\n",
    "    \"\"\"\n",
    "    Analyze the word database and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== WORD DATABASE ANALYSIS ===\")\n",
    "        print(f\"Database: {db_path}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM words')\n",
    "        total_unique_words = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT SUM(total_frequency) FROM words')\n",
    "        total_word_instances = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(DISTINCT pos_category) FROM words')\n",
    "        pos_categories = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"\\\\nBasic Statistics:\")\n",
    "        print(f\"  Unique words: {total_unique_words:,}\")\n",
    "        print(f\"  Total word instances: {total_word_instances:,}\")\n",
    "        print(f\"  POS categories: {pos_categories}\")\n",
    "        \n",
    "        # Top words by frequency\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_category, total_frequency \n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC \n",
    "            LIMIT 20\n",
    "        ''')\n",
    "        top_words = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\\\nTop 20 Most Frequent Words:\")\n",
    "        for i, (word, lemma, pos, freq) in enumerate(top_words, 1):\n",
    "            print(f\"  {i:2d}. {word} ({lemma}) [{pos}] - {freq:,} times\")\n",
    "        \n",
    "        # Words by POS category\n",
    "        cursor.execute('''\n",
    "            SELECT pos_category, COUNT(*) as count, AVG(total_frequency) as avg_freq\n",
    "            FROM words \n",
    "            GROUP BY pos_category \n",
    "            ORDER BY count DESC\n",
    "        ''')\n",
    "        pos_stats = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\\\nWords by POS Category:\")\n",
    "        for pos, count, avg_freq in pos_stats:\n",
    "            print(f\"  {pos}: {count:,} words (avg freq: {avg_freq:.1f})\")\n",
    "        \n",
    "        # Yearly trends\n",
    "        cursor.execute('''\n",
    "            SELECT year, COUNT(*) as word_count, SUM(frequency) as total_freq\n",
    "            FROM word_frequencies \n",
    "            GROUP BY year \n",
    "            ORDER BY year\n",
    "        ''')\n",
    "        yearly_trends = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\\\nYearly Word Trends:\")\n",
    "        for year, word_count, total_freq in yearly_trends:\n",
    "            print(f\"  {year}: {word_count:,} unique words, {total_freq:,} total instances\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Database file not found: {db_path}\")\n",
    "\n",
    "def export_word_lists(db_path=\"dutch_words_database.sqlite\", output_dir=\"exports\"):\n",
    "    \"\"\"\n",
    "    Export word lists in various formats for different use cases.\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database\n",
    "        output_dir (str): Directory to save exports\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        print(f\"=== EXPORTING WORD LISTS ===\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # 1. All words list (for general use)\n",
    "        print(\"\\\\n1. Exporting all words list...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT DISTINCT word FROM words ORDER BY word')\n",
    "        all_words = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        with open(f\"{output_dir}/all_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word in all_words:\n",
    "                f.write(word + '\\\\n')\n",
    "        print(f\"   Exported {len(all_words):,} words to all_words.txt\")\n",
    "        \n",
    "        # 2. Common words (frequency >= 10)\n",
    "        print(\"\\\\n2. Exporting common words (frequency >= 10)...\")\n",
    "        cursor.execute('SELECT word, total_frequency FROM words WHERE total_frequency >= 10 ORDER BY total_frequency DESC')\n",
    "        common_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/common_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in common_words:\n",
    "                f.write(f\"{word}\\\\t{freq}\\\\n\")\n",
    "        print(f\"   Exported {len(common_words):,} words to common_words.txt\")\n",
    "        \n",
    "        # 3. Words by POS category\n",
    "        print(\"\\\\n3. Exporting words by POS category...\")\n",
    "        pos_categories = ['noun', 'verb', 'adjective', 'adverb']\n",
    "        \n",
    "        for pos in pos_categories:\n",
    "            cursor.execute('''\n",
    "                SELECT word, total_frequency \n",
    "                FROM words \n",
    "                WHERE pos_category = ? \n",
    "                ORDER BY total_frequency DESC\n",
    "            ''', (pos,))\n",
    "            pos_words = cursor.fetchall()\n",
    "            \n",
    "            with open(f\"{output_dir}/{pos}_words.txt\", 'w', encoding='utf-8') as f:\n",
    "                for word, freq in pos_words:\n",
    "                    f.write(f\"{word}\\\\t{freq}\\\\n\")\n",
    "            print(f\"   Exported {len(pos_words):,} {pos} words to {pos}_words.txt\")\n",
    "        \n",
    "        # 4. CSV export with full data\n",
    "        print(\"\\\\n4. Exporting full data to CSV...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, lemma, pos_tag, pos_category, total_frequency, first_seen, last_seen\n",
    "            FROM words \n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        \n",
    "        import csv\n",
    "        with open(f\"{output_dir}/words_full_data.csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['word', 'lemma', 'pos_tag', 'pos_category', 'total_frequency', 'first_seen', 'last_seen'])\n",
    "            writer.writerows(cursor.fetchall())\n",
    "        print(f\"   Exported full data to words_full_data.csv\")\n",
    "        \n",
    "        # 5. Game-friendly word list (4-8 letters, common words)\n",
    "        print(\"\\\\n5. Exporting game-friendly word list...\")\n",
    "        cursor.execute('''\n",
    "            SELECT word, total_frequency \n",
    "            FROM words \n",
    "            WHERE LENGTH(word) BETWEEN 4 AND 8 \n",
    "            AND total_frequency >= 5\n",
    "            AND pos_category IN ('noun', 'verb', 'adjective')\n",
    "            ORDER BY total_frequency DESC\n",
    "        ''')\n",
    "        game_words = cursor.fetchall()\n",
    "        \n",
    "        with open(f\"{output_dir}/game_words.txt\", 'w', encoding='utf-8') as f:\n",
    "            for word, freq in game_words:\n",
    "                f.write(word + '\\\\n')\n",
    "        print(f\"   Exported {len(game_words):,} words to game_words.txt\")\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"\\\\n✅ All exports completed in: {output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "\n",
    "# Test analysis (will work with existing test database)\n",
    "print(\"Testing analysis functions...\")\n",
    "if os.path.exists(\"test_dutch_words.sqlite\"):\n",
    "    analyze_word_database(\"test_dutch_words.sqlite\")\n",
    "    export_word_lists(\"test_dutch_words.sqlite\", \"test_exports\")\n",
    "else:\n",
    "    print(\"No test database found. Run the processing steps first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
